#!/usr/bin/env qore
# -*- mode: qore; indent-tabs-mode: nil -*-

/*
    Qorus Integration Engine(R) Community Edition

    Copyright (C) 2003 - 2023 Qore Technologies, s.r.o., all rights reserved

    LICENSE: GNU GPLv3

    https://www.gnu.org/licenses/gpl-3.0.en.html
*/

# Qorus Database Loader/Release Manager: loads code and configuration objects into the Qorus DB

%requires qore >= 1.0

# here we add fallback paths to the QORE_INCLUDE_DIR search path,
# in case QORE_INCLUDE_DIR is not set properly
%append-include-path /var/opt/qorus/qlib:$OMQ_DIR/qlib:/opt/qorus/qlib

%require-our
%enable-all-warnings
%strict-args
%new-style
%require-types
%allow-injection

%exec-class oload
%no-child-restrictions

%include qorus-client.ql

# here we add fallback paths to the QORE_MODULE_DIR search path,
# in case QORE_MODULE_DIR is not set properly for Qorus
%append-module-path /var/opt/qorus/qlib:$OMQ_DIR/qlib:/opt/qorus/qlib:$OMQ_DIR/user/modules

%requires CsvUtil

%requires QorusVersion

# for Util::compare_version
%requires Util

# for services using the AbstractServiceWebSocketHandler class
%requires WebSocketHandler

# for services using the AbstractServiceDataStreamResponseHandler class
%requires DataStreamRequestHandler

# for SQL operations
%requires SqlUtil

# for services implementing stream handlers
%requires DataStreamRequestHandler

# for services extending ServiceFileHandler
%requires WebUtil

# for user schema handling
%requires Schema

# for connection handling
%requires ConnectionProvider

# for Program introspection
%requires reflection

# for filesystem handling
%requires FsUtil

%requires QorusObjectParser
%requires Qyaml
%requires Logger
%requires Qorize

%try-module jni
%define NO_JNI
%endtry

%ifndef NO_JNI
%module-cmd(jni) add-classpath $OMQ_DIR/jar/qore-jni.jar
%module-cmd(jni) add-classpath $OMQ_DIR/jar/qore-jni-compiler.jar

%module-cmd(jni) import java.util.ArrayList
%module-cmd(jni) import javax.tools.DiagnosticCollector
%module-cmd(jni) import org.qore.jni.compiler.QoreJavaCompiler
%module-cmd(jni) import org.qore.jni.compiler.CompilerOutput
%endif

# application object; assigned in constructor
our oload oload;

# global SQLInterface object
our SQLInterface sqlif;

class oload {
    private {
        # maps a workflow name and version to an ID
        hash workflowmap;

        # holds the path to the schema-tool command once found
        string schema_tool;

        # flag for group changes
        bool reload_groups = False;

        # hash of files already processed
        hash processed;

        # all warnings except "duplicate-local-vars"
        const WarningMask = WARN_ALL & ~WARN_DUPLICATE_LOCAL_VARS;

        hash deleteMethodsMap;
        hash deleteByIdMethodsMap;

        QorusObjectParser::Parser qorusObjectParser();
    }

    public {
        const FormatVersions = {
            "1.0": True,
            "1.1": True,
            "1.2": True,
            "2.0": True,
            "2.6": True,
        };

        const TextFileTypes = {
            "txt": True,
            "csv": True,
            "htm": True,
            "html": True,
            "js": True,
            "css": True,
            "json": True,
            "conf": True,
            "pl": True,
            "php": True,
            "q": True,
            "wsdl": True,
            "xht": True,
            "xhtml": True,
            "xsd": True,
            "xsl": True,
            "xslt": True,
            "yaml": True,
        };

        const TemplateFileTypes = {
            "qhtml": True,
            "qjs": True,
            "qcss": True,
            "qjson": True,
            "qxhtml": True,
            "qyaml": True,
        };

        const AsyncKeys = ("endname", "endversion", "queue");

        # BUG 1844: check at least the lastest WF version for keys
        const ValidWorkflowKeys = (
            "attach",
            "author",
            "autostart",
            "classes",
            "constants",
            "desc",
            "description",
            "detach",
            "errorfunction",
            "error_handler",
            "functions",
            "groups",
            "keylist",
            "mappers",
            "max_instances",
            "name",
            "oldkeys",
            "onetimeinit",
            "options",
            "patch",
            "remote",
            "sla_threshold",
            "statuses",
            "steps",
            "tags",
            "version",
            "vmaps",
            "workflowid",
            "workflow-modules",
            "class",
        );

        const opts = {
            "redef"              : "A,allow-redef",
            "data_ts"            : "data-ts=s@",
            "ds"                 : "D,datasource=s@",
            "del"                : "X,delete=s@",
            "delete-id"          : "delete-id=s@",
            "force"              : "f,force",
            "full"               : "F,full-release",
            "help"               : "h,help",
            "index_ts"           : "index-ts=s@",
            "javac"              : "compile-java=s",
            "list"               : "L,list=s@",
            "noerrupdate"        : "E,no-error-update",
            "override"           : "O,override",
            "proxy"              : "p,proxy-url=s",
            "quiet"              : "q,quiet",
            "refresh"            : "r,refresh=s",
            "reload"             : "R,reload",
            "omqschema"          : "schema=s@",
            "showdefines"        : "d,show-defines",
            "showfullerror"      : "w,show-full-error",
            "showrel"            : "s,show-release=s@",
            "show"               : "W,show-url",
            "sign"               : "sign-schema",
            "token"              : "t,token=s",
            "usesched"           : "S,use-schedule",
            "userschema"         : "user-schema=s@",
            "url"                : "u,url=s",
            "validate"           : "l,validate",
            "valsvc"             : "validate-service=s",
            "valjob"             : "validate-job=s",
            "verbose"            : "v,verbose:i+",
            "export_cfg_val"     : "export-cfg-val:s",
            "import_cfg_val_rest": "import-cfg-val-rest",
        };

        # base class names without the "...Step" or "...ArrayStep" endings
        const StepClassBaseClasses = {
            ExecNormal: "QorusNormal",
            ExecAsync: "QorusAsync",
            ExecSubWorkflow: "QorusSubworkflow",
            ExecEvent: "QorusEvent",
        };

        const ConfigItemBasicTypes = {
            "int": True,
            "string": True,
            "bool": True,
            "float": True,
            "date": True,
        };

        const ConfigItemBasicStarTypes = map {("*" + $1): True}, keys ConfigItemBasicTypes;

        const ConfigItemExtendedTypes = {
            "data-provider": True,
            "connection": True,
            "mapper": True,
            "value-map": True,
            "workflow": True,
            "service": True,
            "job": True,
        };

        const ConfigItemExtendedStarTypes = map {("*" + $1): True}, keys ConfigItemExtendedTypes;

        const ConfigItemTypes = ConfigItemBasicTypes + {
            "hash": True,
            "list": True,
        } + ConfigItemExtendedTypes;

        const ConfigItemStarTypes = ConfigItemBasicStarTypes + {
            "*hash": True,
            "*list": True,
            "any": True
        } + ConfigItemExtendedStarTypes;

        #! hash of Qorus classes needed for all fake APIs for dev modules
        const FakeCommonApiClasses = {
            "QorusSystemAPIHelper": True,
            "QorusSystemRestHelper": True,
        };

        const ConfigItemEnumTypes = ConfigItemBasicTypes + ConfigItemBasicStarTypes - ("bool", "*bool");

        const StrMap = {
            "fsm": True,
            "pipelines": True,
            "types": True,
            "config_values": True,
        };

        const AsyncStepTypes = {
            OMQ::ExecAsync: True,
            OMQ::ExecSubWorkflow: True,
            OMQ::ExecEvent: True,
        };

        # refresh command
        bool refresh = False;

        # maps from a class ID to a hash of class info
        hash<string, hash<auto>> classmap;
        # maps from a class name and version to an ID
        hash classrmap;

        # maps from a constant ID to a hash of constant info
        hash constmap;
        # maps from a constant name and version to an ID
        hash constrmap;

        # maps a function name and version to a stepid
        hash stepmap;

        # map from event names to IDs
        hash emap;
        # map from event IDs to names
        hash ermap;

        # map from queue names to IDs
        hash qmap;
        # map from queue IDs to names
        hash qrmap;

        # map function names to ID
        hash fmap;
        # map function IDs to names
        hash frmap;

        const ObjectTypeToGroupHashName = {
            "service": "gsmap",
            "job": "gjmap",
            "mapper": "gmmap",
            "value map": "gvmmap",
            "fsm": "gfmap",
            "pipeline": "gpmap",
        };

        # map group names to group info
        hash gmap;
        # map group IDs to names
        hash grmap;
        # map workflowids to group name
        hash gwmap;
        # map serviceids to group name
        hash gsmap;
        # map jobids to group name
        hash gjmap;
        # map mapperids to group name
        hash gmmap;
        # map vmap ids to group name
        hash gvmmap;
        # map FSMs to group names
        hash gfmap;
        # map pipelines to group name
        hash gpmap;

        # map mapper names to mapper info
        hash mmap;
        # map mapper IDs to mapper info
        hash mrmap;

        # map vmap names to vmap info
        hash vmmap;
        # map vmap IDs to mapper info
        hash vmrmap;

        # maps object names to source files
        hash omap;

        # program options
        hash o;

        # client options
        hash coptions;

        # Qorus options
        hash options;

        # release list
        list rlist = ();

        # source file lists
        list yaml_sources      = ();  # list of definitions in YAML format
        hash yaml_code_files   = {};  # hash of files to be ignored by oload (they are referenced from YAML defintion file)
        hash yaml_config_items = {};  # hash of YAML files with config items, files -> config item name -> config item
        list slist             = ();  # list of service files
        list flist             = ();  # list of function files
        list wflist            = ();  # list of workflow files
        list classlist         = ();  # list of class files
        list constlist         = ();  # list of constant files
        list jlist             = ();  # list of job files
        list connlist          = ();  # list of user connection files
        list qsmlist           = ();  # list of qorus user schema modules
        list qmlist            = ();  # list of mapper definition files
        list qvmaplist         = ();  # list of value maps
        list<hash<auto>> qpkglist = ();  # list of installation packages
        # hash of lists of release scripts (qscripts)
        hash qscripts = {
           "pre": (),
           "post": (),
           "post_reload": (),
        };

        hash valsh;                     # hash of services to be validated
        hash valwh;                     # hash of workflows to be validated
        hash<string, hash<auto>> valjh; # hash of jobs to be validated

        hash valmh;              # hash of mappers to be validated

        hash valfh;              # hash of function_instanceids to be used for validation in case of option "-l"

        # hash of classids to be used for validation in case of option "-l": classid -> True
        hash<string, bool> valclh;

        # hash of stepids to be used for validation; stepid -> True
        hash<string, bool> val_step_map;

        hash valcoh;             # hash of constantids to be used for validation in case of option "-l"

        # hash of function_instanceids inserted that possibly add new versions to existing library functions to be used for validation in case of option "-l"
        hash valflh;

        # hash of FSMs to be used for validation; name -> True
        hash<string, bool> val_fsm_map;

        # hash of pipelines to be used for validation; name -> True
        hash<string, bool> val_pipeline_map;

        # encoding of system datasource
        string encoding;

        # maps from a servicename to a hash of service info
        hash smap;

        # issue #3281: config item value map for config item value migration
        /** map: interface type (service, job, step, fsm) -> ID -> config item name -> value
            for config item value migration
        */
        hash<string, hash<string, hash<auto>>> config_item_value_map;

        # maps from step names to step info: name -> step info hash
        hash<string, hash<auto>> step_name_map;

        # maps from class names to class info: name -> class info hash
        hash<string, hash<auto>> class_name_map;

        # application map
        hash amap;

        # objects already validated type -> id -> True
        hash<string, hash<string, bool>> already_validated_map;

        # error manager
        ErrorManager EM;

        # audit object
        Audit audit;

        # user connection file parser
        UserConnectionFileHelper ucf;

        # oload mapper container
        ClientMappers um();

        # default warning mask
        int warningMask = WarningMask;

        # show full error?
        bool show_full_error;

        # hash of Tables (transaction management)
        static hash tables = {};

        # hash of omqmap maps to reload on exit
        static hash omqmap_reload;

        # flag to reload error defs on exit
        static bool reload_errors = False;

        # flag to reload user connections on exit. Keys must follow rest API paths for remote/*
        static hash reload_connections = {};

        # flag to show warning banner and end oload process with return value != 0 at the end
        static bool has_any_error = False;

        # all dependency needed for currently loading FSM
        hash<auto> fsm_dependencies = {};

        # all dependencies between FSM when deleting
        hash<auto> delete_fsm_dependencies = {};

        # already deleted FSM
        hash<auto> deleted_fsm = {};
    }

    static private usage() {
        printf(
"usage: %s [options] [file]
 FILE OPTIONS:
  <file>.qfd         function definitions
  <file>.qwf         workflow definitions
  <file>.qsd         service definitions
  <file>.qsd.java    Java service definitions
  <file>.qjob        job definitions
  <file>.qjob.java   Java job definitions
  <file>.qclass      class definitions
  <file>.qclass.java Java class definitions
  <file>.qconst      constant definitions
  <file>.qconn       user connection definitions
  <file>.qmapper     mapper definition
  <file>.qvmap       value map definition
  <file>.qsm         user schema modules for alignment
  <file>.qrf         load command file to execute
  <file>.qscript     custom scripts run at the end of deployment
  <file>.yaml        qorus object definitions in YAML format
  <file>.tar*        qorus installation packages
  @<file>            load command file to execute

 SCHEMA AND OBJECT DELETION OPTIONS: USE WITH EXTREME CARE!
     --schema=arg        execute SQL commands from file in Qorus DB schema
     --user-schema=ARG   execute SQL commands from file in Qorus User DB schema
     --data-ts=ARG       set data tablespace name, ARG=<dsname=tsname>
     --index-ts=ARG      set index tablespace name, ARG=<dsname=tsname>
  -X,--delete=ARG        delete object, ARG=type:spec
         workflow           ARG: workflow:<name>:<version>
         wf orders          ARG: wfinstances:<name>:<version>
         step               ARG: step:<name>:<version>
         function           ARG: function:<name>:<version>
         class              ARG: class:<name>:<version>
         constant           ARG: constant:<name>:<version>
         user service       ARG: service:<name>:<version>
         mapper             ARG: mapper:<name>:<version>
         user svc method    ARG: method:<svc-name>:<version>:<method-name>
         queue              ARG: queue:<name>
         job                ARG: job:<name>
         job instances      ARG: jobinstances:<name>
         wf event type      ARG: event:<name>
         user connection    ARG: uconn:<name>
         datasource         ARG: datasource:<name>
         qorus remote       ARG: remote:<name>
         value map          ARG: vmap:<name>
         type               ARG: type:<type-path>
         pipeline           ARG: pipeline:<name>
         fsm                ARG: fsm:<name>
  --delete-id=ARG        delete object by id, ARG=type:id
         workflow           ARG: workflow:<id>
         wf orders          ARG: wfinstances:<id>
         step               ARG: step:<id>
         function           ARG: function:<id>
         class              ARG: class:<id>
         constant           ARG: constant:<id>
         user service       ARG: service:<id>
         mapper             ARG: mapper:<id>
         user svc method    ARG: method:<id>
         queue              ARG: queue:<id>
         job                ARG: job:<id>
         job instances      ARG: jobinstances:<id>
         wf event type      ARG: event:<id>
         value map          ARG: vmap:<id>

 SERVER OPTIONS:
  -D,--datasource=ARG    overrides datasource params
  -p,--proxy-url=ARG     sets the proxy URL (ex: http://proxy_host:port)
  -R,--reload            reload interfaces in the server
  -t,--token=ARG         use the given token with HTTP requests to the Qorus server
  -u,--url=arg           sets Qorus URL (ex: http://host:port)
  -W,--show-url          displays the default Qorus system URL

 MISC OPTIONS:
  -A,--allow-redef       allow dangerous workflow redefinitions (for dev only)
  -d,--show-defines      show defines and exit
  -f,--force             force schema verification/downgrade with user schemas
  -F,--full-release      error if the release refers to external objects
  -E,--no-error-update   do not update workflow error definitions; only create
                         new global error definitions
  -h,--help              shows this help text
  -l,--validate          validate all functions recursively
  -q,--quiet             only display error messages
  -v,--verbose           sets verbosity level (more v's = more info)
  -L,--list=ARG          lists objects in the database
                         valid args: workflows, services, classes, constants,
                            functions, queues, events, groups, steps, releases,
                            uconn, jobs, mappers, vmaps
                         (only the first character(s) of the arg are needed,
                          shows more information with -v)
  -O,--override          override manually updated entries
  -s,--show-release=ARG  lists the specified release contents
  -S,--use-schedule      override manually updated job schedules
  -w,--show-full-error   show full error backtrace
  --export-cfg-val=ARG   exports config item values
                         arg: YAML file where values will be exported to (optional)
  --import-cfg-val-rest  import config item values using the Qorus REST API.

 IGNORED / DEPRECATED:
  -r,--refresh=ARG       refresh/reload interfaces (ARG ignored) [deprecated by --reload]
  --sign-schema          for backward compatibility [ignored]
", get_script_name());
        exit();
    }

    constructor() {
        initializeDeleteMethodsMaps();

        # set stack size to 8MB; oload is single-threaded anyway, but this will allow Python recursion limits to be
        # set with a sane value
        set_default_thread_stack_size(8 * 1024 * 1024);

        OMQ::ThreadLocalData ntld();
        tld = ntld;

        oload = self;

        # initialize Qorus client library
        QorusClient::initFast();

        coptions = omqclient.getOptions("qorus-client");
        options = omqclient.getOptions("qorus");

        # set default warning mask
        if (!coptions."warn-deprecated-api") {
            warningMask &= ~WARN_DEPRECATED;
        }

        # build application map
        map amap.$1 = True, coptions.applications;

        *hash<auto> del = process_command_line();
        if (o.show) {
            if (exists coptions."proxy-url")
                printf("%s (through HTTP proxy: %s)\n", coptions."client-url", coptions."proxy-url");
            else
                printf("%s\n", coptions."client-url");
            exit(0);
        }

        HashIterator it(coptions);
        while (it.next()) {
            string dsn = it.getKey().split("-")[0];

            if (it.getKey().regex("^.*-data-tablespace") && !o.dts{dsn})
                o.dts{dsn} = it.getValue();
            else if (it.getKey().regex("^.*-index-tablespace") && !o.its{dsn})
                o.its{dsn} = it.getValue();
        }

        omqclient.initOmq();
        sqlif = omqclient.getSQLIF();

        encoding = sqlif.omqp.getOSEncoding();
        if (encoding == "unknown") {
            encoding = "utf8";
        }

        qorusObjectParser.setEncoding(encoding);

        Logger logger = new Logger("oload");
        logger.setLevel(Logger::LoggerLevel::INFO);
        auto std_appender = new LoggerAppenderStdOut("stdout", new LoggerLayoutPattern("%m%n"));
        std_appender.open();
        logger.addAppender(std_appender);

        QorusObjectParser::Parser::setLogger(logger);
        Qyaml::Validator::setLogger(logger);

        EM = new ErrorManager(sqlif);

        if (!options."auto-error-update" || o.noerrupdate)
            EM.setAuto(False);

        audit = new Audit(sqlif, options.audit);

        {
            int errs;
            foreach string arg in (ARGV) {

%ifdef Windows
                if (arg =~ /[\*\?]/) {
                    foreach string farg in (glob(arg)) {
                        if (hstat(farg).type == "DIRECTORY")
                            continue;
                        int rc = processFile(farg);
                        if (rc && !errs)
                            errs = rc;
                    }
                    continue;
                }
%endif
                if (hstat(arg).type == "DIRECTORY")
                    continue;
                int rc = processFile(arg);
                if (rc && !errs)
                    errs = rc;
            }
            if (errs) {
                stderr.printf("correct the errors above and try again.\n");
                exit(1);
            }
        }

        if (o.export_cfg_val) {
            InterfaceConfigContainer::exportConfigItemValues(NOTHING, o.export_cfg_val.typeCode() == NT_STRING ?
                o.export_cfg_val : NOTHING);
            exit(0);
        }

        if (!flist && !wflist && !slist && !o.schema
            && !constlist && !classlist && !jlist && !connlist && !qsmlist && !qmlist
            && !o.sign && !del && !o.list && !o.showrel && !qvmaplist && !qpkglist && !qscripts.pre && !qscripts.post
            && !qscripts.post_reload && !yaml_sources && !o.javac && !o.valsvc && !o.valjob) {
            printf("nothing to do!\n");
            exit(0);
        }

        # process internal options first
        if (o.javac) {
            compileJava(o.javac);
        }

        if (o.valsvc) {
            validateService(o.valsvc);
        }

        if (o.valjob) {
            validateJob(o.valjob);
        }

        if (o.url) {
            qrest.setURL(o.url);
        }
        if (o.proxy) {
            qrest.setProxyURL(o.proxy);
        }

        if (!ENV.USER)
            ENV.USER = "omq";

        if (o.list)
            listObjects(o.list);

        if (o.showrel)
            showReleases(o.showrel);

        if (o.checkShutdown)
            checkShutdown();

        if (del{"delete"}) {
            deleteObjects(del{"delete"}, deleteMethodsMap);
        }

        if (del{"delete-id"}) {
            # deletion by id is not supported for connections (their names are unique)
            deleteObjects(del{"delete-id"}, deleteByIdMethodsMap);
        }

        runSchemaFiles();

        oload::checkDataModel();

        try {
            # run client qscripts - before anything deployed
            if (qscripts.pre) {
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf("processing pre-release scripts: ");
                    flush();
                }
                map runQscript("pre", $1), qscripts.pre;
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf(" OK\n");
                }
            }

            # load reference maps from the system schema
            getMaps();

            # YAML definitions have higher priority
            hash<auto> yaml_definitions = parseAndValidateYaml(yaml_sources);
            createObjectsFromYaml(yaml_definitions);

            if (flist) {
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf("processing functions (deprecated format): ");
                    flush();
                }
                createObjects(new FunctionDbInserter(), flist, "function");
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf(" OK\n");
                }
            }
            if (constlist) {
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf("processing constants (deprecated format): ");
                    flush();
                }
                createObjects(new ConstantDbInserter(), constlist, "constant");
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf(" OK\n");
                }
            }

            # load reference maps from the system schema
            getMaps();

            if (classlist) {
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf("processing classes (deprecated format): ");
                    flush();
                }
                createObjects(new ClassDbInserter(), classlist, "class");
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf(" OK\n");
                }
            }
            ClassDbInserter::finalize();

            # create connections from connection description files
            if (connlist) {
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf("processing connections (deprecated format): ");
                    flush();
                }
                map createConnections($1), connlist;
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf(" OK\n");
                }
            }

            # perform user schema alignment from user schema description modules
            if (qsmlist) {
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf("processing user schemas: ");
                    flush();
                }
                map alignUserSchema($1), qsmlist;
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf(" OK\n");
                }
            }

            if (qvmaplist) {
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf("processing value maps (deprecated format): ");
                    flush();
                }
                createObjects(new ValueMapDbInserter(), qvmaplist, "value map");
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf(" OK\n");
                }
            }

            if (qmlist) {
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf("processing mappers (deprecated format): ");
                    flush();
                }
                createObjects(new MapperDbInserter(), qmlist, "mapper");
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf(" OK\n");
                }
            }

            if (jlist) {
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf("processing jobs (deprecated format): ");
                    flush();
                }
                createObjects(new JobDbInserter(), jlist, "job");
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf(" OK\n");
                }
            }

            if (slist) {
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf("processing services (deprecated format): ");
                    flush();
                }
                map createServices(new ServiceDbInserter(), $1{"file"}, $1{"release"}), slist;
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf(" OK\n");
                }
            }

            if (wflist) {
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf("processing workflows (deprecated format): ");
                    flush();
                }
                map doWorkflow($1), wflist;
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf(" OK\n");
                }
            }

            if (qpkglist) {
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf("processing packages: ");
                    flush();
                }
                map installPackage($1.file), qpkglist;
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf(" OK\n");
                }
            }

            # find more workflows and services to validate
            if (o.validate) {
                updateValidationLists();
            }

            # run client qscripts - post deployment
            if (qscripts.post) {
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf("processing post-release scripts: ");
                    flush();
                }
                map runQscript("post", $1), qscripts.post;
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf(" OK\n");
                }
            }

            bool validation_result = validate();

            # Cannot be done in the createObjectsFromYaml method because we need to do this after
            # validation is completed in order to get config items for which we're going to add global values.
            # Otherwise will fail in case global config item values are loaded within config item definitions.
            map InterfaceConfigContainer::importConfigItemValues($1, $1{"yaml_source"}{"file"}),
                yaml_definitions{"config-item-values"};

            map InterfaceConfigContainer::importGlobalConfigItemValues($1, $1{"yaml_source"}{"file"}),
                yaml_definitions{"global-config-item-values"};

            # Validation exceptions are ignored
            #    so all validations can be completed,
            #    in case of invalid code the validation will return False
            if (validation_result && refresh) {
                refreshServer();
            }

            if (rlist) {
                map $1.commit(), new ListReverseIterator(rlist);
            }

            # run client qscripts - post refresh
            if (qscripts.post_reload) {
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf("processing post-reload scripts: ");
                }
                map runQscript("post_reload", $1), qscripts.post_reload;
                if (!oload.o.quiet && !oload.o.verbose) {
                    printf(" OK\n");
                }
            }

            if (oload::has_any_error) {
                printf("\n*** OLOAD NOT CLEAN; note issues listed above ***\n\n");
                exit(1);
            }
        } catch (hash<ExceptionInfo> ex) {
            # flush I/O from all possible output streams to ensure that the error message appears in the right place
            flush();
            stdout.sync();
            stderr.sync();
            if (options."debug-system" || o.showfullerror) {
                stderr.printf("%s\n", Util::get_exception_string(ex));
                auto arg = ex.arg;
                try {
                    while (arg.typeCode() == NT_OBJECT && arg.hasCallableMethod("getCause")) {
                        arg = arg.getCause();
                    }
                    if (exists arg) {
                        stderr.printf("%s\n", arg.toString());
                    }
                } catch (hash<ExceptionInfo> ex0) {
                    if (ex0.err != "PROGRAM-ERROR") {
                        rethrow;
                    }
                }
            } else {
                stderr.printf("%s: %s: %s\n", get_ex_pos(ex), ex.err, ex.desc);
                if (ex.err !~ /DBI:/) {
                    oload::showEx(ex);
                }
            }
            exit(1);
        }
    }

    private compileJava(string input_file) {
%ifdef NO_JNI
        throw "NO-JAVA", "the jni module is not available";
%else
        string rv;
        try {
            hash<auto> info = Serializable::deserialize(input_file == "-" ? stdin_stream : new FileInputStream(input_file));
            if (!info.filename) {
                stderr.printf("missing filename in deserialized input for Java compilation (keys: %y)\n", keys info);
                exit(1);
            }
            if (info.content_index) {
                JavaClassHelper::content_index = info.content_index;
            }
            getClassMap();
            rv = JavaClassHelper::compileJavaSourceIntern(info.type, info.class_name, info.version, info."code",
                info.user_tags, info.info, info.requires);
            File f();
            f.open2(info.filename, O_CREAT | O_TRUNC | O_WRONLY, 0644, rv.encoding());
            f.write(rv);
        } catch (hash<ExceptionInfo> ex) {
            stderr.printf("failed to parse/process input for Java compilation: %s\n", get_exception_string(ex));
            exit(1);
        }
        #print(rv);
%endif
    }

    private validateService(string input_file) {
        try {
            hash<auto> info = Serializable::deserialize(input_file == "-"
                ? stdin_stream
                : new FileInputStream(input_file)
            );
            getMaps();
            ServiceValidator val(info);
        } catch (hash<ExceptionInfo> ex) {
            stderr.printf("failed to parse/process input for service validation: %s\n", get_exception_string(ex));
            exit(1);
        }
    }

    private validateJob(string input_file) {
        try {
            hash<auto> info = Serializable::deserialize(input_file == "-"
                ? stdin_stream
                : new FileInputStream(input_file)
            );
            getMaps();
            JobValidator val(info);
        } catch (hash<ExceptionInfo> ex) {
            stderr.printf("failed to parse/process input for service validation: %s\n", get_exception_string(ex));
            exit(1);
        }
    }

    initializeDeleteMethodsMaps() {
        deleteMethodsMap = {
            "wfinstances": \oload::deleteWorkflowInstances(),
            "workflow": \oload::deleteWorkflow(),
            "method": \oload::deleteServiceMethod(),
            "service": \oload::deleteService(),
            "step": \oload::deleteStep(),
            "queue": \oload::deleteQueue(),
            "jobinstances": \oload::deleteJobInstances(),
            "job": \oload::deleteJob(),
            "fsm": \oload::deleteFsm(),
            "pipeline": \oload::deletePipeline(),
            "group": \self.deleteGroup(),
            "event": \oload::deleteWorkflowEventType(),
            "uconn": \oload::deleteUserConnection(),
            "datasource": \oload::deleteDatasource(),
            "remote": \oload::deleteRemote(),
            "mapper": \oload::deleteMapper(),
            "function": \oload::deleteFunction(),
            "class": \oload::deleteClass(),
            "constant": \oload::deleteConstant(),
            "vmap": \oload::deleteValueMap(),
            "type": \oload::deleteType(),
        };

        deleteByIdMethodsMap = deleteMethodsMap - ("uconn", "datasource", "remote");
    }

    private static list<int> get_widths(hash<auto> q, int m) {
        list<int> l = ();
        list<string> keysv = keys q;
        if (!m) {
            m = elements keysv;
        }
        for (int ki = 0; ki < m; ++ki) {
            string k = keysv[ki];
            for (int i = 0; i < elements q{k}; ++i) {
                int sl = strlen(q{k}[i]);
                if (sl > l[ki]) {
                    l[ki] = sl;
                }
            }
        }
        return l;
    }

    checkApplications(string type, string name, string val) {
        list al = val.split(",");
        trim al;
        list el = ();
        foreach string app in (al) {
            if (amap{app})
                return;

            el += app;
        }

        throw "APPLICATION-ERROR", sprintf("%s %y \"application\" tag claims application%s %y which %s not registered for this instance of Qorus (known applications: %y)", type, name, el.size() == 1 ? "" : "s", el, el.size() == 1 ? "is" : "are", amap.keys());

    }

    private static string get_str(*string str, int max = 100) {
        if (!exists str) {
            return "n/a";
        }
        str =~ s/\n/ /g;
        str =~ s/\s+/ /g;
        if (str.length() > max) {
            splice str, max;
            str += "...";
        }
        return str;
    }

    private listObjects(softlist<auto> l) {
        foreach string a in (l) {
            int len = strlen(a);
            if (len > 0) {
                if (substr("services", 0, len) == a) {
                    hash<auto> q = sqlif.omqp.select("select serviceid, service_type, version, name, description, "
                        "autostart, manual_autostart, remote, modified from services order by serviceid");
                    # get field widths
                    list<auto> fl = oload::get_widths(q, 4);
                    string fmt = "%" + sprintf("%dd ", fl[0]) + "%-" + sprintf("%ds ", fl[1])
                        + "%s%s%s v%-" + sprintf("%dw ", fl[2]) + "%-" + sprintf("%dw: ", fl[3]) + "%s\n";

                    context (q) {
                        if (o.verbose) {
                            printf("%s: ", oload::fd(%modified));
                        }
                        f_printf(fmt, %serviceid, %service_type, %remote ? "R" : " ", %autostart ? "A" : " ",
                            %manual_autostart ? "*" : " ", %version, %name, oload::get_str(%description));
                    }
                    continue;
                } else if (substr("workflows", 0, len) == a) {
                    hash<auto> q = sqlif.omqp.select("select workflowid, version, patch, name, description, modified, "
                        "autostart, manual_autostart, remote, deprecated from workflows order by workflowid");

                    # get field widths
                    list<auto> fl = oload::get_widths(q, 4);
                    # patch width cannot be less than 3
                    if (fl[2] < 3) fl[2] = 3;
                    string fmt = "%" + sprintf("%dd ", fl[0]) + "%s%s%s v%-" + sprintf("%dw ", fl[1])
                        + "(patch %-" + sprintf("%dw) ", fl[2]) + "%-" + sprintf("%dw: ", fl[3]) + "%s\n";

                    context (q) {
                        if (o.verbose) {
                            printf("%s: ", oload::fd(%modified));
                        }
                        f_printf(fmt, %workflowid, %remote ? "R" : " ", sprintf("A%02d", %autostart),
                            %deprecated ? "X" : (%manual_autostart ? "*" : " "), %version, oload::get_str(%patch),
                            %name, oload::get_str(%description));
                    }

                    continue;
                } else if (substr("constants", 0, len) == a) {
                    hash<auto> q = sqlif.omqp.select("select constantid, version, patch, name, description, created, modified from constants order by constantid");

                    # get field widths
                    list fl = oload::get_widths(q, 4);
                    # patch width cannot be less than 3
                    if (fl[2] < 3) fl[2] = 3;
                    string fmt = "%" + sprintf("%dd ", fl[0]) + "v%-" + sprintf("%dw ", fl[1])
                        + "(patch %-" + sprintf("%dw) ", fl[2]) + "%-" + sprintf("%dw: ", fl[3]) + "%s\n";

                    context (q) {
                        if (o.verbose)
                            printf("%s: ", oload::fd(%modified));
                        f_printf(fmt, %constantid, %version, oload::get_str(%patch),
                                 %name, oload::get_str(%description));
                    }

                    continue;
                } else if (substr("classes", 0, len) == a) {
                    hash<auto> q = sqlif.omqp.select("select classid, version, patch, name, language, description, created, modified from classes order by classid");

                    # get field widths
                    list<int> fl = oload::get_widths(q, 5);
                    # patch width cannot be less than 3
                    if (fl[2] < 3) {
                        fl[2] = 3;
                    }
                    string fmt = sprintf("%%%dd v%%-%dw %%-%dw (patch %%-%dw) %%-%dw: %%s\n", fl[0], fl[1], fl[4], fl[2], fl[3]);

                    context (q) {
                        if (o.verbose) {
                            printf("%s: ", oload::fd(%modified));
                        }
                        f_printf(fmt, %classid, %version, %language, oload::get_str(%patch),
                                 %name, oload::get_str(%description));
                    }

                    continue;
                } else if (substr("queues", 0, len) == a) {
                    hash<auto> q = sqlif.omqp.select("select queueid, name, description, created, modified from queues order by queueid");

                    # get field widths
                    list fl = oload::get_widths(q, 2);
                    string fmt = "%" + sprintf("%dd ", fl[0]) + "%-" + sprintf("%dw ", fl[1]) + "%s\n";

                    context (q) {
                        if (o.verbose)
                            printf("%s: ", oload::fd(%modified));
                        f_printf(fmt, %queueid, %name, oload::get_str(%description));
                    }

                    continue;
                } else if (substr("events", 0, len) == a) {
                    hash<auto> q = sqlif.omqp.select("select workflow_event_typeid, name, description, created, modified from workflow_event_types order by workflow_event_typeid");

                    # get field widths
                    list fl = oload::get_widths(q, 2);
                    string fmt = "%" + sprintf("%dd ", fl[0]) + "%-" + sprintf("%dw ", fl[1]) + "%s\n";

                    context (q) {
                        if (o.verbose)
                            printf("%s: ", oload::fd(%modified));
                        f_printf(fmt, %workflow_event_typeid, %name, oload::get_str(%description));
                    }

                    continue;
                } else if (substr("groups", 0, len) == a) {
                    hash<auto> q = sqlif.omqp.select("select * from groups order by groupid");

                    # get field widths
                    list fl = oload::get_widths(q, 2);
                    string fmt = "%" + sprintf("%dd ", fl[0]) + "%-3s %-" + sprintf("%dw ", fl[1]) + "%s\n";

                    context (q) {
                        if (o.verbose)
                            printf("%s: ", oload::fd(%modified));
                        f_printf(fmt, %groupid, %enabled ? "ON" : "OFF", %name, oload::get_str(%description));
                        *hash<auto> sq = sqlif.omqp.select("select g.serviceid, version, name, description, autostart, manual_autostart from group_services g, services s where groupid = %v and g.serviceid = s.serviceid", int(%groupid));

                        context (sq) {
                            f_printf("  + service : %s/%s (%d) %s%s: %s\n", %name, %version, %serviceid, %autostart ? "A" : "-", %manual_autostart ? "*" : "-", oload::get_str(%description));
                        }

                        sq = sqlif.omqp.select("select g.workflowid, autostart, manual_autostart, deprecated, version, name, description from group_workflows g, workflows w where groupid = %v and g.workflowid = w.workflowid", int(%groupid));

                        context (sq) {
                            f_printf("  + workflow: %s/%s (%d) %s%s: %s\n", %name, %version, %workflowid, sprintf("A%02d", %autostart), %deprecated ? "X" : (%manual_autostart ? "*" : "-"), oload::get_str(%description));
                        }

                        sq = sqlif.omqp.select("select g.jobid, version, name, description from group_jobs g, jobs j where groupid = %v and g.jobid = j.jobid", int(%groupid));

                        context (sq) {
                            f_printf("  + job: %s/%s(%s) %s\n", %name, %version, %jobid, oload::get_str(%description));
                        }
                    }

                    continue;
                } else if (substr("functions", 0, len) == a) {
                    hash<auto> q = sqlif.omqp.select("select function_instanceid, function_type, version, patch, name, description, created, modified from function_instance order by function_instanceid");

                    # get field widths
                    list fl = oload::get_widths(q, 5);
                    # patch width cannot be less than 3
                    if (fl[3] < 3) fl[3] = 3;
                    string fmt = "%" + sprintf("%dd ", fl[0]) + "%-" + sprintf("%dw", fl[1] + 3)
                        + "v%-" + sprintf("%dw ", fl[2]) + "(patch %-" + sprintf("%dw) ", fl[3])
                        + "%-" + sprintf("%dw: ", fl[4]) + "%s\n";

                    context (q) {
                        if (o.verbose)
                            printf("%s: ", oload::fd(%modified));

                        f_printf(fmt, %function_instanceid, %function_type, %version, oload::get_str(%patch),
                                 %name, oload::get_str(%description));
                    }

                    continue;
                } else if (substr("jobs", 0, len) == a) {
                    hash<auto> q = sqlif.omqp.select("select jobid, name, version, description, "
                        "active, last_executed, expiry_date, minute, hour, day, month, wday, recurring, created, "
                        "remote, modified from jobs order by jobid");

                    # get field widths
                    list fl = oload::get_widths(q, 3);
                    string fmt = "%" + sprintf("%dd ", fl[0]) + "%s%s v%-" + sprintf("%dw ", fl[2]) + "%-" +
                        sprintf("%dw: ", fl[1]) + "%s %s\n";

                    context (q) {
                        string sched = %recurring
                            ? sprintf("(dur: %n)", seconds(%recurring))
                            : sprintf("(cron: %s %s %s %s %s)", %minute, %hour, %day, %month, %wday);

                        if (o.verbose) {
                            printf("%s: ", oload::fd(%modified));
                        }
                        f_printf(fmt, %jobid, %remote ? "R" : " ", %active ? "A" : " ",
                                 %version, %name, sched, oload::get_str(%description));
                    }

                    continue;
                } else if (substr("uconn", 0, len) == a) {
                    hash<auto> q = sqlif.omqp.select("select name, url, description, options, created, modified from connections order by created");

                    # get field widths
                    list fl = oload::get_widths(q, 3);
                    string fmt = sprintf("%%-%dw (%%s): %%s", fl[0]);

                    map f_printf(fmt + (o.verbose ? (" (created: " + $1.created.format("YYYY-MM-DD") + ")\n") : "\n"), $1.name, $1.url, $1.description), q.contextIterator();

                    continue;
                } else if (substr("steps", 0, len) == a) {
                    hash<auto> q = sqlif.omqp.select("select
                        stepid, s.name, s.version, s.patch, steptype, arraytype, c.language,
                        validationfunction_instanceid, step_classid,
                        s.modified,
                        q.name as queuename, e.name as eventname
                    from steps s
                        left join queues q on (s.queueid = q.queueid)
                        left join workflow_event_types e on (s.workflow_event_typeid = e.workflow_event_typeid)
                        left join classes c on (s.step_classid = c.classid)
                    order by stepid");

                    # get field widths
                    list<int> fl = oload::get_widths(q, 6);
                    if (fl[5] < 4) {
                        fl[5] = 4;
                    }
                    string fmt = sprintf("%%%dd %%s%%s %%-%dw %%-%dw v%%-%dw (patch %%-%dw) %%-%dw%%s\n", fl[0], fl[5], fl[4], fl[2], fl[3], fl[1]);

                    context (q) {
                        if (o.verbose) {
                            printf("%s: ", oload::fd(%modified));
                        }
                        *string other;
                        if (%queuename) {
                            other = sprintf(" (queue: %y)", %queuename);
                        } else if (%eventname) {
                            other = sprintf(" (event: %y)", %eventname);
                        }
                        bool has_val = %validationfunction_instanceid.toBool();
                        if (!has_val && %step_classid &&
                            (%steptype == ExecNormal || %steptype == ExecAsync)) {
                            has_val = True;
                        }
                        f_printf(fmt, %stepid, %arraytype == "NONE" ? " " : "A",
                            has_val ? "V" : " ",
                            %language ?? "qore",
                            %steptype, %version, oload::get_str(%patch), %name, other);
                    }

                    continue;
                } else if (substr("releases", 0, len) == a) {
                    showReleaseIntern(NOTHING, o.verbose);
                    continue;
                } else if (substr("mappers", 0, len) == a) {
                    hash<auto> q = sqlif.omqp.select("select mapperid, name, version, patch, type, description, created, modified from mappers");
                    list<auto> fl = oload::get_widths(q, 5);
                    string fmt = sprintf("%%%dd %%-%dw v%%-%dw (patch %%-%dw) %%-%dw\n", fl[0], fl[1], fl[2], fl[3], fl[4]);
                    context (q) {
                        if (o.verbose)
                            printf("%s: ", oload::fd(%modified));
                        f_printf(fmt, %mapperid, %name, %version, oload::get_str(%patch), %type);
                    }
                    continue;
                } else if (substr("types", 0, len) == a) {
                    context (sqlif.omqp.select("select path, typeinfo from data_types")) {
                        try {
                            AbstractDataProviderType type = Serializable::deserialize(%typeinfo);
                            printf("%s: %s\n", %path, type.getName());
                        } catch (hash<ExceptionInfo> ex) {
                            printf("%s: invalid type: %s: %s\n", %path, ex.err, ex.desc);
                        }
                    }
                    continue;
                } else if (substr("fsms", 0, len) == a) {
                    hash<auto> q = sqlif.omqp.select("select name, description, states, options, created, modified from fsm");
                    list<auto> fl = oload::get_widths(q, 1);
                    string fmt = sprintf("%%-%dw (States: %%2d Opts: %%d): %%s\n", fl[0]);
                    context (q) {
                        if (o.verbose)
                            printf("%s: ", oload::fd(%modified));
                        f_printf(fmt, %name, parse_yaml(%states).size(), parse_yaml(%options).size(), %description);
                    }
                    continue;
                } else if (substr("pipelines", 0, len) == a) {
                    hash<auto> q = sqlif.omqp.select("select name, description, created, modified from pipelines");
                    list<auto> fl = oload::get_widths(q, 1);
                    string fmt = sprintf("%%-%dw: %%s\n", fl[0]);
                    context (q) {
                        if (o.verbose)
                            printf("%s: ", oload::fd(%modified));
                        f_printf(fmt, %name, %description);
                    }
                    continue;
                }

                stderr.printf("ERROR: do not know how to list %y (try %s -h for more info)\n", a, get_script_name());
                exit(1);
            }
        }
    }

    private showReleases(softlist l) {
        map showReleaseIntern($1, o.verbose + 1), l;
    }

    private showReleaseIntern(*string rname, *softint verbose) {
        string sql = "select r.release_name, created, modified, file_count.ct file_count, component_count.ct component_count from releases r join (select release_name, count(1) as ct from release_files rf group by release_name) file_count on file_count.release_name = r.release_name join (select release_name, count(1) as ct from release_file_contents c group by release_name) component_count on component_count.release_name = r.release_name";
        if (rname)
            sql += " where r.release_name = %v";
        sql += " order by created";
        hash<auto> q = sqlif.omqp.select(sql, rname);

        if (!q.release_name && rname) {
            stderr.printf("ERROR: release %y does not exist\n", rname);
            return;
        }

        list fl = oload::get_widths(q, 1);
        string fmt = sprintf("%%-%ds: C: %%s M: %%s files: %%4d components: %%4d\n", fl[0]);

        *hash cq;
        *list cfl;
        *string cfmt;
        if (verbose) {
            hash<auto> tm = {
                "WORKFLOW": ("id": "workflowid", "table": "workflows"),
                "SERVICE": ("id": "serviceid", "table": "services"),
                "FUNCTION": ("id": "function_instanceid", "table": "function_instance"),
                "CLASS": ("id": "classid", "table": "classes"),
                "CONSTANT": ("id": "constantid", "table": "constants"),
                "JOB": ("id": "jobid", "table": "jobs"),
                "VALUE MAP": ("id": "id", "table": "value_maps"),
                "MAPPER": ("id": "mapperid", "table": "mappers"),
            };
            code get_type = string sub(string type) {
                *string rv = tm{type}.id;
                return rv ?? "n/a";
            };

            sql = "select component, component_version, component_id, file_type, c.release_name, c.file_name, c.hash_type, c.hash from release_file_contents c, release_files f where f.file_name = c.file_name";
            if (rname)
                sql += " and c.release_name = %v";
            sql += " order by c.release_name, c.file_name, component";
            cq = sqlif.omqp.select(sql, rname);
            cq.component_version = map $1 === NULL ? "n/a" : "v" + $1, cq.component_version;

            cq.component_id = map $1.component_id === NULL ? "n/a" : sprintf("(%s: %d)", get_type($1.file_type), $1.component_id), cq.contextIterator();

            cfl = oload::get_widths(cq, 4);
            cfmt = sprintf(" + %%-%ds: %%-%ds %%-%ds %%-%ds\n", cfl[3], cfl[0], cfl[1], cfl[2]);
        }

        context rel (q) {
            *date mod = %modified == %created ? NOTHING : %modified;
            f_printf(fmt, %release_name, oload::fd(%created), oload::fd(mod), %file_count, %component_count);
            if (verbose) {
                context (cq) where (%rel:release_name == %release_name) {
                    f_printf(cfmt, %file_type, %component, %component_version, %component_id);
                    if (verbose > 1)
                        f_printf(" + (%s)%s\n", %hash_type, %hash);
                }
            }
        }
    }

    static private string fd(date date) {
        return format_date("YYYY-MM-DD HH:mm:SS", date);
    }

    static private string fd(nothing n) {
        return sprintf("%-19s", "-");
    }

    registerObject(string type, string name, string ver, string fn) {
        if (exists omap{type}{name}{ver}) {
            stderr.printf("ERROR: %s %y v%s was already created from file %y; also defined in file %y\n", type, name, ver, omap{type}{name}{ver}, fn);
            exit(1);
        }
        omap{type}{name}{ver} = fn;
    }

    checkShutdown() {
        try {
            qrest.get("system/ping");
            stderr.printf("ERROR: Qorus is running; the system must be shutdown before this patch can be installed\n");
            exit(1);
        } catch (hash<ExceptionInfo> ex) {
            if (ex.err == "SOCKET-CONNECT-ERROR") {
                if (o.verbose)
                    printf("Qorus is not running, continuing with release\n");
                return;
            }
            # rethrow any other exception
            rethrow;
        }
    }

    private static processDel(reference v, list l, string type) {
        if (!l[0] || !l[1]) {
            stderr.printf("missing name and/or version for %s delete command: name=%n, ver=%n\n", type, l[0], l[1]);
            exit(1);
        }
        if (!v)
            v = ();
        v += {
            "name"    : l[0],
            "version" : l[1],
        };
    }

    *hash<auto> process_command_line() {
        GetOpt g(opts);

        if (!ENV.OLOAD_ARGS.empty())
            ARGV += ENV.OLOAD_ARGS;

        o = g.parse3(\ARGV);

        if (o.showdefines) {
            map printf("%s\n", $1), keys options.defines;
            exit(1);
        }

        # ensure we have a value here...
        if (!exists o.verbose) {
            o.verbose = 0;
        }
        if (!exists o.quiet) {
            o.quiet = False;
        }

        *hash del;

        if (o.help) {
            oload::usage();
        }

        if (o.quiet && o.verbose) {
            stderr.printf("ERROR: --quiet and --verbose cannot both be given on the command line\n");
            exit(1);
        }

        # check deletion list
        for (int i = 0; i < elements o.del; ++i) {
            list<string> l = split(":", o.del[i]);
            *string cmd = shift l;

            if (!strlen(l[0])) {
                oload::error("missing first argument for %s delete command", cmd);
            }

            if (cmd == "workflow") {
                oload::processDel(\del.workflow, l, "workflow");
            } else if (cmd == "wfinstances") {
                oload::processDel(\del.wfinstances, l, "wfinstances");
            } else if (cmd == "service") {
                oload::processDel(\del.service, l, "service");
            } else if (cmd =~ /^meth(od)?$/) {
                if (!strlen(l[0]) || !strlen(l[1]) || !strlen(l[2]))    {
                    oload::error("missing name and/or version for service method delete command: svc name=%y, ver=%y, method=%y\n",
                                  l[0], l[1], l[2]);
                }
                if (!exists del.method)
                    del.method = ();
                del.method += {
                    "name"   : l[0],
                    "version": l[1],
                    "method" : l[2],
                };
            } else if (cmd == "step") {
                oload::processDel(\del.step, l, "step");
            } else if (cmd =~ /^func(tion)?$/) {
                oload::processDel(\del.function, l, "function");
            } else if (cmd == "class") {
                oload::processDel(\del."class", l, "class");
            } else if (cmd =~ /^const(ant)?$/) {
                oload::processDel(\del.constant, l, "constant");
            } else if (cmd == "queue") {
                del.queue += ({"name": shift l},);
            } else if (cmd == "job") {
                del.job += ({"name": shift l},);
            } else if (cmd == "jobinstances") {
                del.jobinstances += ({"name": shift l},);
            } else if (cmd == "group") {
                del.group += ({"name": shift l},);
            } else if (cmd == "event") {
                del.event += ({"name": shift l},);
            } else if (cmd == "uconn") {
                del.uconn += ({"name": shift l},);
            } else if (cmd == "datasource") {
                del.datasource += ({"name": shift l},);
            } else if (cmd == "remote") {
                del.remote += ({"name": shift l},);
            } else if (cmd =~ /^map(per)?$/) {
                oload::processDel(\del.mapper, l, "mapper");
            } else if (cmd == "vmap") {
                del.vmap += ({"name": shift l},);
            } else if (cmd == "type") {
                del.type += ({"name": shift l},);
            } else if (cmd == "pipeline") {
                del.pipeline += ({"name": shift l},);
            } else if (cmd == "fsm") {
                del.fsm += ({"name": shift l},);
            } else {
                oload::error("unknown or missing object type: %y", cmd);
            }
        }

        # check deletion by id list
        *hash<auto> delete_id;
        foreach auto delete_arguments in (o{"delete-id"}) {
            list arguments = split(":", delete_arguments);
            *string type = shift arguments;
            if (!deleteByIdMethodsMap.hasKey(type)) {
                oload::error("unknown or missing object type: %y", type);
            }
            auto id = shift arguments;
            if (!id.intp()) {
                oload::error("given id %y cannot be converted to integer", id);
            }
            delete_id{type} += !exists delete_id{type} ? list(id.toInt()) : id.toInt();
        }

        # setup lists for potential load commands if necessary
        if (!exists o.schema) {
            o.schema = ();
        }

        foreach string str in (o.omqschema) {
            o.schema += ("arg": str, "dsname": "omq");
        }

        foreach string str in (o.userschema) {
            o.schema += ("arg": str, "dsname": "omquser");
        }

        # check tablespace arguments
        foreach string arg in (o.data_ts) {
            (*string ds, *string ts) = (arg =~ x/(.*)=(.*)/);
            if (!exists ts) {
                stderr.printf("invalid option to --data-ts, expected <datasource>=<tablespace>, got %y\n", arg);
                exit(1);
            }
            o.dts{ds} = ts;
        }
        foreach string arg in (o.index_ts) {
            (*string ds, *string ts) = (arg =~ x/(.*)=(.*)/);
            if (!exists ts) {
                stderr.printf("invalid option to --index-ts, expected <datasource>=<tablespace>, got %y\n", arg);
                exit(1);
            }
            o.its{ds} = ts;
        }

        foreach string ds in (o.ds) {
            omqclient.setDatasourceFromText(ds);
        }

        if (o.refresh || o.reload || exists del || exists delete_id) {
            refresh = True;
        }

        return (exists del || exists delete_id) ? {"delete": del, "delete-id": delete_id} : NOTHING;
    }

    static auto intOrNull(auto value) {
        if (!exists value || value == NULL) {
            return NULL;
        }
        return int(value);
    }

    private int processFile(string arg, *Release rel) {
%ifdef Windows
        arg =~ s/\//\\/g;
%endif

        # first process arguments with prefixes in the name
        if (arg =~ /^@/)
            return processLoadFile(normalize_dir(substr(arg, 1)));

        arg = normalize_dir(arg);
        if (processed{arg})
            return 0;

        processed{arg} = True;

        int errs = 0;

        if (arg =~ /\.py$/) {
            # ignore python sources - assume they are referenced from a YAML metadata file
        } else if (arg =~ /\.qrf$/) {
            errs += processLoadFile(arg);
        } else if (arg =~ /\.yaml$/) {
            yaml_sources += {"release": rel, "file": arg};
        } else if (arg =~ /\.qwf$/) {
            wflist += {"release": rel, "file": arg};
        } else if (arg =~ /\.qfd$/) {
            flist += {"release": rel, "file": arg};
        } else if (arg =~ /\.qsd(.java)?$/) {
            slist += {"release": rel, "file": arg};
        } else if (arg =~ /\.qjob(.java)?$/) {
            jlist += {"release": rel, "file": arg};
        } else if (arg =~ /\.qclass(.java)?$/) {
            classlist += {"release": rel, "file": arg};
        } else if (arg =~ /\.qconst$/) {
            constlist += {"release": rel, "file": arg};
        } else if (arg =~ /\.qconn$/) {
            connlist += {"release": rel, "file": arg};
        } else if (arg =~ /\.qsm$/) {
            qsmlist += {"release": rel, "file": arg};
        } else if (arg =~ /\.qmapper$/) {
            qmlist += {"release": rel, "file": arg};
        } else if (arg =~ /\.qvmap$/) {
            qvmaplist += {"release": rel, "file": arg};
        } else if (arg =~ /\.qscript$/) {
            if (arg =~ /-pre\.qscript$/) {
                qscripts.pre += {"release": rel, "file": arg};
            }
            else if (arg =~ /-post\.qscript$/) {
                qscripts.post += {"release": rel, "file": arg};
            }
            else if (arg =~ /-post_reload\.qscript$/) {
                qscripts.post_reload += {"release": rel, "file": arg};
            }
            # default scripts go to the 'post' phase
            else {
                qscripts.post += {"release": rel, "file": arg};
            }
        } else if (arg =~ /\.tar(\.[a-z0-9]+)?$/ || arg =~ /\.tgz$/ || arg =~ /\.tbz2$/) {
            qpkglist += {
                "file": arg,
            };
        } else if (arg !~ /~$/ && arg !~ /\.qtest$/ && arg !~ /\.java$/ && arg !~ /\.qstep$/
            && arg !~ /\.qmc$/) {
            printf("WARNING: unrecognized extension in file: %s\n", arg);
        }

        return errs;
    }

    # Find the correct location of the items located in
    # command file. Params:
    #  cmdFile: file name of the command line
    #  fileName: file name (item) taken from the cmdFile.
    # Names are checked for full/relative paths and physical
    # location of the files.
    # Absolute path for file is returned if the file exists (new behavior)
    # or relative path related to the current working dir (legacy behavior).
    private string locateRelativePath(string cmdFile, string fileName) {
        # find the common subpath to ensure the right relative path links
        string dir = dirname(cmdFile);
        # check for "relative to cmdFile location"
        if (is_readable(dir + DirSep + fileName))
            return dir + DirSep + fileName;
        else
            return fileName;
    }

    private int processLoadFile(string fn) {
        FileLineIterator i(fn);
        int errs = 0;

        # get release name
        string rname = basename(fn);
        rname =~ s/\.[a-z]+$//i;

        # create release and add to release list
        Release rel(rname);
        rlist += rel;

        while (i.next()) {
            string line = i.getValue();
            chomp line;
            if (line && line !~ /^#/) {
                # get command and argument
                (*string cmd, *string arg) = (line =~ x/([^ ]+) (.*)/);
                if (!cmd)
                    cmd = line;

                if (cmd == "load")
                    errs += doFile(rel, locateRelativePath(fn, arg), fn, i.index());
                else if (cmd == "omquser-exec-sql")
                    runSchemaFile("omquser", locateRelativePath(fn, arg));
                else if (cmd == "verify-schema")
                    verifySchema(arg);
                else if (cmd == "verify-empty-schema")
                    oload::verifyEmptySchema();
                else if (cmd == "verify-load-schema")
                    verifySchema(arg, "schema-load-compatibility");
                else if (cmd == "sign-schema")
                    printf("application signature no longer required\n");
                else if (cmd == "refresh-workflows")
                    refresh = True;
                else if (cmd == "refresh-services")
                    refresh = True;
                else if (cmd == "refresh-jobs")
                    refresh = True;
                else if (cmd == "refresh-all")
                    refresh = True;
                else if (cmd == "refresh-recursive") {
                    refresh = True;
                    o.validate = True;
                } else if (cmd == "check-shutdown")
                    o.checkShutdown = True;
                else
                    oload::error("%s:%d: unrecognized command %y", fn, i.index(), cmd);
            }
        }

        return errs;
    }

    private int doFile(Release rel, string arg, string fn, int c) {
        int errs = 0;

        *softlist files;
        # if there are wildcards, then expand them
        if (index(arg, "*") != -1 || index(arg, "?") != -1) {
            files = glob(arg);
            if (!files)
                oload::error("%s:%d: \"load\" pattern %y matches no files", fn, c, arg);
        }
        else
            files = arg;

        map errs += processFile($1, rel), files;
        return errs;
    }

    static error(string msg) {
        if (oload.o.showfullerror) {
            throw "ERROR", vsprintf(msg, argv);
        }
        stderr.printf("ERROR: %s\n", vsprintf(msg, argv));
        exit(1);
    }

    static warning(string msg) {
        stderr.printf("WARNING: %s\n", vsprintf(msg, argv));
    }

    static validateObjectName(string name) {
        try {
            QorusObjectParser::validate_object_name(name);
        } catch (hash<ExceptionInfo> exc) {
            if (exc.err == "INVALID-NAME") {
                oload::error(exc.desc);
            }
            rethrow;
        }
    }

    private:internal readCode(reference<hash> parsed_yaml, string code_file, string yaml_source) {
        if (parsed_yaml.typeCode() != NT_HASH || !exists parsed_yaml{"code"}) {
            return;
        }

        yaml_code_files{code_file} = yaml_source;

        parsed_yaml{"code"} = ReadOnlyFile::readTextFile(code_file);
    }

    private:internal string wrapFieldCode(string code_) {
        return "auto sub (auto ctx, auto record) { return " + code_ + "(ctx, record); }";
    }

    private:internal string wrapOptionCode(string code_, *list args) {
        string result = "" + code_ + "(";
        if (exists args) {
            map result += ($1.typeCode() == NT_STRING ? '"' + $1 + '"' : $1.toString()) + ",", args;
            result = substr(result, 0, result.length() - 1);
        }
        result += ")";
        return result;
    }

    private:internal wrapCode(reference<hash> parsed_yaml, string yaml_key, code wrap_function, bool args) {
        foreach string key in (keys parsed_yaml{yaml_key}) {
            if (parsed_yaml{yaml_key}{key}.typeCode() == NT_HASH &&
                parsed_yaml{yaml_key}{key}.hasKey("code")) {

                parsed_yaml{yaml_key}{key}{"code"} = wrap_function(parsed_yaml{yaml_key}{key}{"code"},
                                                                   args ? parsed_yaml{yaml_key}{key}{"args"} : NOTHING
                                                                   );
                remove parsed_yaml{yaml_key}{key}{"args"};
            }
        }
    }

    private:internal prepareMapperYaml(reference<hash> parsed_yaml) {
        parsed_yaml{"is_yaml"} = True;
        if (parsed_yaml."parse-options") {
            parsed_yaml{"parse_options"} = parsed_yaml{"parse-options"}.join(",");
        }
        # do not wrap field code

        if (parsed_yaml.hasKey("options")) {
            wrapCode(\parsed_yaml, "options", \wrapOptionCode(), True);
            wrapCode(\parsed_yaml{"options"}, "runtime", \wrapOptionCode(), True);
        }
    }

    private:internal serializeYamlTags(reference<hash> parsed_yaml, list<string> tags) {
        foreach string tag in (tags) {
            if (exists parsed_yaml{tag}) {
                parsed_yaml{tag + "_deserialized"} = parsed_yaml{tag};
                parsed_yaml{tag} = UserApi::serializeQorusDataWithNothing(parsed_yaml{tag});
            }
        }
    }

    private:internal hash parseAndValidateYaml(softlist yaml_sources) {
        # all parsed objects where key is object type and value is a list of objects
        hash result = {};

        foreach auto yaml_source in (yaml_sources) {
            hash parsed_yaml;
            try {
                parsed_yaml = Qyaml::parse_yaml_file(yaml_source{"file"});
            } catch (hash<ExceptionInfo> ex) {
                oload.error("%y: %s: %s", yaml_source.file, ex.err, ex.desc);
            }
            *string type = parsed_yaml{"type"};
            if (!exists type) {
                oload.warning("%s: no object type is provided in YAML file; ignoring", yaml_source{"file"});
                continue;
            }

            if (!Qyaml::KnownObjects.hasKey(type)) {
                oload.error("%s: unknown object type: %s in YAML file", yaml_source{"file"}, type);
            }

            Qyaml::Validator validator = Qyaml::create_validator(type);

            # this is workaround of https://github.com/qorelanguage/qore/issues/3417
            Qyaml::ParsedYaml parsed_yaml_object(parsed_yaml);

            try {
                validator.validate(yaml_source.file, parsed_yaml_object);
            } catch (hash<ExceptionInfo> ex) {
                oload.error("%s: %s: %s", yaml_source{"file"}, ex.err, ex.desc);
            }
            parsed_yaml = parsed_yaml_object.data_;

            string directory = dirname(yaml_source{"file"}) + Qore::DirSep;
            string code_file = directory + parsed_yaml{"code"};
            parsed_yaml{"tags"} += {
                "_source": code_file,
                "_offset": 0,
                "_host": gethostname(),
                "_user": getusername()
            };
            if (parsed_yaml{"base-class-name"}) {
                parsed_yaml{"tags"} += {
                    "_base_class_name": parsed_yaml{"base-class-name"}
                };
            }
            readCode(\parsed_yaml, code_file, yaml_source{"file"});

            # assume every object as class-based (will be used only for services and jobs)
            parsed_yaml{"class-based"} = True;

            if (exists parsed_yaml{"author"}) {
                parsed_yaml{"author"} = parsed_yaml{"author"}.join("; ");
            }

            if (type == "mapper") {
                prepareMapperYaml(\parsed_yaml);
            } else if (type == "service") {
                foreach auto method in (\parsed_yaml{"methods"}) {
                    if (exists method{"author"}) {
                        method{"author"} = method{"author"}.join("; ");
                    }
                }

                if (parsed_yaml{"define-auth-label"}) {
                    parsed_yaml{"authlabels"} = parsed_yaml{"define-auth-label"};
                }
            }

            # fill type tag (e.g. for services type will be filled from servicetype)
            # if present otherwise remove type tag
            if (exists parsed_yaml{type + "type"}) {
                parsed_yaml{"type"} = parsed_yaml{type + "type"};
            } else {
                delete parsed_yaml{"type"};
            }

            # serialize config-items, class-connectors, processor tags if present so they can be stored in the DB
            serializeYamlTags(\parsed_yaml, ("config-items", "class-connectors", "processor"));

            if (!result{type}) {
                result{type} = list();
            }
            result{type} += parsed_yaml + {"yaml_source": yaml_source};
        }

        return result;
    }

    private:internal insertObjectsFromYaml(DbInserter inserter, string object_type, *list<auto> objects,
            *string name) {
        inserter.insertObjects(object_type, objects, name);
    }

    private:internal createObjectsFromYaml(hash<auto> yaml_definitions) {
        if (yaml_definitions.group) {
            if (!o.quiet && !o.verbose) {
                printf("processing interface groups: ");
            }
            foreach auto group in (yaml_definitions{"group"}) {
                if (!o.quiet) {
                    if (o.verbose) {
                        printf("%y: ", group{"yaml_source"}{"file"});
                    }
                }
                createGroupInternal(group{"name"}, group{"desc"});
            }
            if (!o.verbose && !o.quiet) {
                printf(" OK\n");
            }
        }

        insertObjectsFromYaml(new TypeDbInserter(), "type", yaml_definitions.type);
        insertObjectsFromYaml(new FunctionDbInserter(), "function", yaml_definitions{"function"});
        insertObjectsFromYaml(new ConstantDbInserter(), "constant", yaml_definitions{"constant"});

        getMaps();

        insertObjectsFromYaml(new ClassDbInserter(), "class", yaml_definitions{"class"});
        insertObjectsFromYaml(new ClassDbInserter(), "class", yaml_definitions{"mapper-code"}, "mapper class");

        # issue #3655: must process mappers before pipelines and FSMs
        insertObjectsFromYaml(new ValueMapDbInserter(), "value map", yaml_definitions{"value-map"});
        insertObjectsFromYaml(new MapperDbInserter(), "mapper", yaml_definitions{"mapper"});

        insertObjectsFromYaml(new PipelineDbInserter(), "pipeline", yaml_definitions{"pipeline"});

        {
            # commit/rollback all FSM only after dependencies are checked (one FSM can trigger another)
            AbstractTable fsm_table = oload::getTable("fsm");
            on_success FsmDbInserter::commit(fsm_table);
            on_error FsmDbInserter::error(fsm_table);
            insertObjectsFromYaml(new FsmDbInserter(), "fsm", yaml_definitions{"fsm"});
            checkFsmDependencies();
        }

        map createConnectionFromYaml($1), yaml_definitions{"connection"};

        if (yaml_definitions.queue) {
            if (!o.quiet && !o.verbose) {
                printf("processing async queues: ");
            }
            map createQueue($1, True), yaml_definitions{"queue"};
            if (!o.verbose && !o.quiet) {
                printf(" OK\n");
            }
        }
        if (yaml_definitions.event) {
            if (!o.quiet && !o.verbose) {
                printf("processing sync events: ");
            }
            map createEvent($1, True), yaml_definitions{"event"};
            if (!o.verbose && !o.quiet) {
                printf(" OK\n");
            }
        }

        insertObjectsFromYaml(new JobDbInserter(), "job", yaml_definitions{"job"});
        insertObjectsFromYaml(new ServiceDbInserter(), "service", yaml_definitions{"service"});
        insertObjectsFromYaml(new StepDbInserter(), "step", yaml_definitions{"step"});

        if (yaml_definitions.workflow) {
            if (!o.quiet && !o.verbose) {
                printf("processing workflows: ");
            }
            map createWorkflowYaml($1, $1{"yaml_source"}{"file"},
                                addRelease("workflow", $1{"yaml_source"}{"file"}, $1{"yaml_source"}{"release"})),
                yaml_definitions{"workflow"};
            if (!o.verbose && !o.quiet) {
                printf(" OK\n");
            }
        }
    }

    private:internal checkFsmDependencies() {
        foreach string fsm in (keys fsm_dependencies) {
            # (table, object_type, object_name, dependent object type, dependent object name)
            DbInserter::checkLibraryObject("fsm", "fsm", fsm, "fsm", fsm_dependencies{fsm});
        }
    }

    private:internal createServices(ServiceDbInserter inserter, string file_name, *Release release) {
        if (isFileReferencedByYAML(file_name)) {
            return;
        }

        *ReleaseFile release_file = addRelease("service", file_name, release);

        if (!oload.o.quiet && oload.o.verbose) {
            printf("%y: ", file_name);
        }

        # we have to pass a service to the DbInserter together with its methods

        list<QorusObject> qorus_objects = qorusObjectParser.parse(file_name);
        # Qorus Object Parser returns service as a last element
        QorusObject service = pop qorus_objects;
        hash<auto> tags = service.getTags();

        if (tags{"author"}.typeCode() == NT_LIST) {
            tags{"author"} = tags{"author"}.join("; ");
        }

        tags{"methods"} = ();
        foreach auto qorus_object in (qorus_objects) {
            hash method_tags = qorus_object.getTags();
            if (method_tags{"author"}.typeCode() == NT_LIST) {
                method_tags{"author"} = method_tags{"author"}.join("; ");
            }
            tags{"methods"} += method_tags + {"user_tags": qorus_object.getUserTags()};
        }

        hash<auto> ignored;
        code post_insert = sub (hash<auto> tags, softint id, string file_name) {
            if (tags.definedGroups) {
                oload.createGroups(tags.definedGroups, !oload.o.verbose);
            }

            if (tags.groups) {
                hash<string, hash<auto>> service_with_groups;
                service_with_groups{id} = {
                    "type": service.getType(),
                    "name": tags.name,
                    "version": tags.version,
                    "groups": tags.groups,
                };

                processGroups(service_with_groups, file_name, !oload.o.verbose);
            }
        };
        inserter.insert(\ignored, tags, service.getUserTags(), file_name, post_insert, release_file);
    }

    private:internal installPackage(string file) {
        file = normalize_dir(file);

        # create temporary directory
        TmpDir dir("qorus-oload");

        try {
            if (chdir(dir.path)) {
                throw "INSTALL-ERROR", sprintf("%s: %s", dir.path, strerror());
            }

            # untar file in dir
            int rc = system("tar xf \"" + file + "\"");
            if (rc) {
                throw "UNTAR-ERROR", "error extracting archive";
            }

            # check for single directory
            list<string> l = dir.listDirs();
            if (l.size() != 1) {
                throw "INSTALL-ERROR", sprintf("expected single directory in install package; got %y", l);
            }
            chdir(dir.path + DirSep + l[0]);

            # check for install.sh
            if (!is_file("./install.sh")) {
                throw "INSTALL-ERROR", "package does not contain install.sh";
            }
            if (!is_executable("./install.sh")) {
                chmod("./install.sh", 0755);
            }

            # calculate args
            string cmd = "./install.sh";

            {
                hash<auto> opts;
                if (o.redef) {
                    opts.A = True;
                }
                if (o.noerrupdate) {
                    opts.E = True;
                }
                if (o.full) {
                    opts.F = True;
                }
                if (o.override) {
                    opts.O = True;
                }
                if (o.reload) {
                    opts.R = True;
                }
                if (o.usesched) {
                    opts.S = True;
                }
                if (o.force) {
                    opts.f = True;
                }
                if (o.validate) {
                    opts.l = True;
                }
                if (o.quiet) {
                    opts.q = True;
                }
                if (opts) {
                    cmd += " -" + (foldl $1 + $2, keys opts);
                }
            }
            if (o.data_ts) {
                cmd += sprintf(" --data-ts=%s", o.data_ts);
            }
            if (o.index_ts) {
                cmd += sprintf(" --index-ts=%s", o.data_ts);
            }
            if (o.proxy) {
                cmd += sprintf(" -p=%s", o.proxy);
            }
            if (o.refresh) {
                cmd += sprintf(" -r=%s", o.proxy);
            }
            if (o.schema) {
                map cmd += sprintf(" --omqschema=%s", $1), o.omqschema;
            }
            if (o.token) {
                cmd += sprintf(" -t=%s", o.token);
            }
            if (o.verbose) {
                cmd += sprintf(" -%s", strmul("v", o.verbose));
            }

            # run install.sh
            if (PlatformOS != "Windows") {
                cmd += " 2>&1";
            }
            string output = backquote(cmd, \rc);
            if (rc) {
                throw "UNTAR-ERROR", sprintf("error executing installation script with cmd %y: %s", cmd, output);
            }
        } catch (hash<ExceptionInfo> ex) {
            throw ex.err, sprintf("%s: %s", file, ex.desc), ex.arg;
        }
    }

    private:internal bool isFileReferencedByYAML(string file_name) {
        if (!yaml_code_files.hasKey(file_name)) {
            return False;
        }
        return True;
    }

    private:internal createObjects(DbInserter inserter, list<auto> obj_list, string object_type) {
        hash<string, hash<auto>> ohash;

        foreach hash<auto> obj in (obj_list) {
            string file_name = obj.file;
            if (isFileReferencedByYAML(file_name)) {
                continue;
            }

            list<QorusObject> qorus_objects = qorusObjectParser.parse(file_name);
            *ReleaseFile release_file = addRelease(object_type, file_name, obj.release);

            ohash += inserter.makeHash(map $1.getTags() + {
                "tags": $1.getUserTags(),
                "obj": $1,
                "file_name": file_name,
                "release_file": release_file,
            }, qorus_objects);
        }

        # file_name -> id -> info
        hash<string, hash<string, hash<auto>>> objects_with_groups = {};
        hash<auto> groups = {};

        while (ohash) {
            string file_name;
            QorusObject qorus_object;
            *ReleaseFile release_file;
            {
                hash<auto> h = remove ohash{ohash.firstKey()};
                file_name = h.file_name;
                release_file = h.release_file;
                qorus_object = h.obj;
            }

            hash<auto> tags = qorus_object.getTags();
            string type = qorus_object.getType();

            if (tags{"author"}.typeCode() == NT_LIST) {
                tags{"author"} = tags{"author"}.join("; ");
            }

            code post_insert = sub (hash<auto> tags, softstring id, string file_name) {
                if (tags.groups) {
                    objects_with_groups{file_name}{id} = {
                        "type": qorus_object.getType(),
                        "name": tags.name,
                        "version": tags.version,
                        "groups": tags.groups,
                    };
                }
                if (tags{"definedGroups"}) {
                    groups += tags{"definedGroups"};
                }
            };

            inserter.insert(\ohash, tags + {"object_type": type}, qorus_object.getUserTags(), file_name,
                post_insert, release_file);
        }

        if (groups) {
            oload.createGroups(groups, !oload.o.verbose);
        }

        map processGroups($1.value, $1.key, !oload.o.verbose), objects_with_groups.pairIterator();
    }

    *ReleaseFile addRelease(string type, string file_name, *Release release) {
        *ReleaseFile release_file;
        if (release) {
            release_file = release.add(file_name, type.upr(), True, ReadOnlyFile::readTextFile(file_name));
        }
        return release_file;
    }

    processGroups(hash objects_with_groups, string file_name, bool quiet = False) {
        foreach auto id in (keys objects_with_groups) {
            hash object_with_groups = objects_with_groups{id};
            string object_type = object_with_groups.type;

            string desc = sprintf("%s %y%s%s", object_type, object_with_groups.name,
                (object_with_groups.version
                    ? sprintf("/%s", object_with_groups.version)
                    : ""
                ),
                (id && id != object_with_groups.name
                    ? sprintf(" (%s)", id)
                    : ""
                )
            );

            # generate delta list and update groups
            if (ObjectTypeToGroupHashName{object_type}) {
                # value maps have id instead of '<type>id' column
                string group_key;
                if (object_type == "fsm" || object_type == "pipeline") {
                    group_key = object_type;
                } else {
                    group_key = (object_type == "value map") ? "id" : object_type + "id";
                    id = id.toInt();
                }

                list result = oload.analyzeGroups(object_with_groups.groups, desc, group_key, id,
                                                  ObjectTypeToGroupHashName{object_type});
                oload.updateGroups(object_type, file_name, result, quiet);
            }
        }
    }

    createConnectionFromYaml(hash connection_yaml) {
        if (!ucf) {
            ucf = new UserConnectionFileHelper(sqlif.omqp);
        }

        # issue #3609: Qorus does not accept valid options in YAML connection files
        foreach hash<auto> i in (connection_yaml.options.pairIterator()) {
            if (i.value.type.typeCode() == NT_STRING) {
                connection_yaml.options{i.key} = i.value.value;
            }
        }

        AbstractConnection connection = ucf.newConnection(connection_yaml{"name"},
                                                          connection_yaml{"desc"},
                                                          connection_yaml{"url"}, {},
                                                          connection_yaml{"options"});


        *ReleaseFile release_file = addRelease("connection", connection_yaml{"yaml_source"}{"file"},
                                               connection_yaml{"yaml_source"}{"release"});

        if (!insertConnection(connection, connection_yaml{"yaml_source"}{"file"}, release_file)) {
            string msg = sprintf("WARNING: ignoring definition for %y in file; already manually updated in DB",
                                 connection_yaml{"name"});
            if (o.verbose) {
                printf("%s\n", msg);
            } else {
                printf(" %s ", msg);
            }
        }
    }

    createConnections(hash conh) {
        string file_name = normalize_dir(conh.file);

        if (!ucf) {
            ucf = new UserConnectionFileHelper(sqlif.omqp);
        }

        ucf.parse(file_name);

        *hash ch = ucf.getInfo();

        if (!ch) {
            oload::error("%s: contained no user connection definitions", file_name);
        }

        #printf("got connections: %N\n", ch);

        if (!o.quiet && o.verbose) {
            printf("%y: ", file_name);
        }

        *ReleaseFile release_file;
        if (conh{"release"}) {
            release_file = conh{"release"}.add(file_name, "CONNECTION", True, ReadOnlyFile::readTextFile(file_name));
        }

        insertConnections(file_name, release_file);
    }

    private:internal *list insertConnections(string file_name, *ReleaseFile release_file) {
        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        foreach AbstractConnection connection in (ucf.getDataIterator()) {
            if (insertConnection(connection, file_name, release_file)) {
                continue;
            }

            string msg = sprintf("WARNING: ignoring definition for %y in file; already manually updated in DB",
                                 connection.name);
            if (!o.quiet) {
                if (o.verbose) {
                    printf("%s\n", msg);
                } else {
                    printf(" %s ", msg);
                }
            }
        }
    }

    private:internal bool insertConnection(AbstractConnection connection, string file_name,
                                           *ReleaseFile release_file) {
        # see if connection already exists
        *hash<auto> q = sqlif.omqp.selectRow("select manually_updated, connectionid from connections where name = %v "
            "and connection_type = %v", connection.name, ucf.getQorusConnectionType(connection));
        if (release_file) {
            release_file.add(connection.name, sprintf("%y", connection.getConfigHash()));
        }

        if (o.verbose) {
            printf("%s: %s conn %y: ", basename(file_name), ucf.getQorusConnectionType(connection),
                   connection.name);
        }

        if (q) {
            connection.internal_info.id = q.connectionid;
            if (q.manually_updated) {
                if (o.override) {
                    if (o.verbose) {
                        printf("WARNING: overriding manually updated configuration in DB\n");
                    } else {
                        mystat("O");
                    }
                } else {
                    return False;
                }
            }
            if (o.verbose) {
                printf("updating: url: %y desc: %y opts: %y\n", connection.url, connection.desc, connection.getRealOptions());
            } else {
                mystat("U");
            }

            ucf.updateConnection(connection, False);
        } else {
            if (o.verbose) {
                printf("inserting: url: %y desc: %y opts: %y\n", connection.url, connection.desc, connection.getRealOptions());
            } else {
                mystat("I");
            }

            ucf.insertConnection(connection);
        }

        if (connection instanceof DatasourceConnection) {
            reload_connections.datasources = True;
        } else if (connection instanceof QorusHttpConnection) {
            # here is the URL transformed from qorus:// to http:// so we need real protocol instead
            reload_connections.qorus = True;
        } else {
            reload_connections.user = True;
        }

        return True;
    }

    alignUserSchema(hash sh) {
        string fn = normalize_dir(sh.file);
        printf("loading qsm file: %s\n", fn);
        UserSchemaHelper schema(fn);

        int cc = schema.align(o.force, o.verbose);
        # if there are any changes, then the SQL cache in Qorus is reset
        if (cc) {
            printf("%s: user schema %y is at version %s\n", schema.getDesc(), fn, schema.getVersion());
            if (o.verbose) {
                printf(" - resetting sql cache for %s: ", schema.getDesc());
                flush();
            }
            softstring ret;
            try {
                ret = schema.resetSqlCache();
            } catch (hash<ExceptionInfo> ex) {
                # ignore connection refused errors (system down), otherwise output a warning msg
                if (ex.err != "SOCKET-CONNECT-ERROR")
                    oload::warning("failed to reset SQL cache for %s: %s: %s", schema.getDesc(), ex.err, ex.desc);
            }
            if (o.verbose)
                printf("%s\n", ret);
        }

        if (sh{"release"})
            sh{"release"}.add(fn, "SCHEMA", False, ReadOnlyFile::readTextFile(fn)).add(schema.getName(), schema.getSource(), schema.getVersion());
    }

    runQscript(string label, hash<auto> qs) {
        string fn = normalize_dir(qs.file);
        printf("Running custom script (%s): %s\n", label, fn);
        string src = ReadOnlyFile::readTextFile(fn);
        if (src =~ /^#!/ && src !~ /qore/) {
            # execute using hashbang instead
            if (!((int mode = hstat(fn).mode) & 0100)) {
                chmod(fn, mode | 0100);
            }
            int rc = system(fn);
            if (rc) {
                throw "QSCRIPT-ERROR", sprintf("qscript %y returned error code %d", fn, rc);
            }
        } else {
            Program pgm();

            # defines from options
            HashIterator it(options.defines);
            while (it.next()) {
                pgm.define(it.getKey(), it.getValue());
            }

            *hash<auto> r = pgm.parse(src, basename(fn), Qore::WARN_ALL, fn);
            while (r) {
                printf("warning: %s:%d: %s: %s\n", r.file, r.line, r.err, r.desc);
                r = r.next;
            }

            pgm.run();
        }

        if (qs{"release"}) {
            qs{"release"}.add(fn, "QSCRIPT", False, src);
        }
    }

    *int doWorkflow(hash<auto> wfh) {
        if (isFileReferencedByYAML(wfh.file)) {
            return;
        }

        string wf = normalize_dir(wfh.file);

        File f();
        if (f.open(wf)) {
            printf("%s: %s\n", wf, strerror());
            return -1;
        }

        *string buf = f.read(-1);
        Program p;
        if (!strlen(buf)) {
            stderr.printf("%s: empty workflow definition file\n", wf);
            return -1;
        }

        # bug 990: convert \r\n -> \n in workflow files
        buf = replace(buf, "\r\n", "\n");

        p = new Program();
        # ensure old style for embedded Programs
        p.disableParseOptions(PO_NEW_STYLE);
        # ignore warnings while parsing this file
        p.parse(buf, wf);
        p.run();

        *ReleaseFile rf;
        if (wfh{"release"}) {
            rf = wfh{"release"}.add(wf, "WORKFLOW", True, buf);
        }

        createWorkflowQore(p, wf, rf);
    }

    # issue #2776: disable any interfaces about to be deleted
    static bool disableInterface(string interface, int id, *string name, *string version) {
        string desc = sprintf("%s%s%s (%d)", interface,
                                             name ? sprintf(" %s", name) : "",
                                             version ? sprintf(" v%s", version) : "",
                                             id);
        try {
            qrest.put(interface + "s/" + id + "/disable");
            printf("disabled %s before deleting\n", desc);
            return True;
        } catch (hash<ExceptionInfo> ex) {
            # issue #3435: ignore if object does not exist in the server
            if (ex.arg.status_code == 404) {
                return False;
            }
            # ignore errors if the system is down
            if (ex.err != "SOCKET-CONNECT-ERROR") {
                rethrow;
            }
            if (oload.o.verbose) {
                printf("could not disable %s before deleting; server is not responding, assuming down\n", desc);
            }
        }
        return False;
    }

    static bool enableInterface(string interface, int id, *string name, *string version) {
        string desc = sprintf("%s%s%s (%d)", interface,
                                             name ? sprintf(" %s", name) : "",
                                             version ? sprintf(" v%s", version) : "",
                                             id);
        try {
            qrest.put(interface + "s/" + id + "/enable");
            printf("reenabled %s after deletion operation\n", desc);
            return True;
        } catch (hash<ExceptionInfo> ex) {
            # issue #3435: ignore if object does not exist in the server
            if (ex.arg.status_code == 404) {
                return False;
            }
            # ignore errors if the system is down
            if (ex.err != "SOCKET-CONNECT-ERROR") {
                rethrow;
            }
            if (oload.o.verbose) {
                printf("could not enable %s after deleting; server is not responding, assuming down\n", desc);
            }
        }
        return False;
    }

    static deleteWorkflowInstances(int id, string name, string version) {
        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        printf("deleting workflow instances %s:%s (workflowid %d)\n", name, version, id);

        int rows;

        # fix for bug 480: cannot delete a workflow if audit event references exist for the workflow's orders
        # clear audit events referencing audit events about to be deleted
        # use alternate approach for mysql, as mysql has a weird limitation on the from clause in subqueries
        if (sqlif.omqp.getDriverName() == "mysql") {
            sqlif.omqp.exec("create temporary table x_ae_ids as select audit_eventid from audit_events where workflowid = %d and workflow_instanceid is not null", id);
            rows = sqlif.omqp.exec("update audit_events set related_audit_eventid = null where related_audit_eventid in (select audit_eventid from x_ae_ids)");
            sqlif.omqp.exec("drop table x_ae_ids");
        } else {
            rows = sqlif.omqp.exec("update audit_events set related_audit_eventid = null where related_audit_eventid in (select audit_eventid from audit_events where workflowid = %v and workflow_instanceid is not null)", id);
        }
        if (rows) {
            printf("+ %d related audit event row(s) updated\n", rows);
        }
        # delete audit events
        rows = sqlif.omqp.exec("delete from audit_events where workflowid = %v and workflow_instanceid is not null", id);
        if (rows) {
            printf("+ %d audit event row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from error_instance where workflow_instanceid in (select workflow_instanceid from workflow_instance where workflowid = %v)", id);
        if (rows) {
            printf("+ %d error_instance row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from queue_data where workflow_instanceid in (select workflow_instanceid from workflow_instance where workflowid = %v)", id);
        if (rows) {
            printf("+ %d queue_data row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from subworkflow_instance where subworkflow_instanceid in (select workflow_instanceid from workflow_instance where workflowid = %v) or workflow_instanceid in (select workflow_instanceid from workflow_instance where workflowid = %v)", id, id);
        if (rows) {
            printf("+ %d subworkflow_instance row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from step_instance_events where workflow_instanceid in (select workflow_instanceid from workflow_instance where workflowid = %v)", id);
        if (rows) {
            printf("+ %d step_instance_events row(s) deleted\n", rows);
        }
        # issue #2880: must delete step instance data
        rows = sqlif.omqp.exec("delete from step_instance_data where workflow_instanceid in (select "
            "workflow_instanceid from workflow_instance where workflowid = %v)", id);
        if (rows) {
            printf("+ %d step_instance_data row(s) deleted\n", rows);
        }

        rows = sqlif.omqp.exec("delete from step_instance where workflow_instanceid in (select workflow_instanceid from workflow_instance where workflowid = %v)", id);
        if (rows) {
            printf("+ %d step_instance row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from segment_instance where workflowid = %v", id);
        if (rows) {
            printf("+ %d segment_instance row(s) deleted\n", rows);
        }
        # use alternate approach for mysql, as mysql has a weird limitation on the from clause in subqueries
        if (sqlif.omqp.getDriverName() == "mysql") {
            sqlif.omqp.exec("create temporary table x_wf_ids as select workflow_instanceid from workflow_instance where workflowid = %d", id);
            rows = sqlif.omqp.exec("update workflow_instance set parent_workflow_instanceid = null where parent_workflow_instanceid in (select workflow_instanceid from x_wf_ids)");
            sqlif.omqp.exec("drop table x_wf_ids");
        }
        else {
            rows = sqlif.omqp.exec("update workflow_instance set parent_workflow_instanceid = null where parent_workflow_instanceid in (select workflow_instanceid from workflow_instance where workflowid = %v)", id);
        }

        if (rows) {
            printf("+ %d workflow_instance row(s) cleared of the parent_workflow_instance pointer\n", rows);
        }
        # delete all order data rows
        rows = sqlif.omqp.exec("delete from order_instance_keys where workflowid = %v", id);
        if (rows) {
            printf("+ %d order instance key row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from order_instance where workflow_instanceid in (select workflow_instanceid from workflow_instance where workflowid = %v)", id);
        if (rows) {
            printf("+ %d order instance row(s) deleted\n", rows);
        }
        # bug 828: cannot delete a workflow if it has feedback events
        rows = sqlif.omqp.exec("delete from workflow_feedback where workflow_instanceid in (select workflow_instanceid from workflow_instance where workflowid = %v)", id);
        if (rows) {
            printf("+ %d workflow_feedback row(s) deleted\n", rows);
        }
        # bug 1235: oload cannot delete workflow order instances with order notes
        rows = sqlif.omqp.exec("delete from order_instance_notes where workflow_instanceid in (select workflow_instanceid from workflow_instance where workflowid = %v)", id);
        if (rows) {
            printf("+ %d order instance notes row(s) deleted\n", rows);
        }
        # issue #3008: delete sensitive order data keys
        rows = sqlif.omqp.exec("delete from sensitive_order_data_keys where workflow_instanceid in (select "
            "workflow_instanceid from workflow_instance where workflowid = %v)", id);
        if (rows) {
            printf("+ %d sensitive order data keys row(s) deleted\n", rows);
        }

        # issue 1407: delete sensitive order data
        rows = sqlif.omqp.exec("delete from sensitive_order_data where workflow_instanceid in (select workflow_instanceid from workflow_instance where workflowid = %v)", id);
        if (rows) {
            printf("+ %d sensitive order data row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from workflow_instance where workflowid = %v", id);
        if (rows) {
            printf("+ %d workflow_instance row(s) deleted\n", rows);
        }
        printf("+ workflow instances deleted from the database\n");
    }

    static deleteWorkflowInstances(hash<auto> workflow) {
        *softint id = sqlif.omqp.selectRow("select workflowid from workflows where name = %v and version = %v",
                                           string(workflow.name), string(workflow.version)).workflowid;
        if (!id) {
            throw "UNKNOWN-WORKFLOW", sprintf("%s:%s", workflow.name, workflow.version);
        }
        # issue #2776: disable any interfaces about to be deleted
        bool disabled = oload::disableInterface("workflow", id, workflow.name);
        on_exit if (disabled) {
            oload::enableInterface("workflow", id, workflow.name);
        }
        oload::deleteWorkflowInstances(id, workflow.name, workflow.version);
    }

    static deleteWorkflowInstances(int id) {
        *hash<auto> workflow = sqlif.omqp.selectRow("select name, version from workflows where workflowid = %v", id);
        if (!workflow) {
            throw "UNKNOWN-WORKFLOW", sprintf("workflow with id %d does not exist", id);
        }
        # issue #2776: disable any interfaces about to be deleted
        bool disabled = oload::disableInterface("workflow", id, workflow.name);
        on_exit if (disabled) {
            oload::enableInterface("workflow", id, workflow.name);
        }
        oload::deleteWorkflowInstances(id, workflow.name, workflow.version);
    }

    static deleteWorkflow(int id, string name, string version) {
        # FIXME: deletion APIs must be implemented in the server itself

        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        printf("deleting workflow %s:%s (workflowid %d)\n", name, version, id);

        InterfaceConfigContainer::deleteAllWorkflowConfigItemValues(id);

        int rows = sqlif.omqp.exec("delete from custom_statuses where workflowid = %v", id);
        if (rows) {
            printf("+ %d custom_statuses row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from workflow_options where workflowid = %v", id);
        if (rows) {
            printf("+ %d workflow_options row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from workflow_lib where workflowid = %v", id);
        if (rows) {
            printf("+ %d workflow_lib row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from workflow_mappers where workflowid = %v", id);
        if (rows) {
            printf("+ %d workflow_mappers row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from workflow_vmaps where workflowid = %v", id);
        if (rows) {
            printf("+ %d workflow_vmaps row(s) deleted\n", rows);
        }
        #printf("%s\n", sql);
        rows = sqlif.omqp.exec("delete from workflow_steps where workflowid = %v", id);
        if (rows) {
            printf("+ %d workflow_steps row(s) deleted\n", rows);
        }
        #printf("%s\n", sql);
        rows = sqlif.omqp.exec("delete from segment_dependencies where workflowid = %v", id);
        if (rows) {
            printf("+ %d segment_dependencies row(s) deleted\n", rows);
        }
        #printf("%s\n", sql);
        rows = sqlif.omqp.exec("delete from segment_steps where workflowid = %v", id);
        if (rows) {
            printf("+ %d segment_steps row(s) deleted\n", rows);
        }
        #printf("%s\n", sql);
        rows = sqlif.omqp.exec("delete from segment_async_link where workflowid = %v", id);
        if (rows) {
            printf("+ %d segment_async_link row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from workflow_keys where workflowid = %v", id);
        if (rows) {
            printf("+ %d workflow_keys row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from group_workflows where workflowid = %v", id);
        if (rows) {
            printf("+ %d group_workflows row(s) deleted\n", rows);
        }
        # fix for bug 480: cannot delete a workflow if audit event references exist for the workflow's orders
        # clear audit events referencing audit events about to be deleted
        # use alternate approach for mysql, as mysql has a weird limitation on the from clause in subqueries
        if (sqlif.omqp.getDriverName() == "mysql") {
            sqlif.omqp.exec("create temporary table x_ae_ids as select audit_eventid from audit_events where workflowid = %d", id);
            # delete the temporary table even if there's an error in the update statement
            on_exit sqlif.omqp.exec("drop table x_ae_ids");
            rows = sqlif.omqp.exec("update audit_events set related_audit_eventid = null where related_audit_eventid in (select audit_eventid from x_ae_ids)");
        } else {
            rows = sqlif.omqp.exec("update audit_events set related_audit_eventid = null where related_audit_eventid in (select audit_eventid from audit_events where workflowid = %v)", id);
        }
        if (rows) {
            printf("+ %d related audit event row(s) updated\n", rows);
        }
        rows = sqlif.omqp.exec("delete from audit_events where workflowid = %v", id);
        if (rows) {
            printf("+ %d audit event row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from workflow_tags where workflowid = %v", id);
        if (rows) {
            printf("+ %d workflow_tags row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from workflow_errors where workflowid = %v", id);
        if (rows) {
            printf("+ %d workflow_errors row(s) deleted\n", rows);
        }
        # issue 2446: delete workflow stats stage
        rows = sqlif.omqp.exec("delete from workflow_instance_stats_stage where objectid = %v", id);
        if (rows) {
            printf("+ %d workflow instance stats stage row(s) deleted\n", rows);
        }

        # issue 2446: delete workflow stats
        rows = sqlif.omqp.exec("delete from workflow_instance_stats where objectid = %v", id);
        if (rows) {
            printf("+ %d workflow instance stats row(s) deleted\n", rows);
        }

        sqlif.omqp.exec("delete from workflows where workflowid = %v", id);
        printf("+ workflow deleted from the database\n");

        omqmap_reload.workflows{id} = True;
    }

    static deleteWorkflow(hash<auto> workflow) {
        *softint id = sqlif.omqp.selectRow("select workflowid from workflows where name = %v and version = %v",
                                           string(workflow.name), string(workflow.version)).workflowid;
        if (!id) {
            throw "UNKNOWN-WORKFLOW", sprintf("%s:%s", workflow.name, workflow.version);
        }
        # issue #2776: disable any interfaces about to be deleted
        oload::disableInterface("workflow", id, workflow.name);
        # issue #3187: first delete workflow instances
        oload::deleteWorkflowInstances(id, workflow.name, workflow.version);
        oload::deleteWorkflow(id, workflow.name, workflow.version);
    }

    static deleteWorkflow(int id) {
        *hash<auto> workflow = sqlif.omqp.selectRow("select name, version from workflows where workflowid = %v", id);
        if (!workflow) {
            throw "UNKNOWN-WORKFLOW", sprintf("workflow with id %d does not exist", id);
        }
        # issue #2776: disable any interfaces about to be deleted
        oload::disableInterface("workflow", id, workflow.name);
        # issue #3187: first delete workflow instances
        oload::deleteWorkflowInstances(id, workflow.name, workflow.version);
        oload::deleteWorkflow(id, workflow.name, workflow.version);
    }

    static deleteServiceMethod(int method_id, string method, int service_id, string name, string version) {
        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        printf("deleting service method %s(%s).%s() (serviceid %d, service_methodid %d)\n", name, version, method, service_id, method_id);

        int rows = sqlif.omqp.exec("delete from service_method_tags where service_methodid = %v", method_id);
        if (rows) {
            printf("+ %d service_method_tags row(s) deleted\n", rows);
        }
        sqlif.omqp.exec("delete from service_methods where service_methodid = %v", method_id);
        printf("+ method deleted from the database\n");

        omqmap_reload.services{method_id} = True;
    }

    static deleteServiceMethod(hash service) {
        # bug 574: don't allow system service methods to be deleted
        service += sqlif.omqp.selectRow("select serviceid, service_type from services where name = %v and version = %v",
                                        service.name, service.version);
        if (!service.serviceid) {
            throw "UNKNOWN-SERVICE", sprintf("service %s:%s does not exist", service.name, service.version);
        }
        if (service.service_type != "USER") {
            throw "DELETION-FORBIDDEN", sprintf("deletion of system service methods is forbidden");
        }

        *softint id = sqlif.omqp.selectRow("select service_methodid from service_methods where serviceid = %v and name = %v",
                                           service.serviceid, service.method).service_methodid;
        if (!id) {
            throw "UNKNOWN-METHOD", sprintf("method %y does not exist in user service %s:%s (%d)",
                                            service.method,
                                            service.name,
                                            service.version,
                                            service.serviceid);
        }

        oload::deleteServiceMethod(id, service.method, service.serviceid, service.name, service.version);
    }

    static deleteServiceMethod(int method_id) {
       AbstractTable services_table = oload::getTable("services");

        # get service_type
        hash<auto> sh = {
            "columns": ("service_type", cop_as("serviceid", "id"), "name", "version", cop_as("sm.name", "method")),
            "join": join_inner("service_methods", "sm", {"serviceid": "serviceid"}, {"service_methodid": method_id}),
        };
        *hash<auto> service = services_table.selectRow(sh);
        if (!service) {
            throw "UNKNOWN-SERVICE", sprintf("service with method id %d does not exist", method_id);
        }
        if (service.service_type != "USER") {
            throw "DELETION-FORBIDDEN", sprintf("deletion of system service methods is forbidden");
        }

        oload::deleteServiceMethod(method_id, service.method, service.id, service.name, service.version);
    }

    static deleteService(int id, string name, string version) {
        printf("deleting service %s:%s (serviceid %d)\n", name, version, id);

        # issue #2776: disable any interfaces about to be deleted
        oload::disableInterface("service", id, name, version);

        # FIXME: deletion APIs must be implemented in the server itself

        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        # delete service options
        int rows = sqlif.omqp.exec("delete from service_options where serviceid = %v", id);
        if (rows) {
            printf("+ %d service_options row(s) deleted\n", rows);
        }
        # delete library references
        rows = sqlif.omqp.exec("delete from service_lib where serviceid = %v", id);
        if (rows) {
            printf("+ %d service_lib row(s) deleted\n", rows);
        }
        # delete mappers
        rows = sqlif.omqp.exec("delete from service_mappers where serviceid = %v", id);
        if (rows) {
            printf("+ %d service_mappers row(s) deleted\n", rows);
        }
        # delete value maps
        rows = sqlif.omqp.exec("delete from service_vmaps where serviceid = %v", id);
        if (rows) {
            printf("+ %d service_vmaps row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from service_method_tags where service_methodid in (select service_methodid "
            "from service_methods where serviceid = %v)", id);
        if (rows) {
            printf("+ %d service_method_tags row(s) deleted\n", rows);
        }
        # delete all methods
        rows = sqlif.omqp.exec("delete from service_methods where serviceid = %v", id);
        printf("+ %d service method(s) deleted\n", rows);

        # delete all file resources
        rows = sqlif.omqp.exec("delete from service_file_resources where serviceid = %v", id);
        printf("+ %d file resource(s) deleted\n", rows);

        # delete group entries
        rows = sqlif.omqp.exec("delete from group_services where serviceid = %v", id);
        if (rows){
            printf("+ %d service group entry(ies) deleted\n", rows);
        }
        # delete service state data
        rows = sqlif.omqp.exec("delete from service_state_data where serviceid = %v", id);
        if (rows) {
            printf("+ %d service_state_data row(s) deleted\n", rows);
        }

        # delete config items
        oload::deleteItems("service", id, True);

        rows = sqlif.omqp.exec("delete from service_tags where serviceid = %v", id);
        if (rows) {
            printf("+ %d service_tags row(s) deleted\n", rows);
        }
        # delete auth labels
        rows = sqlif.omqp.exec("delete from service_auth_labels where serviceid = %v", id);
        if (rows) {
            printf("+ %d authentication labels row(s) deleted\n", rows);
        }

        # delete audit events
        rows = sqlif.omqp.exec("delete from audit_events where serviceid = %v", id);
        if (rows) {
            printf("+ %d audit event row(s) deleted\n", rows);
        }

        sqlif.omqp.exec("delete from services where serviceid = %v", id);
        printf("+ service deleted from the database\n");

        omqmap_reload.services{id} = True;
    }

    static deleteService(hash service) {
        *softint id = sqlif.omqp.selectRow("select serviceid from services where name = %v and version = %v",
                                           service.name, service.version).serviceid;
        if (!id) {
            throw "UNKNOWN-SERVICE", sprintf("service %s:%s does not exist", service.name, service.version);
        }

        oload::deleteService(id, service.name, service.version);
    }

    static deleteService(int id) {
        *hash<auto> service = sqlif.omqp.selectRow("select name, version from services where serviceid = %v", id);
        if (!service) {
            throw "UNKNOWN-SERVICE", sprintf("service with id %d does not exist", id);
        }

        oload::deleteService(id, service.name, service.version);
    }

    static deleteStep(int id, string name, string version) {
        # check if step is referenced
        *list ql = sqlif.omqp.selectRows("select w.workflowid, name, version from workflow_steps ws, workflows w where stepid = %v and ws.workflowid = w.workflowid", id);
        if (ql) {
            string str = foldl $1 + ", " + $2, (map sprintf("%s:%s (%d)", $1.name, $1.version, $1.workflowid), ql);
            throw "ACTIVE-STEP", sprintf("step %s:%s (%d) is referenced by workflow%s: %s",
                                         name, version, id,
                                         ql.size() == 1 ? "" : "s", str);
        }

        # if there are no workflow_step rows, then we don't need to check segment_step and segment_async_link

        # see if there are step instances
        *softint count = sqlif.omqp.selectRow("select count(1) as \"count\" from step_instance where stepid = %v", id).count;

        if (count) {
            throw "STEP-INSTANCES", sprintf("step %s:%s (%d) has %d step instance%s",
                                            name, version, id,
                                            count,
                                            count == 1 ? "" : "s");
        }

        # if there are no step_instance rows, then we don't need to check
        # subworkflow_instance
        # error_instance
        # queue_data
        # step_instance_events

        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        # delete libraries
        int rows = sqlif.omqp.exec("delete from step_lib where stepid = %v", id);
        if (rows) {
            printf("+ %d step_lib row(s) deleted\n", rows);
        }

        # delete mappers
        rows = sqlif.omqp.exec("delete from step_mappers where stepid = %v", id);
        if (rows) {
            printf("+ %d step_mappers row(s) deleted\n", rows);
        }
        # delete value maps
        rows = sqlif.omqp.exec("delete from step_vmaps where stepid = %v", id);
        if (rows) {
            printf("+ %d step_vmaps row(s) deleted\n", rows);
        }

        # delete config items
        oload::deleteItems("step", id, True);

        int ntags = sqlif.omqp.exec("delete from step_tags where stepid = %v", id);

        sqlif.omqp.exec("delete from steps where stepid = %v", id);
        printf("+ step %s:%s (%d) deleted from the database (%d tag%s)\n",
               name, version, id,
               ntags,
               ntags == 1 ? "" : "s");

        omqmap_reload.steps{id} = True;
    }

    static deleteStep(hash step) {
        *softint id = sqlif.omqp.selectRow("select stepid from steps where name = %v and version = %v",
                                           step.name, step.version).stepid;
        if (!id) {
            throw "UNKNOWN-STEP", sprintf("step %s:%s does not exist", step.name, step.version);
        }
        oload::deleteStep(id, step.name, step.version);
    }

    static deleteStep(int id) {
        *hash<auto> step = sqlif.omqp.selectRow("select name, version from steps where stepid = %v", id);
        if (!step) {
            throw "UNKNOWN-STEP", sprintf("step with id %d does not exist", id);
        }
        oload::deleteStep(step);
    }

    static *list checkIfLibraryObjectIsActive(*int id, string name, *string version, string type,
                                              string interface_type, *string table_name = NOTHING,
                                              bool interface_version = True, bool no_id_column = False,
                                              bool check = True) {
        AbstractTable interface_table = oload::getTable(table_name ?? interface_type + "s");
        AbstractTable lib_table = oload::getTable(interface_type + "_lib");

        string id_column = no_id_column ? interface_type : interface_type + "id";

        list columns = (id_column, "o.name");
        if (interface_version) {
            columns += "o.version";
        }

        hash<auto> join_cols;
        if (no_id_column) {
            join_cols = {id_column: "name"};
        } else {
            join_cols = {id_column: id_column};
        }

        hash<auto> select_hash = {
            "columns": columns,
            "where": {
                "type": type,
                "name": name,
            },
            "join": join_inner(interface_table, "o", join_cols),
        };

        *list result = lib_table.selectRows(select_hash);
        if (check) {
            oload::checkLibraryObjectActiveResult(result, id, name, version, type, interface_type,
                                                  no_id_column ? NOTHING : id_column);
        }

        return result;
    }

    static checkLibraryObjectActiveResult(*list result, *int id, string name, *string version, string type,
                                          string interface_type, *string id_column) {
        if (!result) {
            return;
        }

        id_column = id_column ?? interface_type + "id";

        throw sprintf("ACTIVE-%s", type),
            sprintf("%s %s%s%s is referenced by %s%s: %s", type.lwr(), name, version ? sprintf(":%s", version) : "",
                exists id ? sprintf(" (%d)", id) : "", interface_type, result.size() == 1 ? "" : "s",
                (foldl $1 + ", " + $2, (map sprintf("%s%s%s", $1.name, $1.version ? sprintf(":%s", $1.version) : "",
                $1{id_column} ? sprintf(" (%d)", $1{id_column}) : ""), result)));
    }

    static deleteFunction(int id, string name, string version) {
        # check if function is referenced by a workflow
        *list q = sqlif.omqp.selectRows("select workflowid, name, version
                                    from workflows
                                    where errorfunction_instanceid = %v
                                        or attach_func_instanceid = %v
                                        or detach_func_instanceid = %v
                                        or onetimeinit_func_instanceid = %v
                                        or errhandler_func_instanceid = %v", id, id, id, id, id);

        oload::checkLibraryObjectActiveResult(q, id, name, version, "FUNCTION", "workflow");
        # check if function is referenced by a step
        q = sqlif.omqp.selectRows("select stepid, name, version
                                from steps
                                where stepfunction_instanceid = %v
                                    or validationfunction_instanceid = %v
                                    or endfunction_instanceid = %v
                                    or arrayfunction_instanceid = %v", id, id, id, id);

        oload::checkLibraryObjectActiveResult(q, id, name, version, "FUNCTION", "step");

        # bug 572: see if there is another function with this name to see if we need to check for library references
        q = sqlif.omqp.selectRows("select function_instanceid from function_instance where name = %v and function_instanceid != %v", name, id);

        # check if there is a reference as a library object if deleting the last version with this name
        if (!q) {
            oload::checkIfLibraryObjectIsActive(id, name, version, OT_FUNCTION, "workflow");
            oload::checkIfLibraryObjectIsActive(id, name, version, OT_FUNCTION, "step");
            oload::checkIfLibraryObjectIsActive(id, name, version, OT_FUNCTION, "service");
            oload::checkIfLibraryObjectIsActive(id, name, version, OT_FUNCTION, "job");
            oload::checkIfLibraryObjectIsActive(id, name, version, OT_FUNCTION, "mapper");
        }

        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        int ntags = sqlif.omqp.exec("delete from function_instance_tags where function_instanceid = %v", id);

        sqlif.omqp.exec("delete from function_instance where function_instanceid = %v", id);
        printf("+ function %s:%s (%s) deleted from the database (%d tag%s)\n", name, version, id, ntags, ntags == 1 ? "" : "s");

        omqmap_reload.functions{id} = True;
   }

    static deleteFunction(hash function) {
        *softint id = sqlif.omqp.selectRow("select function_instanceid from function_instance where name = %v and version = %v",
                                           string(function.name), string(function.version)).function_instanceid;
        if (!id) {
            throw "UNKNOWN-FUNCTION", sprintf("function %s:%s does not exist", function.name, function.version);
        }
        oload::deleteFunction(id, function.name, function.version);
    }

    static deleteFunction(int id) {
        *hash<auto> function = sqlif.omqp.selectRow("select name, version from function_instance where function_instanceid = %v", id);
        if (!function) {
            throw "UNKNOWN-FUNCTION", sprintf("function with id %d does not exist", function.id);
        }
        oload::deleteFunction(id, function.name, function.version);
    }

    static deleteClass(int id, string name, string version) {
        # bug 572: see if there is another class with this name to see if we need to check for library references
        *list q = sqlif.omqp.selectRows("select classid from classes where name = %v and classid != %v", name, id);

        # check if there is a reference as a library object if deleting the last version with this name
        if (!q) {
            oload::checkIfLibraryObjectIsActive(id, name, version, OT_CLASS, "workflow");
            oload::checkIfLibraryObjectIsActive(id, name, version, OT_CLASS, "step");
            oload::checkIfLibraryObjectIsActive(id, name, version, OT_CLASS, "service");
            oload::checkIfLibraryObjectIsActive(id, name, version, OT_CLASS, "job");
            oload::checkIfLibraryObjectIsActive(id, name, version, OT_CLASS, "mapper");
            oload::checkIfLibraryObjectIsActive(id, name, version, OT_CLASS, "pipeline", NOTHING, False, True);
        }

        # see if this is the last class with this name and if it is listed as a dependency of another
        {
            AbstractTable classes = oload::getTable("classes");

            hash<auto> sh = {
                "columns": cop_as(cop_count(), "count"),
                "where": {"name": name},
            };
            if (classes.selectRow(sh).count == 1) {
                sh = {
                    "columns": (cop_distinct("classid"), "name", "version"),
                    "where": {
                        "cd.dependson_class": name,
                    },
                    "join": join_inner(oload::getTable("class_dependencies"), "cd"),
                };
                q = classes.selectRows(sh);
                if (q) {
                    throw "ACTIVE-CLASS", sprintf("class %s:%s (%d) is the last class of its name and is referenced as a dependency by class%s: %s",
                        name, version, id, q.size() == 1 ? "" : "s",
                        foldl $1 + ", " + $2, (map sprintf("%s:%s (%d)", $1.name, $1.version, $1.classid), q));
                }
            }
        }

        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        int ndeps = sqlif.omqp.exec("delete from class_dependencies where classid = %v", id);
        int ntags = sqlif.omqp.exec("delete from class_tags where classid = %v", id);
        sqlif.omqp.exec("delete from classes where classid = %v", id);
        printf("+ class %s:%s (%d) deleted from the database (%d dep%s, %d tag%s)\n",
               name, version, id, ndeps, ndeps == 1 ? "" : "s", ntags, ntags == 1 ? "" : "s");

        omqmap_reload.classes{id} = True;
    }

    static deleteClass(hash cls) {
        AbstractTable classes = oload::getTable("classes");

        # get classid
        hash<auto> sh = {
            "columns": "classid",
            "where": {
                "name": cls.name,
                "version": cls.version,
            },
        };
        *softint id = classes.selectRow(sh).classid;
        if (!id) {
            throw "UNKNOWN-CLASS", sprintf("class %s:%s does not exist", cls.name, cls.version);
        }

        oload::deleteClass(id, cls.name, cls.version);
    }

    static deleteClass(int id) {
        AbstractTable classes = oload::getTable("classes");

        hash<auto> sh = {
            "columns": ("name", "version"),
            "where": {
                "classid": id,
            },
        };
        *hash<auto> cls = classes.selectRow(sh);
        if (!cls) {
            throw "UNKNOWN-CLASS", sprintf("class with id %d does not exist", id);
        }

        oload::deleteClass(id, cls.name, cls.version);
    }

    static deleteConstant(int id, string name, string version) {
        # bug 572: see if there is another constant with this name to see if we need to check for library references
        *list q = sqlif.omqp.selectRows("select constantid from constants where name = %v and constantid != %v", name, id);

        # check if there is a reference as a library object if deleting the last version with this name
        if (!q) {
            oload::checkIfLibraryObjectIsActive(id, name, version, OT_CONSTANT, "workflow");
            oload::checkIfLibraryObjectIsActive(id, name, version, OT_CONSTANT, "step");
            oload::checkIfLibraryObjectIsActive(id, name, version, OT_CONSTANT, "service");
            oload::checkIfLibraryObjectIsActive(id, name, version, OT_CONSTANT, "job");
            oload::checkIfLibraryObjectIsActive(id, name, version, OT_CONSTANT, "mapper");
        }

        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        int ntags = sqlif.omqp.exec("delete from constant_tags where constantid = %v", id);

        sqlif.omqp.exec("delete from constants where constantid = %v", id);
        printf("+ constant %s:%s (%d) deleted from the database (%d tag%s)\n",
               name, version, id, ntags, ntags == 1 ? "" : "s");

        omqmap_reload.constants{id} = True;
    }

    static deleteConstant(hash constant) {
        *softint id = sqlif.omqp.selectRow("select constantid from constants where name = %v and version = %v", string(constant.name), string(constant.version)).constantid;
        if (!id) {
            throw "UNKNOWN-CONSTANT", sprintf("constant %s:%s does not exist", constant.name, constant.version);
        }
        oload::deleteConstant(id, constant.name, constant.version);
    }

    static deleteConstant(int id) {
        *hash<auto> constant = sqlif.omqp.selectRow("select name, version from constants where constantid = %v", id);
        if (!constant) {
            throw "UNKNOWN-CONSTANT", sprintf("constant with id %d does not exist", id);
        }
        oload::deleteConstant(id, constant.name, constant.version);
    }

    static deleteQueue(int id, string name) {
        # bug 573: see if queue is in use by any steps
        *list q = sqlif.omqp.selectRows("select stepid, name, version from steps where queueid = %v", id);
        if (q) {
            throw "ACTIVE-QUEUE", sprintf("queue %s (%d) is referenced by step%s: %s", name, id,
                                          q.size() == 1 ? "" : "s",
                                          (foldl $1 + ", " + $2, (map sprintf("%s:%s (%d)", $1.name, $1.version, $1.stepid), q)));
        }
        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        # bug 573: delete queue data
        int qdrows = sqlif.omqp.exec("delete from queue_data where queueid = %v", id);
        int ntags = sqlif.omqp.exec("delete from queue_tags where queueid = %v", id);

        sqlif.omqp.exec("delete from queues where queueid = %v", id);
        printf("+ queue %s (%d) deleted from the database (%d data row%s, %d tag%s)\n", name, id,
               qdrows, qdrows == 1 ? "" : "s", ntags, ntags == 1 ? "" : "s");

        omqmap_reload.queues{id} = True;
    }

    static deleteQueue(hash queue) {
        *softint id = sqlif.omqp.selectRow("select queueid from queues where name = %v", queue.name).queueid;
        if (!id) {
            throw "UNKNOWN-QUEUE", sprintf("queue %y does not exist", queue.name);
        }
        oload::deleteQueue(id, queue.name);
    }

    static deleteQueue(int id) {
        *hash<auto> queue = sqlif.omqp.selectRow("select name from queues where queueid = %v", id);
        if (!queue) {
            throw "UNKNOWN-QUEUE", sprintf("queue with id %d does not exist", id);
        }
        oload::deleteQueue(id, queue.name);
    }

    static deleteJobInstances(int id, string name) {
        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        printf("deleting %s\n", sprintf("job instances %s (jobid %d)", name, id));

        int rows = sqlif.omqp.exec("delete from job_errors where job_instanceid in (select job_instanceid from job_instance where jobid = %v)", id);
        if (rows) {
            printf("+ %d job_errors row(s) deleted\n", rows);
        }
        # delete audit events
        rows = sqlif.omqp.exec("delete from audit_events where jobid = %v and job_instanceid is not null", id);
        if (rows) {
            printf("+ %d audit event row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from job_instance where jobid = %v", id);
        if (rows) {
            printf("+ %d job_instance row(s) deleted\n", rows);
        }
        printf("+ job instances deleted from the database\n");

        # issue 2446: delete job instance stats stage
        rows = sqlif.omqp.exec("delete from job_instance_stats_stage where objectid = %v", id);
        if (rows) {
            printf("+ %d job instance stats stage row(s) deleted\n", rows);
        }

        # issue 2446: delete job instance stats
        rows = sqlif.omqp.exec("delete from job_instance_stats where objectid = %v", id);
        if (rows) {
            printf("+ %d job instance stats row(s) deleted\n", rows);
        }
    }

    static deleteJobInstances(hash job) {
        *softint id = sqlif.omqp.selectRow("select jobid from jobs where name = %v", job.name).jobid;
        if (!id) {
            throw "UNKNOWN-JOB", sprintf("job %s does not exist", job.name);
        }
        # issue #2776: disable any interfaces about to be deleted
        bool disabled = oload::disableInterface("job", id, job.name);
        on_exit if (disabled) {
            oload::enableInterface("job", id, job.name);
        }
        oload::deleteJobInstances(id, job.name);
    }

    static deleteJobInstances(int id) {
        *hash<auto> job = sqlif.omqp.selectRow("select name from jobs where jobid = %v", id);
        if (!job) {
            throw "UNKNOWN-JOB", sprintf("job with id %d does not exist", id);
        }
        # issue #2776: disable any interfaces about to be deleted
        bool disabled = oload::disableInterface("job", id, job.name);
        on_exit if (disabled) {
            oload::enableInterface("job", id, job.name);
        }
        oload::deleteJobInstances(id, job.name);
    }

    static deleteJob(int id, string name) {
        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        printf("deleting job %s (jobid %d)\n", name, id);

        # delete job options
        int rows = sqlif.omqp.exec("delete from job_options where jobid = %v", id);
        if (rows) {
            printf("+ %d job_options row(s) deleted\n", rows);
        }
        # delete job library rows
        rows = sqlif.omqp.exec("delete from job_lib where jobid = %v", id);
        if (rows) {
            printf("+ %d job_lib row(s) deleted\n", rows);
        }
        # delete mappers
        rows = sqlif.omqp.exec("delete from job_mappers where jobid = %v", id);
        if (rows) {
            printf("+ %d job_mappers row(s) deleted\n", rows);
        }
        # delete value maps
        rows = sqlif.omqp.exec("delete from job_vmaps where jobid = %v", id);
        if (rows) {
            printf("+ %d job_vmaps row(s) deleted\n", rows);
        }
        # delete group jobs
        rows = sqlif.omqp.exec("delete from group_jobs where jobid = %v", id);
        if (rows) {
            printf("+ %d group_jobs row(s) deleted\n", rows);
        }
        # delete audit events
        rows = sqlif.omqp.exec("delete from audit_events where jobid = %v", id);
        if (rows) {
            printf("+ %d audit event row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from job_state_data where jobid = %v", id);
        if (rows) {
            printf("+ %d job_state_data row(s) deleted\n", rows);
        }
        rows = sqlif.omqp.exec("delete from job_persistent_state_data where jobid = %v", id);
        if (rows) {
            printf("+ %d job_persistent_state_data row(s) deleted\n", rows);
        }

        # delete config items
        oload::deleteItems("job", id, True);

        rows = sqlif.omqp.exec("delete from job_tags where jobid = %v", id);
        if (rows) {
            printf("+ %d job_tags row(s) deleted\n", rows);
        }

        sqlif.omqp.exec("delete from jobs where jobid = %v", id);
        printf("+ job deleted from the database\n");

        omqmap_reload.jobs{id} = True;
    }

    static deleteJob(hash<auto> job) {
        *softint id = sqlif.omqp.selectRow("select jobid from jobs where name = %v", job.name).jobid;
        if (!id) {
            throw "UNKNOWN-JOB", sprintf("job %s does not exist", job.name);
        }
        # issue #2776: disable any interfaces about to be deleted
        oload::disableInterface("job", id, job.name);
        # issue #3187: first delete job instances
        oload::deleteJobInstances(id, job.name);
        oload::deleteJob(id, job.name);
    }

    static deleteJob(int id) {
        *hash<auto> job = sqlif.omqp.selectRow("select name from jobs where jobid = %v", id);
        if (!job) {
            throw "UNKNOWN-JOB", sprintf("job with id %d does not exist", id);
        }
        # issue #2776: disable any interfaces about to be deleted
        oload::disableInterface("job", id, job.name);
        # issue #3187: first delete job instances
        oload::deleteJobInstances(id, job.name);
        oload::deleteJob(id, job.name);
    }

    #! delete config items
    /** @param type the interface object type
        @param id the interface ID
        @param migrating set to True when migrating or deleting objects, otherwise deletion will fail if the object
        has a non-local value
    */
    static deleteItems(string type, int id, *bool migrating) {
        InterfaceConfigContainer interface_config_container(type, id);
        int rows = interface_config_container.deleteConfigItems(migrating ?? oload.o.redef, False);
        printf("+ %d config item row(s) deleted\n", rows);
    }

    deleteGroup(int id, string name) {
        printf("deleting group %s (groupid %d)\n", name, id);

        # delete group from the DB
        hash<auto> h = sqlif.deleteGroup(id);
        if (h.workflows) {
            printf("+ %d group_workflows row(s) deleted\n", h.workflows);
        }
        if (h.services) {
            printf("+ %d group_services row(s) deleted\n", h.services);
        }
        if (h.jobs) {
            printf("+ %d group_jobs row(s) deleted\n", h.jobs);
        }
        if (h.mappers) {
            printf("+ %d group_mappers row(s) deleted\n", h.mappers);
        }
        if (h.vmaps) {
            printf("+ %d group_vmaps row(s) deleted\n", h.vmaps);
        }
        if (h.fsms) {
            printf("+ %d group_fsms row(s) deleted\n", h.fsms);
        }
        if (h.pipelines) {
            printf("+ %d group_pipelines row(s) deleted\n", h.pipelines);
        }

        printf("+ group %s (groupid %d) deleted from the database\n", name, id);
        reload_groups = True;
    }

    deleteGroup(hash<auto> group) {
        *softint id = sqlif.omqp.selectRow("select groupid from groups where name = %v", group.name).groupid;
        if (!id) {
            throw "UNKNOWN-GROUP", sprintf("group %s does not exist", group.name);
        }
        deleteGroup(id, group.name);
    }

    deleteGroup(int id) {
        *hash<auto> group = sqlif.omqp.selectRow("select name from groups where groupid = %v", id);
        if (!group) {
            throw "UNKNOWN-GROUP", sprintf("group with id %d does not exist", id);
        }
        deleteGroup(id, group.name);
    }

    static deleteWorkflowEventType(int id, string name) {
        if (id === 0) {
            throw "INVALID-WORKFLOW-EVENT", sprintf("workflow event type %y (%d) is a system event and cannot be deleted",
                                                    name, id);
        }

        *list q = sqlif.omqp.selectRows("select stepid, name, version from steps where workflow_event_typeid = %v", id);
        if (q) {
            throw "ACTIVE-WORKFLOW-EVENT", sprintf("workflow event type %y (%d) is referenced by step%s: %s",
                                                   name, id,
                                                   q.size() == 1 ? "" : "s", (foldl $1 + ", " + $2, (map sprintf("%s:%s (%d)", $1.name, $1.version, $1.stepid), q)));
        }

        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        int qdrows = sqlif.omqp.exec("delete from workflow_events where workflow_event_typeid = %v", id);
        int ntags = sqlif.omqp.exec("delete from workflow_event_type_tags where workflow_event_typeid = %v", id);

        sqlif.omqp.exec("delete from workflow_event_types where workflow_event_typeid = %v", id);
        printf("+ workflow event type %y (%d) deleted from the database (%d event row%s, %d tag%s)\n",
               name, id,
               qdrows, qdrows == 1 ? "" : "s", ntags, ntags == 1 ? "" : "s");

        omqmap_reload.events{id} = True;
    }

    # bug 575: allow workflow event types to be deleted
    static deleteWorkflowEventType(hash<auto> workflow_event_type) {
        # get workflow_event_type id
        *softint id = sqlif.omqp.selectRow("select workflow_event_typeid from workflow_event_types where name = %v",
                                           workflow_event_type.name).workflow_event_typeid;

        if (!id) {
            throw "UNKNOWN-WORKFLOW-EVENT", sprintf("workflow event type %y does not exist",
                                                    workflow_event_type.name);
        }
        oload::deleteWorkflowEventType(id, workflow_event_type.name);
    }

    static deleteWorkflowEventType(int id) {
        # get workflow_event_type id
        *hash<auto> workflow_event_type = sqlif.omqp.selectRow("select name from workflow_event_types where workflow_event_typeid = %v", id);
        if (!workflow_event_type) {
            throw "UNKNOWN-WORKFLOW-EVENT", sprintf("workflow event type with id %d does not exist", id);
        }
        oload::deleteWorkflowEventType(id, workflow_event_type.name);
    }

    static deleteConnection(hash<auto> connection) {
        # get connection id
        *int id = sqlif.omqp.selectRow("select connectionid from connections where name = %v and "
            "connection_type = %v", connection.name, connection.type).connectionid;
        if (!id) {
            return;
        }

        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        # delete connection tags
        int trows = sqlif.omqp.exec("delete from connection_tags where connectionid = %v", id);
        int rows = sqlif.omqp.exec("delete from connections where connectionid = %v", id);
        if (!rows) {
            throw sprintf("UNKNOWN-%s", connection.type == "USER" ? "USER-CONNECTION" : connection.type),
                  sprintf("%s %y is unknown",
                          connection.type == "USER" ? "user connection" : connection.type.lwr(),
                          connection.name);
        }

        printf("+ %s %y deleted from the database (%d tag%s)\n",
               connection.type == "USER" ? "user connection" : connection.type.lwr(),
               connection.name, trows, trows == 1 ? "" : "s");

        reload_connections.user = True;
    }

    # bug 1068: allow user connections to be deleted
    static deleteUserConnection(hash user_connection) {
        user_connection.type = "USER";
        oload::deleteConnection(user_connection);
        reload_connections.user = True;
    }

    static deleteDatasource(hash datasource) {
        datasource.type = "DATASOURCE";
        oload::deleteConnection(datasource);
        reload_connections.datasources = True;
    }

    static deleteRemote(hash remote) {
        remote.type = "REMOTE";
        oload::deleteConnection(remote);
        reload_connections.qorus = True;
    }

    static deleteMapper(int id, string name, string version) {
        # see if there is another mapper with this name to see if we need to check for interface references
        *list q = sqlif.omqp.select("select mapperid from mappers where name = %v and mapperid != %v", name, id).mapperid;

        # check if there is a reference if deleting the last version with this name
        if (!q) {
            q = sqlif.omqp.selectRows("select m.workflowid, name, version from workflow_mappers m, workflows w where m.workflowid = w.workflowid and mapperid = %v", id);
            if (q) {
                throw "ACTIVE-MAPPER", sprintf("mapper %s:%s (%d) is referenced by workflow%s: %s", name, version, id,
                                               q.size() == 1 ? "" : "s",
                                               (foldl $1 + ", " + $2, (map sprintf("%s:%s (%d)", $1.name, $1.version, $1.workflowid), q)));
            }
            q = sqlif.omqp.selectRows("select m.serviceid, name, version from service_mappers m, services s where m.serviceid = s.serviceid and mapperid = %v", id);
            if (q) {
                throw "ACTIVE-MAPPER", sprintf("mapper %s:%s (%d) is referenced by service%s: %s", name, version, id,
                                               q.size() == 1 ? "" : "s",
                                               (foldl $1 + ", " + $2, (map sprintf("%s:%s (%d)", $1.name, $1.version, $1.serviceid), q)));
            }
            q = sqlif.omqp.selectRows("select m.jobid, name, version from job_mappers m, jobs j where m.jobid = j.jobid and mapperid = %v", id);
            if (q) {
                throw "ACTIVE-MAPPER", sprintf("mapper %s:%s (%d) is referenced by job%s: %s", name, version, id,
                                               q.size() == 1 ? "" : "s",
                                               (foldl $1 + ", " + $2, (map sprintf("%s:%s (%d)", $1.name, $1.version, $1.jobid), q)));
            }

            string sql = sprintf("select l.pipeline, p.name from pipeline_lib l, pipelines p where l.type = '%s' "
                                 "and l.name = %v and l.pipeline = p.name", OT_MAPPER);
            q = sqlif.omqp.selectRows(sql, name);
            if (q) {
                throw sprintf("ACTIVE-%s", OT_MAPPER), sprintf("mapper %s:%s (%d) is referenced by pipeline%s: %s", name,
                    version, id, q.size() == 1 ? "" : "s", (foldl $1 + ", " + $2, (map $1.name, q)));
            }

            sql = sprintf("select l.fsm, f.name from fsm_lib l, fsm f where l.type = '%s' and l.name = %v "
                          "and l.fsm = f.name", OT_MAPPER);
            q = sqlif.omqp.selectRows(sql, name);
            if (q) {
                throw sprintf("ACTIVE-%s", OT_MAPPER), sprintf("mapper %s:%s (%d) is referenced by fsm: %s", name,
                    version, id, (foldl $1 + ", " + $2, (map $1.name, q)));
            }
        }

        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        int rows = sqlif.omqp.exec("delete from mapper_lib where mapperid = %v", id);
        if (rows)
            printf("+ %d mapper_lib row(s) deleted\n", rows);

        int ntags = sqlif.omqp.exec("delete from mapper_tags where mapperid = %v", id);

        # delete group mappers
        rows = sqlif.omqp.exec("delete from group_mappers where mapperid = %v", id);
        if (rows)
            printf("+ %d group_mappers row(s) deleted\n", rows);

        # issue #3470 delete step mappers
        rows = sqlif.omqp.exec("delete from step_mappers where mapperid = %v", id);
        if (rows) {
            printf("+ %d step_mappers row(s) deleted\n", rows);
        }

        sqlif.omqp.exec("delete from mappers where mapperid = %v", id);
        printf("+ mapper %s:%s (%d) deleted from the database (%d tag%s)\n",
               name, version, id, ntags, ntags == 1 ? "" : "s");

        omqmap_reload.mappers{id} = True;
    }

    static deleteMapper(hash<auto> mapper) {
        *softint id = sqlif.omqp.selectRow("select mapperid from mappers where name = %v and version = %v",
                                           mapper.name, mapper.version).mapperid;
        if (!id) {
            throw "UNKNOWN-MAPPER", sprintf("mapper %s:%s does not exist", mapper.name, mapper.version);
        }
        oload::deleteMapper(id, mapper.name, mapper.version);
    }

    static deleteMapper(int id) {
        *hash<auto> mapper = sqlif.omqp.selectRow("select name, version from mappers where mapperid = %v", id);
        if (!mapper) {
            throw "UNKNOWN-MAPPER", sprintf("mapper with id %d does not exist", id);
        }
        oload::deleteMapper(id, mapper.name, mapper.version);
    }

    static deleteType(hash<auto> type) {
        string path = QorusDataProviderTypeHelper::normalizeTypePath(type.name);

        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        int rows = sqlif.omqp.exec("delete from data_types where path = %v", path);
        if (rows) {
            printf("+ %d type row%s deleted\n", rows, rows == 1 ? "" : "s");
        } else {
            printf("no type path %y to delete\n", path);
        }

        omqmap_reload.types{path} = True;
    }

    static deletePipeline(hash<auto> pipeline) {
        *softstring name = sqlif.omqp.selectRow("select name from pipelines where name = %v", pipeline.name).name;
        if (!name) {
            throw "UNKNOWN-PIPELINE", sprintf("pipeline %s does not exist", pipeline.name);
        }

        oload::checkIfLibraryObjectIsActive(NOTHING, pipeline.name, NOTHING, OT_PIPELINE, "fsm", "fsm", False, True);

        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        int rows = sqlif.omqp.exec("delete from pipeline_lib where pipeline = %v", pipeline.name);
        if (rows) {
            printf("+ %d pipeline_lib row(s) deleted\n", rows);
        }

        rows = sqlif.omqp.exec("delete from pipeline_config_items where pipeline = %v", pipeline.name);
        if (rows) {
            printf("+ %d pipeline_config_items row(s) deleted\n", rows);
        }

        # delete group pipelines
        rows = sqlif.omqp.exec("delete from group_pipelines where pipeline = %v", pipeline.name);
        if (rows) {
            printf("+ %d group_pipelines row(s) deleted\n", rows);
        }

        rows = sqlif.omqp.exec("delete from pipelines where name = %v", pipeline.name);
        if (rows) {
            printf("+ %d pipeline row%s deleted\n", rows, rows == 1 ? "" : "s");
        } else {
            printf("no pipeline %y to delete\n", pipeline.name);
        }

        omqmap_reload.pipelines{pipeline.name} = True;
        oload.val_pipeline_map{pipeline.name} = True;
    }

    static deleteFsm(hash<auto> fsm) {
        *softstring name = sqlif.omqp.selectRow("select name from fsm where name = %v", fsm.name).name;
        if (!name) {
            oload.deleted_fsm{fsm.name} = True;
            throw "UNKNOWN-FSM", sprintf("fsm %s does not exist", fsm.name);
        }

        oload::checkIfLibraryObjectIsActive(NOTHING, fsm.name, NOTHING, OT_FSM, "step");
        oload::checkIfLibraryObjectIsActive(NOTHING, fsm.name, NOTHING, OT_FSM, "service");
        oload::checkIfLibraryObjectIsActive(NOTHING, fsm.name, NOTHING, OT_FSM, "job");

        if (!fsm.no_fsm_check) {
            *list result;
            try {
                result = oload::checkIfLibraryObjectIsActive(NOTHING, fsm.name, NOTHING, OT_FSM, "fsm", "fsm", False,
                                                            True, False);
                oload::checkLibraryObjectActiveResult(result, NOTHING, fsm.name, NOTHING, OT_FSM, "fsm", "");
            } catch (hash<ExceptionInfo> ex) {
                if (ex.err == "ACTIVE-FSM") {
                    oload.delete_fsm_dependencies{fsm.name} = result;
                    # will be checked and deleted later
                    return;
                } else {
                    rethrow;
                }
            }
        }

        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        int rows = sqlif.omqp.exec("delete from fsm_lib where fsm = %v", fsm.name);
        if (rows) {
            printf("+ %d fsm_lib row(s) deleted\n", rows);
        }

        rows = sqlif.omqp.exec("delete from fsm_config_items where fsm = %v", fsm.name);
        if (rows) {
            printf("+ %d fsm_config_items row(s) deleted\n", rows);
        }
        # delete group fsms
        rows = sqlif.omqp.exec("delete from group_fsms where fsm = %v", fsm.name);
        if (rows) {
            printf("+ %d group_fsms row(s) deleted\n", rows);
        }

        rows = sqlif.omqp.exec("delete from fsm where name = %v", fsm.name);
        if (rows) {
            printf("+ %d fsm row%s deleted\n", rows, rows == 1 ? "" : "s");
        } else {
            printf("no fsm %y to delete\n", fsm.name);
        }

        oload.deleted_fsm{fsm.name} = True;
        omqmap_reload.fsm{fsm.name} = True;
        oload.val_fsm_map{fsm.name} = True;
    }

    static deleteValueMap(int id, string name) {
        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        # check if there is a reference
        *list q = sqlif.omqp.selectRows("select m.workflowid, name, version from workflow_vmaps m, workflows w where m.workflowid = w.workflowid and m.id = %v", id);
        if (q) {
            throw "ACTIVE-VALUE-MAP", sprintf("value map %s (%d) is referenced by workflow%s: %s",
                                      name, id,
                                      q.size() == 1 ? "" : "s", (foldl $1 + ", " + $2, (map sprintf("%s:%s (%d)", $1.name, $1.version, $1.workflowid), q)));
        }
        q = sqlif.omqp.selectRows("select m.serviceid, name, version from service_vmaps m, services s where m.serviceid = s.serviceid and m.id = %v", id);
        if (q) {
            throw "ACTIVE-VALUE-MAP", sprintf("value map %s (%d) is referenced by service%s: %s",
                                              name, id,
                                              q.size() == 1 ? "" : "s", (foldl $1 + ", " + $2, (map sprintf("%s:%s (%d)", $1.name, $1.version, $1.serviceid), q)));
        }
        q = sqlif.omqp.selectRows("select m.jobid, name, version from job_vmaps m, jobs j where m.jobid = j.jobid and m.id = %v", id);
        if (q) {
            throw "ACTIVE-VALUE-MAP", sprintf("value map %s (%d) is referenced by job%s: %s",
                                              name, id,
                                              q.size() == 1 ? "" : "s", (foldl $1 + ", " + $2, (map sprintf("%s:%s (%d)", $1.name, $1.version, $1.jobid), q)));
        }
        int rows = sqlif.omqp.exec("delete from value_map_values where value_map_id = %v", id);

        if (rows) {
            printf("+ %d value_map_values row(s) deleted\n", rows);
        }
        # delete group value maps
        rows = sqlif.omqp.exec("delete from group_vmaps where id = %v", id);
        if (rows) {
            printf("+ %d group_vmaps row(s) deleted\n", rows);
        }
        sqlif.omqp.exec("delete from value_maps where id = %v", id);
        printf("+ value map %s (%d) deleted\n", name, id);

        omqmap_reload.vmaps{id} = True;
    }

    static deleteValueMap(hash<auto> vmap) {
        *softint id = sqlif.omqp.selectRow("select id from value_maps where name = %v", vmap.name).id;
        if (!id) {
            throw "UNKNOWN-VALUE-MAP", sprintf("value map '%s' does not exist", vmap.name);
        }
        oload::deleteValueMap(id, vmap.name);
    }

    static deleteValueMap(int id) {
        *hash<auto> vmap = sqlif.omqp.selectRow("select name from value_maps where id = %v", id);
        if (!vmap) {
            throw "UNKNOWN-VALUE-MAP", sprintf("value map with id %d does not exist", id);
        }
        oload::deleteValueMap(id, vmap.name);
    }

    private tryDelete(code call) {
        try {
            call_function_args(call, argv);
        } catch (hash<ExceptionInfo> ex) {
            if (ex.err =~ /^UNKNOWN-/) {
                printf("%s: %s (skipping)\n", ex.err, ex.desc);
            } else {
                rethrow;
            }
        }
    }

    private deleteObjects(*hash delete_hash, hash<auto> methods_map) {
        foreach auto delete_method in (methods_map.pairIterator()) {
            map tryDelete(delete_method.value, $1), delete_hash{delete_method.key};
        }

        # resolve FSM dependencies
        hash<auto> fsm_to_delete = {};
        foreach hash<auto> fsm in (delete_hash.fsm) {
            if (!deleted_fsm.hasKey(fsm.name)) {
                fsm_to_delete{fsm.name} = True;
            }
        }

        #printf("delete_fsm_dependencies: %y\n", delete_fsm_dependencies);
        #printf("fsm_to_delete: %y\n", fsm_to_delete);
        #printf("deleted_fsm: %y\n", deleted_fsm);

        foreach string fsm in (keys fsm_to_delete) {
            foreach hash<auto> dep in (delete_fsm_dependencies{fsm}) {
                if (!fsm_to_delete.hasKey(dep.name) && !deleted_fsm.hasKey(dep.name)) {
                    # dependent FSM is not listed in the deletion list
                    # will throw an error, the dependency list is not empty
                    checkLibraryObjectActiveResult(delete_fsm_dependencies{fsm}, NOTHING, fsm, NOTHING, OT_FSM, "fsm",
                                                   "");
                }
            }
            # try to delete it again without checking for FSM dependencies - already solved
            tryDelete(methods_map.fsm, {"name": fsm, "no_fsm_check": True});
        }
    }

    static *string getSchemaVersion(string key = "schema-version") {
        return sqlif.omqp.selectRow("select value from system_properties where domain = %v and keyname = %v", "omq", key).value;
    }

    verifySchema(string arg, *string key) {
        bool upgrade;
        if (!key) {
            upgrade = True;
            key = "schema-version";
        }

        *string version = oload::getSchemaVersion(key);
        if (!exists version) {
            stderr.printf("ERROR: unable to determine schema version, loader script requires %y for upgrade\n", arg);
            exit(1);
        }

        if (version != arg) {
            stderr.printf("ERROR: current schema key '%s' = %y, loader script requires %y for upgrade\n", key, version, arg);
            exit(1);
        }

        if (upgrade && !o.quiet)
            printf("schema version '%s' will be upgraded\n", arg);
    }

    static verifyEmptySchema() {
        *string ver;
        try {
            ver = oload::getSchemaVersion();
        }
        catch() {
            return;
        }

        stderr.printf("ERROR: current schema is version '%s', loader script requires an empty schema for initialization\n", ver);
        exit(1);
    }

    static checkDataModel() {
        *string version = oload::getSchemaVersion("schema-load-compatibility");
        if (version != OMQ::load_datamodel) {
            stderr.printf("ERROR: datamodel is version %y, loader requires %y\n", version, OMQ::load_datamodel);
            exit(1);
        }
    }

    mystat(string c) {
        if (!o.quiet) {
            printf(c);
            flush();
        }
    }

    # returns true if stepid is directly linked to id
    static bool directLink(hash<auto> segsteps, hash<auto> deps, softstring stepid) {
        bool rc = True;

        # if the dependencies of the step are a subset of the start step(s), then they are directly linked
        if (!list_subset(deps.steps{stepid}, deps.start)) {
            # if all dependents are normal and directly linked, then we are also
            # NOTE: exclude steps if
            # * one of the dependencies is asynchronous, a subworkflow step, or an event step
            # or
            # * the dependency does not exist in this segment (also not a start step)
            # or
            # * one of the dependencies is not directly linked to a start step
            foreach softstring sid in (deps.steps{stepid}) {
                #printf("sid=%d, type=%s, deps.steps{sid}=%s",
                if (AsyncStepTypes{deps.tmap{sid}} ||
                    (!exists deps.steps{sid} && !inlist(sid, deps.start) && !exists segsteps{sid}) ||
                    !oload::directLink(segsteps, deps, sid) ||
                    exists deps.astart{sid}) {
                    rc = False;
                    break;
                }
            }
        }
        #printf("%d directly linked to %n = %n\n", stepid, deps.start, rc ? "True" : "False" );
        return rc;
    }

    # processSegment() creates a new segment with steps directly linked to the start step
    static processSegment(reference segment, reference deps, int c) {
        # remove the start steps from workflowdata
        foreach softstring id in (deps.start)
            if (!exists deps.astart{id})
                delete deps.steps{id};

        # create new segment with steps directly linked to the start steps
        foreach softstring id in (deps.start) {
            # add start ids to segment with no dependencies
            segment[c].steps{id} = ();
            # add to "stepid to segment" map
            if (!exists deps.segmap{id})
                deps.segmap{id} = c;
        }

        # delete astart steps from start list to ensure that no steps can be
        # directly linked to async/subworkflow/evemt steps
        foreach string id in (keys deps.astart) {
            if (inlist(id, deps.start)) {
                remove_from_list(\deps.start, id);
            }
        }

        # check remaining steps to see if they are directly linked to a start step
        foreach string stepid in (keys deps.steps) {
            if (oload::directLink(segment[c].steps, deps, stepid)) {
                #printf("%d belongs in segment %d\n", stepid, c);
                segment[c].steps{stepid} = deps.steps{stepid};
                # add to workflow's step to segment map
                deps.segmap{stepid} = c;
                # asynchronous steps are left in the list for the backend to be executed
                if (deps.tmap{stepid} == OMQ::ExecNormal)
                    delete deps.steps{stepid};
                else
                    deps.steps{stepid} = ();
            }
        }

        # remove all entries from start list
        foreach softstring id in (deps.start)  {
            remove_from_list(\deps.start, id);
        }
        #deps.start += savestart;
    }

    static createSegments(softint workflowid, hash deps) {
        list seg;
        int sid = 0;

        # create start list
        deps.start = ();

        #printf("1: createSegments() deps: %N\n", deps);
        # create all asynchronous segments

        do {
            # find start steps & async/subworkflow/event start steps
            foreach string id in (keys deps.steps) {
                if (!elements deps.steps{id}) {
                    if (AsyncStepTypes{deps.tmap{id}}) {
                        deps.astart{id} = id;
                    }
                    deps.start += id;
                }
            }

            # first, create segments based on all steps directly linked to the start step(s)
            while (deps.start) {
                ##################
                # DEBUG: DELETE ME
                #printf("iteration: %d\n", sid);
                #printf("deps     : %N\n", deps);
                #printf("seg      : %N\n", seg);
                ##################

                # if not initial segment, then all start steps represent the back-end of async steps
                seg[sid].segdeps = ();
                if (sid) {
                    foreach softstring id in (deps.start) {
                        deps.backend{id} = sid;
                    }
                }

                oload::processSegment(\seg, \deps, sid++);

                # add all steps with no dependencies to start list
                foreach string id in (keys deps.steps) {
                    if (!elements deps.steps{id}) {
                        deps.astart{id} = id;
                        delete deps.steps{id};
                    }
                }
                #printf("astart=%N\n", deps.astart);

                # create new start list
                deps.start = deps.astart
                    ? list(remove deps.astart.(deps.astart.firstKey()))
                    : ();
            }

            #printf("2: createSegments() deps: %N\nseg: %N\n", deps, seg);
            #exit();

            # now, if there are steps left that have dependencies in the segment(s) just
            # created, then they need their own segment
            {
                while (True) {
                    softstring found;
                    # find steps with missing dependant steps
                    foreach string id in (keys deps.steps) {
                        foreach softstring depid in (deps.steps{id}) {
                            if (!exists deps.steps{depid}) {
                                found = id;
                                break;
                            }
                        }
                        if (found) {
                            break;
                        }
                    }
                    if (!found) {
                        break;
                    }
                    # save dependency list for new segment
                    *softlist ndeps = deps.steps{found};

                    # if not creating a backend segment, create segdeps from ndeps
                    if (!exists deps.backend{found}) {
                        list segdeps = ();
                        foreach softstring id in (ndeps)
                            if (exists deps.backend{id} &&
                                !inlist(deps.backend{id}, segdeps))
                            segdeps += deps.backend{id};
                        else if (!inlist(deps.segmap{id}, segdeps))
                            segdeps += deps.segmap{id};
                        seg[sid].segdeps = segdeps;
                    }

                    # if the step is an asynchronous/subworkflow/event step, then it must have its own segment
                    if (AsyncStepTypes{deps.tmap{found}}) {
                        #printf("found asynchronous step %d\n", found);
                        seg[sid].steps{found} = ();
                        deps.segmap{found} = sid;
                        deps.backend{found} = ++sid;
                        deps.start = list(found);
                        delete deps.steps{found};
                        break;
                    }

                    # otherwise create a new normal segment
                    deps.start = ();

                    # find all nodes with the same dependency
                    foreach string id in (keys deps.steps) {
                        if (lists_equal_ignore_order(deps.steps{id}, ndeps)) {
                            deps.start += id;
                            delete deps.steps{id};
                        }
                    }

                    #printf("seg %d: found %n, ndeps=%n\n", sid, found, ndeps);
                    #printf("** creating new detached segment: deps=%N\nseg=%N\n", deps, seg);
                    oload::processSegment(\seg, \deps, sid++);
                    #printf("** AFTER deps=%N, seg=%N\n", deps, seg);
                }
            }

            # find any steps left over
            foreach string id in (keys deps.steps) {
                if (!elements deps.steps{id}) {
                    deps.start += id;
                    delete deps.steps{id};
                }
            }
        } while (elements deps.start);

        #printf("sid: %y deps: %N\n", sid, deps);

        # issue #2466: look for trailing async/swf/event with no defined segment
        foreach hash<auto> i in (deps.backend.pairIterator()) {
            if (i.value >= sid) {
                seg[i.value] = {
                    "segdeps": (),
                    "steps": {
                        i.key: (),
                    },
                };
                sid = i.value + 1;
                #printf("i: %y; seg:\n%N\n", i, seg);
                #printf("deps:\n%N\n", deps);
                #exit(1);
            }
        }

        #printf("sid: %N\n", sid);
        #printf("deps: %N\n", deps);
        #printf("seg: %N\n", seg);
        #exit();

        # delete all segment dependency entries for this workflow
        sqlif.omqp.exec("delete from segment_dependencies where workflowid = %v", workflowid);
        # delete all segment async links
        sqlif.omqp.exec("delete from segment_async_link where workflowid = %v", workflowid);
        # delete all segment steps entries for this workflow
        sqlif.omqp.exec("delete from segment_steps where workflowid = %v", workflowid);

        # insert segment_async_link entries
        #printf("sid: %y be: %N\n", sid, deps.backend);
        foreach softint bsid in (keys deps.backend) {
            sqlif.omqp.exec("insert into segment_async_link (workflowid, segmentid, stepid, frontend_segmentid) values (%v, %v, %v, %v)",
                    workflowid, int(deps.backend{bsid}), bsid, int(deps.segmap{bsid}));
        }

        for (int i = 0; i < sid; i++) {
            # create dummy segment dependency row for async segments/initial segment
            if (!elements seg[i].segdeps) {
                sqlif.omqp.exec("insert into segment_dependencies (workflowid, segmentid, dependson_segmentid) values (%v, %v, %v)", workflowid, i, i);
            }
            else {
                foreach softint sd in (seg[i].segdeps) {
                    sqlif.omqp.exec("insert into segment_dependencies (workflowid, segmentid, dependson_segmentid) values (%v, %v, %v)", workflowid, i, sd);
                }
            }

            # insert step dependencies for segment
            foreach string id in (keys seg[i].steps) {
                if (elements seg[i].steps{id}) {
                    foreach softint did in (seg[i].steps{id}) {
                        sqlif.omqp.exec("insert into segment_steps (workflowid, segmentid, stepid, dependson_stepid) values (%v, %v, %v, %v)", workflowid, i, int(id), did);
                    }
                }
                else {
                    sqlif.omqp.exec("insert into segment_steps (workflowid, segmentid, stepid, dependson_stepid) values (%v, %v, %v, %v)", workflowid, i, int(id), int(id));
                }
            }
        }
    }

    addStepIntern(reference steph, hash h) {
        if (!steph{h.name}{h.version})
            steph{h.name}{h.version} = h;
    }

    addStep(reference steph, reference step) {
        switch (step.typeCode()) {
            case NT_LIST: {
                foreach any si in (\step) {
                    addStep(\steph, \si);
                }
                break;
            }
            case NT_HASH:
            case NT_STRING: {
                hash<auto> h = getLoaderStepInfo(step, "STEP FUNCTION ERROR");
                addStepIntern(\steph, h);
                # remove IDs for workflow release hash
                step = h - ("stepid", "queueid", "endid", "arrayid", "valid", "eventid", "id");
                break;
            }
            default: {
                stderr.printf("ERROR in workflow step list, element is not list, string, or hash (type: %y, val: %y)\n", step.type(), step);
                exit(1);
            }
        }
    }

    hash<auto> getLoaderYamlStepInfo(string step, string error_text) {
        (*string name, *string version) = (step =~ x/^([[:alpha:]][[:alnum:]_-]*):([^:]+)$/u);
        if (!exists name || !exists version) {
            error("%s: no version specified for step %y\n", error_text, step);
        }

        # printf("DEBUG: getLoaderYamlStepInfo: %y\n", stepmap{name}{version});
        *hash<auto> stepinfo = sqlif.omqp.selectRow("select * from steps where name = %v and version = %v",
            name, version);
        if (!stepinfo) {
            error("%s: cannot find step %s:%s\n", error_text, name, version);
        }

        return stepinfo.("steptype", "arraytype", "name", "version", "queueid", "stepid");
    }

    hash<auto> getLoaderStepInfo(auto step, string errtxt, bool yaml_workflow = False) {
        if (yaml_workflow) {
            return getLoaderYamlStepInfo(step, errtxt);
        }

        hash<auto> h;

        # if it's a string, then take the last version
        if (step.typeCode() == NT_STRING) {
            # issue #1704: see if the step string is a class designation
            (*string isclass, *string name, *string version) = (step =~ x/^(class:)?([[:alpha:]][[:alnum:]_-]*):([^:]+)$/);
            if (name && version) {
                if (isclass) {
                    h.name = h.classname = name;
                    h.version = h.classversion = version;
                } else {
                    h.name = h.funcname = name;
                    h.version = h.funcversion = version;
                }
            } else {
                stderr.printf("%s: no version specified for function %y\n", errtxt, step);
                exit(1);
            }
        } else if (step.typeCode() == NT_HASH) {
            h += step;

            if (!step.name) {
                stderr.printf("%s: required \"name\" key missing in step hash: %y\n", errtxt, step);
                exit(1);
            }

            # see if it has an embedded version
            if ((int i = index(step.name, ":")) != -1) {
                string ver = substr(step.name, i + 1);
                if (strlen(h.version) && h.version != ver) {
                    stderr.printf("%s: explicit version %y given and version %y also given in step name\n", errtxt, h.version, ver);
                    exit(1);
                }
                h.name = substr(step.name, 0, i);
                h.version = ver;
            } else if (!strlen(h.version)) {
                stderr.printf("%s: no version specified for step %y\n", errtxt, step.name);
                exit(1);
            }

            # set function name and version
            if (strlen(step.funcname)) {
                if (h.classname) {
                    stderr.printf("%s: 01 cannot define both a step function (%y) and a step class (%s) when defining a step\n", errtxt, step.funcname, h.classname);
                }

                # see if it has an embedded version
                if ((int i = index(step.funcname, ":")) != -1) {
                    h.funcname = substr(step.funcname, 0, i);
                    h.funcversion = substr(step.funcname, i + 1);
                } else { # get last version
                    h.funcversion = frmap.(h.funcname).lastversion;
                    if (!exists h.funcversion) {
                        stderr.printf("%s: cannot find last version for function '%s'\n", errtxt, h.funcversion);
                        exit(1);
                    }
                }
            } else if (!step.classname) {
                h.funcname = h.name;
                h.funcversion = h.version;
            }

            if (step.classname) {
                if (h.funcname) {
                    stderr.printf("%s: 02 cannot define both a step function (%y) and a step class (%s) when defining a step\n", errtxt, step.funcname, step.classname);
                }
                if ((int i = index(step.classname, ":")) != -1) {
                    h.classname = substr(step.classname, 0, i);
                    h.classversion = substr(step.classname, i + 1);
                } else { # get last version
                    h.classversion = classrmap.(h.classname).lastversion;
                    if (!exists h.classversion) {
                        stderr.printf("%s: cannot find last version for class '%s'\n", errtxt, h.classname);
                        exit(1);
                    }
                }
            }

            # for backwards compatibility with Petr's change
            if (h.stepname) {
                h.name = h.stepname;
            }

            # get validation function
            if (step.valname) {
                if (h.classname) {
                    stderr.printf("%s: cannot define both a step function (%y) and a step class (%s) when defining a step\n", errtxt, step.valname, h.classname);
                }

                if (h.subworkflow) {
                    stderr.printf("%s: subworkflow steps are not allowed to have validation functions\n", errtxt);
                    exit(1);
                }

                if (h.eventtype) {
                    stderr.printf("%s: event steps are not allowed to have validation functions\n", errtxt);
                    exit(1);
                }

                if ((int i = index(h.valname, ":")) != -1) {
                    h.valversion = substr(h.valname, i + 1);
                    h.valname    = substr(h.valname, 0, i);
                }

                h.valid = frmap.(h.valname).(h.valversion).id;
                if (!h.valid) {
                    stderr.printf("%s: cannot map validation function %s(%s) to a function id\n",
                                  errtxt, h.valname, h.valversion);
                    exit(1);
                }
            } else if (h.valversion) {
                stderr.printf("%s: valversion '%s' without valname found\n", errtxt, h.valversion);
                exit(1);
            }

            # see if it's an array step
            if (h.arrayname) {
                # issue #1704: check for step class
                if (h.classname) {
                    stderr.printf("%s: cannot define an array step function (%y) and a step class (%s) when defining a step\n", errtxt, step.arrayname, h.classname);
                }

                if ((int i = index(h.arrayname, ":")) != -1) {
                    h.arrayversion = substr(h.arrayname, i + 1);
                    h.arrayname    = substr(h.arrayname, 0, i);
                }

                h.arrayid = frmap.(h.arrayname).(h.arrayversion).id;
                if (!h.arrayid) {
                    stderr.printf("%s: cannot map array function %s(%s) to a function id\n",
                                  errtxt, h.arrayname, h.arrayversion);
                    exit(1);
                }
                if (!h.arraytype) {
                    h.arraytype = OMQ::ArraySeries;
                } else if (!inlist(step.arraytype, (OMQ::ArrayNone, OMQ::ArraySeries, OMQ::ArrayParallel))) {
                    stderr.printf("%s: invalid arraytype %y\n", errtxt, h.arraytype);
                    exit(1);
                }
            } else if (h.arraytype) {
                if (h.arraytype != OMQ::ArraySeries && h.arraytype != OMQ::ArrayNone) {
                    stderr.printf("%s: invalid arraytype %y\n", errtxt, h.arraytype);
                    exit(1);
                }
                if (!h.classname && h.arraytype != OMQ::ArrayNone) {
                    stderr.printf("%s: arraytype '%s' is not valid for a step without an array function\n", errtxt, h.arraytype);
                    exit(1);
                }
            }

            # first set endversion if it's embedded in the name
            if (h.endname
                && !h.endversion
                && (int i = index(step.endname, ":")) != -1) {
                h.endversion = substr(h.endname, i + 1);
                h.endname    = substr(h.endname, 0, i);
            }

            # issue #1704: check for step class
            if (h.endname && h.classname) {
                stderr.printf("%s: cannot define an end step function (%y) and a step class (%s) when defining a step\n", errtxt, h.endname, h.classname);
            }

            # now see if it's an asynchronous step
            int count;

            foreach string key in (AsyncKeys) {
                if (h{key}) {
                    ++count;
                }
            }

            if (count) {
                if (!h.classname) {
                    if (count != AsyncKeys.size()) {
                        stderr.printf("ERROR: async step definition missing keys (%d != %d):", count, elements AsyncKeys);
                        foreach string key in (AsyncKeys)
                            if (!exists h{key})
                                stderr.printf(" %s", key);
                        stderr.printf("\n");
                        exit(1);
                    }

                    h.endid = frmap.(h.endname).(h.endversion).id;

                    if (!exists h.endid) {
                        stderr.printf("%s: cannot map async end function %y:%y to a function id\n",
                                    errtxt, h.endname, h.endversion);
                        #printf("functionrmap=%N\n", frmap);
                        exit(1);
                    }
                }

                h.queueid = qmap.(h.queue);
                if (!exists h.queueid) {
                    stderr.printf("%s: queue %s does not exist\n", errtxt, h.queue);
                    exit(1);
                }

                if (h.subworkflow) {
                    stderr.printf("%s: asynchronous step declared and subworkflow = True, step cannot be both\n", errtxt);
                    exit(1);
                }

                if (strlen(h.eventtype)) {
                    stderr.printf("%s: asynchronous step declared with eventtype = %y; cannot be both async and event step\n", errtxt, h.eventtype);
                    exit(1);
                }

                h.steptype = OMQ::ExecAsync;
            } else if (h.eventtype) {
                # see if it's a workflow event step
                h.eventid = emap.(h.eventtype);

                if (!h.eventid) {
                    stderr.printf("%s: workflow event %y does not exist\n", errtxt, h.eventtype);
                    exit(1);
                }

                if (h.subworkflow) {
                    stderr.printf("%s: event step declared and subworkflow = True, step cannot be both\n", errtxt);
                    exit(1);
                }

                h.steptype = OMQ::ExecEvent;
            }
        } else {
            stderr.printf("%s: invalid function, expecting string or hash, got type: %y, value: %y\n", errtxt, type(step), step);
            #throw "here";
            exit(1);
        }
        if (h.subworkflow) {
            h.steptype = OMQ::ExecSubWorkflow;
        }

        if (h.funcname) {
            h.id = frmap.(h.funcname).(h.funcversion).id;
            if (!h.id) {
                stderr.printf("%s: 01 cannot map function name %y version %y to a function id\n", errtxt, h.funcname, h.funcversion);
                exit(1);
            }
        } else if (h.classname) {
            h.classid = classrmap.(h.classname).(h.classversion);
            if (!h.classid) {
                stderr.printf("%s: cannot map class name %y version %y to a class id\n", errtxt, h.classname, h.classversion);
                exit(1);
            }
        }

        # set async attributes to "null" if not an async step
        if (!h.queueid) {
            h.endid = h.queueid = NULL;
        }
        if (!exists h.arraytype) {
            h.arraytype = OMQ::ArrayNone;
        }
        if (!exists h.arrayid) {
            h.arrayid = NULL;
        }
        if (!exists h.valid) {
            h.valid = NULL;
        }
        if (!exists h.steptype) {
            h.steptype = OMQ::ExecNormal;
        }

        # set "author" from primary step function's author
        h.author = frmap.(h.funcname).(h.funcversion).author;

        h.stepid = stepmap.(h.name).(h.version);

        #printf("name=%s version=%s id=%d\n", name, version, id);
        #printf("getLoaderStepInfo() h: %N\n", h);
        return h;
    }

    hash<auto> getClassInfo(auto info, string errtxt) {
        hash<auto> h;

        # if it's a string, then take the last version
        if (info.typeCode() == NT_STRING) {
            # see if it has an embedded version
            if ((int i = index(info, ":")) != -1) {
                h.name = substr(info, 0, i);
                h.version = substr(info, i + 1);
            } else {
                stderr.printf("%s: no version specified for class %y\n", errtxt, info);
                exit(1);
            }
        } else if (info.typeCode() == NT_HASH) {
            # += so "h" stays "hash<auto>"
            h += info;

            # see if it has an embedded version
            if ((int i = index(info.name, ":")) != -1) {
                h.name = substr(info.name, 0, i);
                h.version = substr(info.name, i + 1);
            }
        } else {
            stderr.printf("%s: invalid class, expecting string or hash, got type: %s, data: %y\n", errtxt, info.type(), info);
            exit(1);
        }

        h.id = classrmap{h.name}{h.version};
        if (!h.id) {
            stderr.printf("%s: 02 cannot map to class id (%y)\n", errtxt, h);
            exit(1);
        }

        return h;
    }

    hash<auto> getFunctionInfo(auto func, string errtxt) {
        hash<auto> h;

        # if it's a string, then take the last version
        if (func.typeCode() == NT_STRING) {
            # see if it has an embedded version
            if ((int i = index(func, ":")) != -1) {
                h.name = substr(func, 0, i);
                h.version = substr(func, i + 1);
            }
            else {
                stderr.printf("%s: no version specified for function %y\n", errtxt, func);
                exit(1);
            }
        } else if (func.typeCode() == NT_HASH) {
            # += so "h" stays "hash<auto>"
            h += func;

            # see if it has an embedded version
            if ((int i = index(func.name, ":")) != -1) {
                h.name = substr(func.name, 0, i);
                h.version = substr(func.name, i + 1);
            }
        } else {
            stderr.printf("%s: invalid function, expecting string or hash, got type: %s, data: %y\n", errtxt, func.type(), func);
            exit(1);
        }

        h.id = frmap.(h.name).(h.version).id;
        if (!h.id) {
            stderr.printf("%s: 02 cannot map to function id (%y)\n", errtxt, h);
            exit(1);
        }

        return h;
    }

    # returns True if any steps have user metadata
    bool createSteps(hash<auto> steph) {
        bool has_metadata = False;
        # populate STEPS table
        foreach string name in (keys steph) {
            foreach string version in (keys steph{name}) {
                # += so "h" stays "hash<auto>"
                hash<auto> h += steph{name}{version};

                if (h."user-interaction" && h.steptype != ExecAsync) {
                    stderr.printf("ERROR: step %s:%s has type %y must be %y to enable user interaction\n",
                        h.name, h.version, h.steptype, ExecAsync);
                    exit(1);
                }

                # check if the step has user metadata
                if (!has_metadata && h.classid) {
                    # this can return false positives, but that's not a problem
                    AbstractTable classes = getTable("classes");
                    has_metadata = exists classes.selectRow({
                        "columns": "classid",
                        "where": {
                            "classid": h.classid,
                            "body": op_like("%getStepMetadata%"),
                        },
                    });
                }

                *hash<auto> stepinfo = sqlif.omqp.selectRow("select * from steps where name = %v and version = %v", string(h.name), string(h.version));
                *softint stepid = stepinfo.stepid;
                if (stepid) { # update existing step
                    if (o.verbose) {
                        if (h.funcname) {
                            printf("step %s:%s (%d) updating function=%s:%s (%d)\n", h.name, h.version, stepid, h.funcname, h.funcversion, h.id);
                        } else {
                            printf("step %s:%s (%d) updating class=%s:%s (%d) lang %y\n", h.name, h.version, stepid,
                                h.classname, h.classversion, h.classid, classmap{h.classid}.language);
                        }
                    } else {
                        mystat(".");
                    }

                    # check for dangerous/illegal step redefinitions
                    if (!o.redef) {
                        if (h.steptype != stepinfo.steptype) {
                            stderr.printf("ERROR: step %s:%s (%d) type is currently: %y, new definition: %y\n", h.name, h.version, stepid, stepinfo.steptype, h.steptype);
                            exit(1);
                        }
                        if (h.arraytype != stepinfo.arraytype) {
                            stderr.printf("ERROR: step %s:%s (%d) array type is currently: %y, new definition: %y\n", h.name, h.version, stepid, stepinfo.arraytype, h.arraytype);
                            exit(1);
                        }
                    }

                    sqlif.omqp.exec("update steps
                    set steptype = %v,
                        patch = %v,
                        description = %v,
                        author = %v,
                        step_classid = %v,
                        stepfunction_instanceid = %v,
                        validationfunction_instanceid = %v,
                        endfunction_instanceid = %v,
                        arrayfunction_instanceid = %v,
                        arraytype = %v,
                        queueid = %v,
                        user_interaction = %v,
                        workflow_event_typeid = %v
                    where stepid = %v",
                                    h.steptype,
                                    h.patch,
                                    h.desc,
                                    h.author,
                                    oload::intOrNull(h.classid),
                                    oload::intOrNull(h.id),
                                    oload::intOrNull(h.valid),
                                    oload::intOrNull(h.endid),
                                    oload::intOrNull(h.arrayid),
                                    h.arraytype,
                                    oload::intOrNull(h.queueid),
                                    h."user-interaction".toInt(),
                                    oload::intOrNull(h.eventid),
                                    int(stepid));
                } else {  # insert new step
                    stepid = get_next_sequence_value("seq_steps");
                    if (o.verbose) {
                        if (h.funcname) {
                            printf("step %s:%s (%d) inserting function=%s:%s (%d)\n", h.name, h.version, stepid, h.funcname, h.funcversion, h.id);
                        } else {
                            printf("step %s:%s (%d) inserting class=%s:%s (%d) lang %y\n", h.name, h.version, stepid, h.classname, h.classversion, h.classid, classmap{h.classid}.language);
                        }
                    } else {
                        mystat("I");
                    }

                    sqlif.omqp.exec("insert into steps
                    (stepid, steptype, name, version, patch, description,
                     author, step_classid, stepfunction_instanceid, validationfunction_instanceid,
                     endfunction_instanceid, arrayfunction_instanceid, arraytype,
                     queueid, user_interaction, workflow_event_typeid)
                    values
                    (%v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v)",
                                    stepid,
                                    h.steptype,
                                    h.name,
                                    h.version,
                                    h.patch,
                                    h.desc,
                                    h.author,
                                    oload::intOrNull(h.classid),
                                    oload::intOrNull(h.id),
                                    oload::intOrNull(h.valid),
                                    oload::intOrNull(h.endid),
                                    oload::intOrNull(h.arrayid), h.arraytype,
                                    oload::intOrNull(h.queueid),
                                    h."user-interaction".toInt(),
                                    oload::intOrNull(h.eventid));

                    # issue #3281: make entry in old config item value map for value migration
                    *softint old_id = step_name_map{h.name}.stepid;
                    if (old_id) {
                        *hash<auto> config_item_values = InterfaceConfigContainer::getInterfaceConfigItemValues("step:" + old_id);
                        if (config_item_values) {
                            config_item_value_map.step{stepid} = config_item_values;
                        }
                    }
                }

                omqmap_reload.steps{stepid} = True;
                #printf("%s\n", sql);
                stepmap.(h.name).(h.version) = stepid;
            }
        }
        return has_metadata;
    }

    createWorkflowSteps(hash fl, softlist list) {
        # delete existing step dependencies for workflow (if any)
        sqlif.omqp.exec("delete from workflow_steps where workflowid = %v", int(fl.workflowid));

        foreach hash h in (list) {
            if (exists h.dep) {
                if (o.verbose)
                    printf("%s:%s (%d) step %s:%s (%d) depends on %s:%s (%d)\n",
                           fl.name, fl.version, fl.workflowid,
                           h.step.name, h.step.version, h.step.stepid,
                           h.dep.name, h.dep.version, h.dep.stepid);
                sqlif.omqp.exec("insert into workflow_steps (workflowid, stepid, dependson_stepid) values (%v, %v, %v)", int(fl.workflowid), int(h.step.stepid), int(h.dep.stepid));
            }
            else {
                if (o.verbose)
                    printf("%s:%s (%d) step %s:%s (%d) has no dependencies\n",
                           fl.name, fl.version, fl.workflowid,
                           h.step.name, h.step.version, h.step.stepid);
                sqlif.omqp.exec("insert into workflow_steps (workflowid, stepid, dependson_stepid) values (%v, %v, %v)", int(fl.workflowid), int(h.step.stepid), int(h.step.stepid));
            }
        }
    }

    # check for dangerous redefinitions
    static check_workflow_segments_redefinition(hash fl, softlist list) {
        int err;

        # read in current workflow step dependencies
        *list q = sqlif.omqp.selectRows("select stepid, dependson_stepid from workflow_steps where workflowid = %v", int(fl.workflowid));
        if (!q)
            return;

        # get a list of unique steps in the workflow
        hash old;
        foreach hash row in (q)
            old.(row.stepid) = True;

        # get a list of unique steps in the new definition of the workflow
        hash newv;
        foreach hash row in (list)
            newv.(row.step.stepid) = True;

        # see if any steps are missing in the new definition
        foreach string id in (keys old) {
            if (!newv{id}) {
                stderr.printf("ERROR: new definition of workflow %s:%s (%d) is missing stepid %d; please change the "
                    "workflow version number if there is workflow order data already in the database for the old "
                    "definition and reload, otherwise call oload with the -A or --allow-redef option (in which case "
                    "an incompatible version of the workflow will be loaded, making recoveries and further "
                    "processing of existing workflow data created with the older version impossible; this should "
                    "only be done in a development system, for example)\n", fl.name, fl.version, fl.workflowid, id);
                err++;
            }
        }

        if (err) {
            stderr.printf("correct the error%s listed above and try again\n", err == 1 ? "" : "s");
            exit(1);
        }
    }

    createWorkflowSegments(hash<auto> wf, bool yaml_workflow = False) {
        # create step dependencies for workflow
        WorkflowStepDependencyParser stepDeps(wf, wf.steps, yaml_workflow);

        # check for dangerous/illegal redefinitions
        if (!o.redef)
            oload::check_workflow_segments_redefinition(wf, stepDeps.stepDependencyList);

        # printf("DEBUG: createWorkflowSegments:stepDeps.stepDependencyList: %y\n", stepDeps.stepDependencyList);

        # create new workflow_steps entries
        createWorkflowSteps(wf, stepDeps.stepDependencyList);

        # create segments for workflow
        hash<auto> deps = {
            "amap": stepDeps.arrayTypeMap,
            "tmap": stepDeps.stepTypeMap,
            "steps": stepDeps.stepDependencyMap,
        };
        oload::createSegments(wf.workflowid, deps);
    }

    static checkWorkflowRedefinition(softint wfid, reference wf) {
        *list keysv = sqlif.omqp.select("select keyname from workflow_keys where workflowid = %v", wfid).keyname;
        # if there are any keys in keysv that are not in wf.keylist, then exit with an error
        if (!elements keysv)
            return;

        list el = ();
        foreach string k in (keysv) {
            if (!inlist(k, wf.keylist))
                el += k;
        }

        if (el) {
            stderr.printf("ERROR: new definition of workflow %s:%s (%d) is missing the following keys: %y; add the "
                "missing keys to the workflow definition or reload using the -A or --allow-redef option to confirm "
                "that the missing keys are not used (trying to set values against the missing keys will cause "
                "runtime errors; note that the -A or --allow-redef option is potentially unsafe and should normally "
                "only be used in a development environment as it can allow an incompatible version of a workflow to "
                "be loaded, making recoveries and further processing of existing workflow data impossible)\n",
                wf.name, wf.version, wfid, el);
            exit(1);
        }
        wf.oldkeys = keysv;
    }

    static check_keys(*softint wfid, hash wf) {
        # do not check if the keylist is not a list or there are less than 2 elements
        if (wf.keylist.typeCode() != NT_LIST || elements wf.keylist < 2)
            return;

        for (int i = 0; i < (elements wf.keylist) - 1; i++) {
            for (int j = i + 1; j < elements wf.keylist; j++) {
                if (wf.keylist[i] == wf.keylist[j]) {
                    stderr.printf("ERROR: definition of workflow %s:%s (%d) contains duplicate key %y\n", wf.name, wf.version, wfid, wf.keylist[i]);
                    exit(1);
                }
            }
        }
    }

    private addWorkflowToGroup(softint groupid, softint wfid) {
        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        sqlif.addWorkflowsToGroup(groupid, wfid);
        reload_groups = True;

        string name = grmap{groupid};

        # add to group map
        gmap{name}.workflows += wfid;

        # add to workflow lookup map
        gwmap{wfid} += !exists gwmap{wfid} ? list(name) : name;
    }

    int insertOrUpdateWorkflow(hash<auto> wf, string filename) {
        bool has_detach = False;
        *hash<auto> efh;
        if (wf.errorfunction) {
            efh = getFunctionInfo(wf.errorfunction, "ERROR FUNCTION ERROR");
        }

        *string code_ = wf{"code"};

        *softint af;
        if (wf.attach) {
            if (code_) {
                oload::error("workflow %s v%s: cannot define an attach function when a workflow class has been "
                    "defined", wf.name, wf.version);
            }
            af = getFunctionInfo(wf.attach, "ATTACH FUNCTION ERROR").id;
        }

        *softint df;
        if (wf.detach) {
            if (code_) {
                oload::error("workflow %s v%s: cannot define an detach function when a workflow class has been "
                    "defined", wf.name, wf.version);
            }
            df = getFunctionInfo(wf.detach, "DETACH FUNCTION ERROR").id;
            has_detach = True;
        }

        *softint oif;
        if (wf.onetimeinit) {
            if (code_) {
                oload::error("workflow %s v%s: cannot define a one time init function when a workflow class has been "
                    "defined", wf.name, wf.version);
            }
            oif = getFunctionInfo(wf.onetimeinit, "ONETIMEINIT FUNCTION ERROR").id;
        }

        *softint ehf;
        if (wf.error_handler) {
            if (code_) {
                oload::error("workflow %s v%s: cannot define a one time init function when a workflow class has been "
                    "defined", wf.name, wf.version);
            }
            ehf = getFunctionInfo(wf.error_handler, "ERROR HANDLER FUNCTION ERROR").id;
        }

        *softlist<auto> ml = getMapperList(wf.mappers);

        *softlist<auto> vml = getVMapList(wf.vmaps);

        # issue #1699: check for remote flag in workflow definition
        bool remote = exists wf.remote ? wf.remote.toBool() : coptions.remote;

        # see if workflow exists already
        *hash<auto> wq = sqlif.omqp.selectRow("select workflowid, autostart, manual_autostart, sla_threshold, "
            "manual_sla_threshold, remote, manual_remote, deprecated from workflows where name = %v and version = %v",
            string(wf.name), string(wf.version));
        *softint workflowid;
        if (wq) {
            workflowid = wq.workflowid;
        }

        # check for duplicate keys
        oload::check_keys(workflowid, wf);

        if (wf.desc && !wf.description)
            wf.description = wf.desc;

        if (strlen(wf.description) > SQLDescLen) {
            int ol = length(wf.description);
            wf.description = trunc_str(wf.description, SQLDescLen, encoding);
            if (!o.quiet)
                warning("description of workflow %y truncated to %d characters to fit in DB column with max byte len of %d bytes (was %d characters)", wf.name, length(wf.description), SQLDescLen, ol);
        }

        *int max_instances;
        if (wf.max_instances) {
            max_instances = int(wf.max_instances);
            if (!max_instances) {
                warning("ignoring invalid \"max_instances\" value %y for workflow %s", wf.max_instances, wf.name);
                delete max_instances;
            }
        }

        # if the autostart and deprecated flags are both set then the autostart flag given in the
        # workflow def file will be ignored anyway so we don't need additional checks here
        wf.autostart = int(wf.autostart);

        if (wf.autostart < 0) {
            warning("ignoring invalid \"autostart\" value %y for workflow %s, setting autostart to 0", wf.autostart, wf.name);
            wf.autostart = 0;
        }
        if (exists max_instances && wf.autostart > max_instances) {
            warning("adjusting incorrect \"autostart\" value %y for workflow %s to %y (to fit into max_instances)", wf.autostart, wf.name, max_instances);
            wf.autostart = max_instances;
        }

        # check the sla_threshold value
        if (!exists wf.sla_threshold) {
            if (o.verbose) {
                warning("ignoring missing \"sla_threshold\" value for workflow %s, setting sla_threshold to %d",
                        wf.name, DefaultWorkflowSlaThreshold);
            }
            wf.sla_threshold = DefaultWorkflowSlaThreshold;
        } else {
            wf.sla_threshold = wf.sla_threshold.toInt();

            if (wf.sla_threshold <= 0) {
                warning("ignoring invalid \"sla_threshold\" value %y for workflow %s, setting sla_threshold to %d",
                        wf.sla_threshold, wf.name, DefaultWorkflowSlaThreshold);
                wf.sla_threshold = DefaultWorkflowSlaThreshold;
            }
        }

        # prepare workflow_modules value
        string workflow_modules_string;
        if (wf."workflow-modules") {
            workflow_modules_string = wf."workflow-modules".typeCode() == NT_STRING
                ? wf."workflow-modules"
                : wf."workflow-modules".join(",");
        }

        # issue #3446: get workflow static data type path
        *string staticdata_type;
        if (wf."staticdata-type") {
            staticdata_type = getPathFromTypeHash(wf."staticdata-type");
        }

        # run in single transaction in case the object is rejected due to tags
        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        # language info, if any
        *string lang_info;
        if (wf{"lang"} == "java") {
            # we need to compile any Java source to byte code for insertion in the DB
            lang_info = JavaClassHelper::compileJavaSource("workflow", wf{"class-name"} ?? wf{"name"}, wf{"version"},
                wf{"code"}, wf{"tags"}, wf, cast<*list<string>>(wf{"classes"}));
        }

        # update workflow
        if (workflowid) {
            # check for incompatible workflow redefinition
            if (!o.redef)
                oload::checkWorkflowRedefinition(workflowid, \wf);

            # should we ignore the autostart flag (if it's been updated manually already)
            bool ia = wq.manual_autostart && wf.autostart != wq.autostart;

            if (ia && o.override) {
                warning("overriding autostart value %y for workflow %s", wq.autostart, wf.name);
                ia = False;
            }

            # should we ignore the sla_threshold value (if it's been updated manually already)
            bool ist = wq.manual_sla_threshold && wf.sla_threshold != wq.sla_threshold;

            if (ist && o.override) {
                warning("overriding sla_threshold value %y for workflow %s", wq.sla_threshold, wf.name);
                ist = False;
            }

            # should we ignore the remote value (if it's been updated manually already)
            bool ignore_remote = wq.manual_remote && remote != wq.remote;

            if (ignore_remote && o.override) {
                warning("overriding remote value %y for workflow %s", wq.remote, wf.name);
                ignore_remote = False;
            }

            if (o.verbose) {
                printf("%s:%s (%d) updating workflow; remote: %y%s, autostart: %y%s, sla_threshold: %y%s, "
                    "max_instances: %y\n", wf.name, wf.version, workflowid,
                    remote, ignore_remote ? " (ignoring: manually updated)" : "",
                    wf.autostart, ia ? " (ignoring: manually updated)" : "",
                    seconds(wf.sla_threshold), ist ? " (ignoring: manually updated)" : "",
                    wf.max_instances);
            } else {
                mystat(".");
            }

            # fixed bug 648: workflow tags cannot be loaded twice because they are not handled correctly
            sqlif.omqp.exec("delete from workflow_tags where workflowid = %v", workflowid);

            # delete workflow mappers when updating
            sqlif.omqp.exec("delete from workflow_mappers where workflowid = %v", workflowid);

            # delete workflow value maps when updating
            sqlif.omqp.exec("delete from workflow_vmaps where workflowid = %v", workflowid);

            sqlif.omqp.exec("update workflows set patch = %v, description = %v, author = %v, autostart = %v, "
                "workflow_modules = %v, sla_threshold = %v, max_instances = %v, code = %v, language = %v, "
                "language_info = %v, class_name = %v, has_detach = %v, errorfunction_instanceid = %v, "
                "attach_func_instanceid = %v, detach_func_instanceid = %v, onetimeinit_func_instanceid = %v, errhandler_func_instanceid = %v, "
                "remote = %v, staticdata_type_path = %v where workflowid = %v",
                wf.patch, wf.description, wf.author, ia ? wq.autostart : wf.autostart,
                workflow_modules_string, ist ? wq.sla_threshold : wf.sla_threshold, max_instances,
                code_, wf{"lang"} ?? "qore", lang_info, wf{"class-name"}, has_detach.toInt(), efh.id, af, df, oif, ehf,
                ignore_remote ? wq.remote : remote.toInt(), staticdata_type, workflowid);
        } else {
            workflowid = get_next_sequence_value("seq_workflows");

            if (o.verbose)
                printf("%s:%s (%d) inserting workflow; remote: %y, autostart: %y, max_instances: %y\n", wf.name, wf.version, workflowid, remote, wf.autostart, wf.max_instances);
            else
                mystat("I");

            #printf("wf: %N\n", wf);
            sqlif.omqp.exec("insert into workflows (workflowid, name, version, patch, description, author, autostart, "
                "workflow_modules, sla_threshold, max_instances, code, language, language_info, class_name, "
                "has_detach, errorfunction_instanceid, attach_func_instanceid, detach_func_instanceid, "
                "onetimeinit_func_instanceid, errhandler_func_instanceid, remote, staticdata_type_path) "
                "values (%v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v)",
                workflowid, wf.name, wf.version, wf.patch, wf.description, wf.author, wf.autostart,
                workflow_modules_string, wf.sla_threshold, max_instances, code_, wf{"lang"} ?? "qore", lang_info,
                wf{"class-name"}, has_detach.toInt(), efh.id, af, df, oif, ehf, remote.toInt(), staticdata_type);

            # add workflowmap entry
            workflowmap.(wf.name).(wf.version) = workflowid;
            wf.oldkeys = ();

            # check for existing workflows with newer versions and output a warning
            *hash<auto> q = sqlif.omqp.select("select workflowid, version, autostart from workflows where name = %v "
                "and workflowid != %v", wf.name, workflowid);
            context (q) {
                # output a warning and skip if version is higher
                if (Util::compare_version(wf.version, %version) < 0) {
                    oload::warning("new workflow %s:%s (%d) has an earlier version number than existing workflow "
                        "%s:%s (%d)", wf.name, wf.version, workflowid, wf.name, %version, %workflowid);
                    continue;
                }
                if (!%autostart)
                    continue;
                # set autostart flag for older workflows with the same name to 0
                sqlif.omqp.exec("update workflows set autostart = 0 where workflowid = %v", %workflowid);
                if (o.verbose)
                    printf("%s:%s (%d) clearing autostart on superceded workflow (was: %d)\n", wf.name, %version, %workflowid, %autostart);
                else
                    mystat("U");
            }
        }

        # insert any workflow mappers
        foreach hash mh in (ml) {
            sqlif.omqp.exec("insert into workflow_mappers (workflowid, mapperid) values (%v, %v)", workflowid, mh.mapperid);
            if (o.verbose)
                printf("%s:%s (%d) adding mapper %s v%s (%d)\n", wf.name, wf.version, workflowid, mh.name, mh.version, mh.mapperid);
            else
                mystat("I");
        }

        # insert any workflow value maps
        foreach hash<auto> vmh in (vml) {
            sqlif.omqp.exec("insert into workflow_vmaps (workflowid, id) values (%v, %v)", workflowid, vmh.id);
            if (o.verbose)
                printf("%s:%s (%d) adding value map %s (%d)\n", wf.name, wf.version, workflowid, vmh.name, vmh.mapperid);
            else
                mystat("I");
        }

        # create audit msg
        audit.sourceLoadedEvent(NOTHING, workflowid, NOTHING, NOTHING, filename, "workflow", wf.name, wf.version,
            workflowid);

        processWorkflowGroups(wf, workflowid);

        insertTags("workflow", wf.name, workflowid, wf.tags);

        # only update cached workflows and add to validation list if the workflow is not marked as deprecated
        if (!wq.deprecated) {
            # reload omqmap workflow cache
            omqmap_reload.workflows{workflowid} = True;

            # add wf to reset and validation hashes
            valwh{workflowid} = {
                "id": workflowid,
                "name": wf.name,
                "version": wf.version,
            };
            omqmap_reload.workflows{workflowid} = True;
        }

        oload::createKeys(wf, workflowid);

        return workflowid;
    }

    #! must return a path usable for UserApi::getTypeFromPath()
    static string getPathFromTypeHash(hash<auto> type_hash) {
        string path = sprintf("%s/%s/%s", type_hash.type, type_hash.name, type_hash.path);
        if (type_hash.subtype) {
            path += "/" + type_hash.subtype;
        } else if (type_hash.type != "type") {
            path += "/record";
        }
        return QorusDataProviderTypeHelper::normalizeTypePath(path);
    }

    *list getMapperList(*softlist ml) {
        if (!ml)
            return;
        list l = map getMapperInfo($1), ml;
        # make sure mapper names are unique; no interface can reference more than 1 version of a given mapper
        hash h;
        foreach hash mh in (l) {
            if (h.(mh.name)) {
                if (h.(mh.name) == mh.version)
                    oload::error("\"mappers\" refers to mapper \"%s:%s\" more than once", mh.name, mh.version, mh.name);
                else
                    oload::error("\"mappers\" refers to multiple versions (%y and %y) of mapper %y; only one version of each named mapper may be referenced in an interface", h.(mh.name), mh.version, mh.name);
            }
            h.(mh.name) = mh.version;
        }
        return l;
    }

    hash getMapperInfo(string mapper) {
        trim mapper;
        (*string name, *string ver) = (mapper =~ x/(^[^:]+):([^:]+)$/);
        if (!ver) {
            if (name && mmap{name})
                oload::error("\"mappers\" declaration contains a mapper without a version identifier: %y; recommend changing to %y", mapper, mapper + ":" + (mmap{name}.lastKey()));
            else
                oload::error("\"mappers\" declaration contains unknown mapper without a version identifier: %y; check the mapper name and add an appropriate version identifier to the name separated by a colon", mapper);
        } else if (!mmap{name}{ver}) {
            if (mmap{name})
                oload::error("\"mappers\" declaration contains a reference to an unknown version of mapper %y (%y); recommend changing to %y", name, mapper, mapper + ":" + (mmap{name}.lastKey()));
            else
                oload::error("\"mappers\" declaration contains unknown mapper %y; check the mapper name and version and try again", mapper);
        }
        return mmap{name}{ver};
    }

    hash getMapperInfo(hash mh) {
        if (!exists mh.name)
            oload::error("\"mappers\" declaration contains a hash without the required \"name\" key; add the \"name\" key and try again (%y)", mh);
        if (!exists mh.version) {
            if (mmap.(mh.name))
                oload::error("\"mappers\" declaration contains a hash without the required \"version\" key (%y); recommend adding \"version\": \"%s\"", mh, mmap.name.lastKey());
            else
                oload::error("\"mappers\" declaration contains unknown mapper without the required \"version\" key (%y); check the mapper name and add an appropriate version key to the mapper hash", mh);
        }
        if (!mmap.(mh.name).(mh.version)) {
            if (mmap.(mh.name))
                oload::error("\"mappers\" declaration contains a hash referencing an unknown version \"version\" (%y); recommend adding \"version\": \"%s\"", mh, mmap.name.lastKey());
            else
                oload::error("\"mappers\" declaration contains a hash referencing an unknown mapper (%y); check the mapper name and version key and try again", mh);
        }
        return mmap.(mh.name).(mh.version);
    }

    nothing getMapperInfo(any mapper) {
        oload::error("\"mappers\" declaration contained an element of type %y; expecting \"string\" or \"hash\"", mapper.type());
    }

    *list<auto> getVMapList(*softlist<auto> vml) {
        if (!vml)
            return;
        return map getVMapInfo($1), vml;
    }

    hash<auto> getVMapInfo(string vmap) {
        trim vmap;
        if (!vmmap{vmap})
            oload::error("\"vmaps\" declaration contains unknown value map %y; check the name and try again", vmap);
        return vmmap{vmap};
    }

    nothing getVMapInfo(auto vmap) {
        oload::error("\"vmaps\" declaration contained an element of type %y; expecting \"string\"", vmap.type());
    }

    processWorkflowErrors(hash<auto> wf) {
        if (!wf.errorfunction) {
            return;
        }

        *hash<auto> efh = getFunctionInfo(wf.errorfunction, "ERROR FUNCTION ERROR");
        if (!efh) {
            return;
        }

        EM.initOnce();

        try {
            WorkflowInterfaceContainerBase wv(wf.workflowid);
            wv.init();

            # setup workflow data
            wv.loadErrorFunction(efh.name, efh.id);
            wv.parseCommit();

            # execute the error function (ignore any errors) and load in new errors
            *hash<auto> error_defintions = wv.pgm.callFunction(efh.name);
            processErrorDefinitions(wf, error_defintions);
        } catch (hash<ExceptionInfo> ex) {
            printf("%s: %s: %s\n", get_ex_pos(ex), ex.err, ex.desc);
            showEx(ex);
            oload::has_any_error = True;
        }
    }

    processYamlWorkflowErrors(hash<auto> workflow, string directory) {
        if (!workflow{"errors"}) {
            return;
        }

        *list<auto> error_defintions = parseAndValidateYaml({"file": directory + workflow{"errors"}}){"errors"};
        if (!error_defintions) {
            oload::error("file %y doesn't contain any error definitions", workflow{"errors"});
        }
        *hash<auto> errors = map ({$1.name: $1 - "name"}), error_defintions.first(){"errors"};

        EM.initOnce();
        try {
            processErrorDefinitions(workflow, errors);
        } catch (hash<ExceptionInfo> ex) {
            printf("%s: %s: %s\n", get_ex_pos(ex), ex.err, ex.desc);
            showEx(ex);
            oload::has_any_error = True;
        }
    }

    processErrorDefinitions(hash<auto> wf, *hash<auto> error_defintions) {
        hash<auto> esh;
        foreach string err in (keys error_defintions) {
            hash<auto> eh += {"error": err} + error_defintions{err};

            if (!eh.level) {
                eh.level = ErrLevelAuto;
            } else {
                if (!ErrLevelTypes.(eh.level)) {
                    oload::warning("error %y gives unrecognized level %y; assuming %y (valid levels: %y)", err,
                                   eh.level, ErrLevelAuto, ErrLevelTypes.keys());
                }
            }

            # only update the error if it hasn't already been manually updated
            switch (remove eh.level) {
                case ErrLevelGlobal: {
                    if (!EM.getGlobalError(err).manually_updated) {
                        string res = EM.updateGlobalError(eh, True);
                        ++esh{res};
                        if (res !~ /^(UNCHANGED|IGNORED)-/) {
                            reload_errors = True;
                        }
                    }
                }
                case ErrLevelWorkflow: {
                    if (!EM.hasManuallyUpdatedWorkflowError(wf.workflowid, err)) {
                        string res = EM.updateWorkflowError(wf.workflowid, eh, True);
                        ++esh{res};
                        if (res !~ /^(UNCHANGED|IGNORED)-/) {
                            reload_errors = True;
                        }
                    }
                }
                case ErrLevelAuto: {
                    if (!EM.hasManuallyUpdatedWorkflowError(wf.workflowid, err)) {
                        string res = EM.updateError(wf.workflowid, eh, True);
                        ++esh{res};
                        if (res !~ /^(UNCHANGED|IGNORED)-/) {
                            reload_errors = True;
                        }
                    }
                }
            }
        }
        if (esh && o.verbose) {
            printf("%s:%s (%d) processed error definitions: %y\n", wf.name, wf.version, wf.workflowid, esh);
        }
    }

    private processWorkflowGroups(hash wf, softint workflowid) {
        # get current group list to compare afterwards if any have been removed
        list<auto> og = exists gwmap{workflowid} ? gwmap{workflowid} : ();

        # check group membership
        foreach auto name in (wf.groups) {
            if (name.typeCode() != NT_STRING) {
                stderr.printf("ERROR: workflow %s:%s (%d) expects string group name, got type %s instead (value: %y)\n", wf.name, wf.version, workflowid, type(name), name);
                exit(1);
            }

            auto g = gmap{name};
            if (!exists g) {
                stderr.printf("ERROR: workflow %s:%s (%d) cannot be added to non-existent group %y\n", wf.name, wf.version, workflowid, name);
                exit(1);
            }

            # add workflow to group if it's not there already
            if (!inlist(workflowid, g.workflows)) {
                if (o.verbose)
                    printf("%s:%s (%d) adding to group %s (%d)\n", wf.name, wf.version, workflowid, name, g.id);
                else
                    mystat("+");

                addWorkflowToGroup(g.id, workflowid);
            } else if (o.verbose)
                printf("%s:%s (%d) is already a member of group %s (%d)\n", wf.name, wf.version, workflowid, name, g.id);
        }

        # check if workflow was previously a member of any other groups
        foreach string name in (og) {
            if (!inlist(name, wf.groups)) {
                if (o.verbose)
                    printf("%s:%s (%d) removing workflow from group %s (%d)\n", wf.name, wf.version, workflowid, name, gmap{name}.id);
                else
                    mystat("-");

                # remove workflow from group in DB
                on_success sqlif.omqp.commit();
                on_error sqlif.omqp.rollback();

                sqlif.deleteWorkflowsFromGroup(gmap{name}.id, workflowid);

                # clear list for workflow
                gwmap{workflowid} = select gwmap{workflowid}, $1 != name;

                # remove this workflow from group list
                gmap{name}.workflows = select gmap{name}.workflows, $1 != workflowid;

                # mark to synchronize group list with server
                reload_groups = True;
            }
        }
    }

    static createKeys(hash<auto> wf, softint wfid) {
        *list keysv = wf.oldkeys;
        if (!exists keysv) {
            keysv = sqlif.omqp.select("select keyname from workflow_keys where workflowid = %v", wfid).keyname;
        }

        # for each key in keysv and not in wf.keylist, delete
        foreach string k in (keysv) {
            if (!inlist(k, wf.keylist)) {
                # prevent FK constraint violated for FK_OIK_WFID_KEY using --allow-redef/-A
                # It can take notable amount of time in large systems but it should be fine for dev instances.
                sqlif.omqp.exec("delete from order_instance_keys where workflow_instanceid in (select workflow_instanceid from workflow_instance where workflowid = %v) and keyname = %v", wfid, k);
                # now it's safe to delete key definition
                sqlif.omqp.exec("delete from workflow_keys where workflowid = %v and keyname = %v", wfid, k);
            }
        }

        # for each key in wf.keylist and not in keysv, create
        foreach string k in (wf.keylist) {
            if (!inlist(k, keysv)) {
                sqlif.omqp.exec("insert into workflow_keys (workflowid, keyname, description) values ( %v, %v, %v)", wfid, k, NULL);
            }
        }
    }

    createWorkflowLibrary(hash<auto> wf, softint workflowid) {
        # delete workflow_lib for workflow
        sqlif.omqp.exec("delete from workflow_lib where workflowid = %v", workflowid);

        int load_order = 0;
        # create constant library
        foreach softstring constv in (wf.constants) {
            if (o.verbose)
                printf("%s:%s (%d) adding constant %s\n",
                       wf.name, wf.version, workflowid, constv);
            else
                mystat("I");
            sqlif.omqp.exec("insert into workflow_lib values ( %v, %v, %v, %v )", workflowid, 'CONSTANT', constv, load_order++);
        }
        # create class library
        foreach softstring classv in (wf.classes) {
            if (o.verbose)
                printf("%s:%s (%d) adding class %s\n",
                       wf.name, wf.version, workflowid, classv);
            else
                mystat("I");
            sqlif.omqp.exec("insert into workflow_lib values ( %v, %v, %v, %v )", workflowid, 'CLASS', classv, load_order++);
        }
        # create function library
        foreach softstring f in (wf.functions) {
            if (o.verbose)
                printf("%s:%s (%d) adding function %s()\n",
                       wf.name, wf.version, workflowid, f);
            else
                mystat("I");
            sqlif.omqp.exec("insert into workflow_lib values ( %v, %v, %v, %v )", workflowid, 'FUNCTION', f, load_order++);
        }
    }

    createInterfaceOptions(string type, hash<auto> ix, *hash<auto> options, softint id) {
        # create option hash of existing options
        hash<auto> oh;
        bool config;
        context (sqlif.omqp.select("select name, config, value from %s_options where %sid = %v", type, type, id)) {
            oh.%name = {
                "config": boolean(%config),
                "value": %value,
            };
            if (!config && %config) {
                config = True;
            }
        }

        # parse system options
        if (ix."system-options"."stack-size") {
            oh."stack-size".value = serialize_qorus_data(parse_memory_size(ix."system-options"."stack-size".value,
                True));
            options."stack-size" = OMQ::omq_option_hash."stack-size".desc;
        }

        if (options) {
            sqlif.omqp.exec("delete from %s_options where %sid = %v", type, type, id);
            foreach string opt in (options.keys()) {
                if (o.verbose)
                    printf("%s %s:%s (%d) %sing persistent option %y\n", type,
                           ix.name, ix.version, id, exists oh{opt} ? "keep" : "add", opt);
                else
                    mystat("I");

                if (strlen(options{opt}) > SQLDescLen) {
                    int ol = length(options{opt});
                    options{opt} = trunc_str(options{opt}, SQLDescLen, encoding);
                    if (!o.quiet)
                        stderr.printf("warning: description of %s %y option %y truncated to %d characters to fit in "
                            "DB column with max byte len of %d bytes (was %d characters)\n", type, ix.name, opt,
                            length(options{opt}), SQLDescLen, ol);
                }

                sqlif.insertInterfaceOptionRawNoCommit(type, id, opt, options{opt}, True, (remove oh{opt}).value);
            }
            # reinsert any system options still left
            foreach string opt in (keys oh) {
                if (!OMQ::omq_option_hash{opt})
                    continue;

                if (o.verbose)
                    printf("%s %s:%s (%d) keeping persistent option %y\n", type, ix.name, ix.version, id, opt);
                else
                    mystat("I");

                sqlif.insertInterfaceOptionRawNoCommit(type, id, opt, OMQ::omq_option_hash{opt}.desc, False,
                    oh{opt}.value);
            }
        } else if (config) {
            # interface no longer has any configured options, so delete all configured options
            foreach string opt in (keys oh) {
                if (!oh{opt}.config)
                    continue;

                if (o.verbose) {
                    printf("%s %s:%s (%d) deleting unused persistent option %y\n", type, ix.name, ix.version, id,
                        opt);
                } else {
                    mystat("D");
                }

                sqlif.deleteInterfaceOptionNoCommit(type, id, opt);
            }
        }
    }

    createCustomStatuses(hash<auto> wf, softint workflowid) {
        sqlif.omqp.exec("delete from custom_statuses where workflowid = %v", workflowid);
        foreach string stat in (keys wf.statuses) {
            if (o.verbose)
                printf("%s:%s (%d) adding custom status %y\n",
                       wf.name, wf.version, workflowid, stat);
            else
                mystat("I");

            if (strlen(wf.statuses{stat}) > SQLDescLen) {
                int ol = length(wf.statuses{stat});
                wf.statuses{stat} = trunc_str(wf.statuses{stat}, SQLDescLen, encoding);
                if (!o.quiet)
                    stderr.printf("warning: description of workflow %y custom status %y truncated to %d characters to fit in DB column with max byte len of %d bytes (was %d characters)\n", wf.name, stat, length(wf.statuses{stat}), SQLDescLen, ol);
            }

            sqlif.omqp.exec("insert into custom_statuses (workflowid, statusid, description) values (%v, %v, %v)", workflowid, stat, wf.statuses{stat});
        }
    }

    # make a hash of the unique steps in this workflow
    makeSteps(reference<auto> steph, reference<auto> wf) {
        foreach auto step in (\wf.steps) {
            addStep(\steph, \step);
        }
    }

    createQueues(hash<auto> queues) {
        map createQueue($1.value + {"name": $1.key}), queues.pairIterator();
    }

    private:internal createQueue(hash<auto> queue, bool commit = False) {
        on_success if (commit) sqlif.omqp.commit();
        on_error if (commit) sqlif.omqp.rollback();

        # check for description
        if (!exists queue.desc) {
            stderr.printf("ERROR: %s: no description given for queue\n", queue.name);
            exit(1);
        }

        if (strlen(queue.desc) > SQLDescLen) {
            int ol = length(queue.desc);
            queue.desc = trunc_str(queue.desc, SQLDescLen, encoding);
            if (!o.quiet) {
                stderr.printf("warning: description of queue %y truncated to %d characters to fit in DB column with "
                              "max byte len of %d bytes (was %d characters)\n", queue.name, length(queue.desc), SQLDescLen, ol);
            }
        }

        *softint queueid = sqlif.omqp.selectRow("select queueid from queues where name = %v", string(queue.name)).queueid;
        if (!exists queueid) {
            queueid = get_next_sequence_value("seq_queues");
            # insert qmap entry
            qmap{queue.name} = queueid;
            qrmap{queueid} = queue.name;
            if (o.verbose) {
                printf("queue %s (%d) inserting\n", queue.name, queueid);
            } else {
                mystat("I");
            }
            sqlif.omqp.exec("insert into queues values (%v, %v, %v, %v, %v, %v)", queueid, queue.name, queue.desc, NULL, NULL,
                            NULL);
        } else {
            if (o.verbose) {
                printf("queue %s (%d) updating\n", queue.name, queueid);
            } else {
                mystat(".");
            }
            sqlif.omqp.exec("update queues set description = %v where queueid = %v", queue.desc, queueid);
        }

        omqmap_reload.queues{queueid} = True;
    }

    createGroups(auto groups) {
        error("ERROR: the 'groups' global variable in a workflow definition file must be assigned to a "
              "hash, where the hash keys are the group names and the values are hashes containing at least"
              " a 'desc' key, giving the description for each group; type given: %s\n", type(groups));
    }

    createGroups(hash<auto> groups, bool quiet = False) {
        if (!o.verbose && !quiet && !o.quiet) {
            printf("processing interface groups: ");
        }

        foreach string name in (keys groups) {
            oload::validateObjectName(name);

            hash group = groups{name};
            if (!exists group.desc) {
                stderr.printf("ERROR: %s: no description given for group (%y)\n", name, group);
                exit(1);
            }
            #printf("DBG: createGroups %y gmap: %y\n", name, gmap);
            createGroupInternal(name, group.desc, quiet);
        }

        if (!o.verbose && !o.quiet && !quiet) {
            print(" OK\n");
        }
    }

    private:internal createGroupInternal(string name, string desc, bool quiet = False) {
        if (strlen(desc) > SQLDescLen) {
            int ol = length(desc);
            desc = trunc_str(desc, SQLDescLen, encoding);
            if (!o.quiet && !quiet) {
                stderr.printf("warning: description of group %y truncated to %d characters to fit in DB column "
                              "with max byte len of %d bytes (was %d characters)\n", name, length(desc),
                              SQLDescLen, ol);
            }
        }


        *softint groupid = gmap{name}.id;
        if (!exists groupid) {
            on_success sqlif.omqp.commit();
            on_error sqlif.omqp.rollback();

            groupid = sqlif.createGroup(name, desc);

            # insert map entries
            gmap{name} = {
                "name"      : name,
                "id"        : groupid,
                "desc"      : desc,
                "workflows" : (),
                "services"  : (),
            };

            grmap{groupid} = name;
            if (!o.quiet && !quiet) {
                if (o.verbose) {
                    printf("group %s (%d) inserting\n", name, groupid);
                } else {
                    mystat("I");
                }
            }

            reload_groups = True;
        } else {
            on_success sqlif.omqp.commit();
            on_error sqlif.omqp.rollback();

            if (!o.quiet && !quiet) {
                if (o.verbose) {
                    printf("group %s (%d) updating\n", name, groupid);
                } else {
                    mystat(".");
                }
            }

            if (desc != gmap{name}.desc) {
                sqlif.omqp.exec("update groups set description = %v where groupid = %v", desc, groupid);
                reload_groups = True;
            }
        }
    }

    createEvents(hash<auto> events) {
        if (events.typeCode() != NT_HASH) {
            stderr.printf("ERROR: the 'events' global variable in a workflow definition file must be assigned to a hash, where the hash keys are the event names and the values are hashes containing at least a 'desc' key, giving the description for each event; type given: %s\n", type(events));
            exit(1);
        }

        map createEvent($1.value + {"name": $1.key}), events.pairIterator();
    }

    private:internal createEvent(hash<auto> event, bool commit = False) {
        if (commit) {
            on_success sqlif.omqp.commit();
            on_error sqlif.omqp.rollback();
        }

        # check for description
        if (!exists event{"desc"}) {
            stderr.printf("ERROR: %s: no description given for event (%y)\n", event{"name"}, event);
            exit(1);
        }

        if (strlen(event{"desc"}) > SQLDescLen) {
            int ol = length(event{"desc"});
            event{"desc"} = trunc_str(event{"desc"}, SQLDescLen, encoding);
            if (!o.quiet) {
                stderr.printf("warning: description of event %y truncated to %d characters to fit in DB column with "
                              "max byte len of %d bytes (was %d characters)\n", event{"name"}, length(event{"desc"}),
                              SQLDescLen, ol);
            }
        }

        *softint eventid = sqlif.omqp.selectRow("select workflow_event_typeid from workflow_event_types where name = %v",
                                                event{"name"}).workflow_event_typeid;
        if (!exists eventid) {
            eventid = get_next_sequence_value("seq_workflow_event_types");
            # insert emap entry
            emap{event{"name"}} = eventid;
            ermap{eventid} = event{"name"};
            if (o.verbose) {
                printf("event %s (%d) inserting\n", event{"name"}, eventid);
            } else {
                mystat("I");
            }
            sqlif.omqp.exec("insert into workflow_event_types values (%v, %v, %v, %v, %v)", eventid, event{"name"},
                            event{"desc"}, NULL, NULL);
        } else {
            if (o.verbose) {
                printf("event %s (%d) updating\n", event{"name"}, eventid);
            } else {
                mystat(".");
            }
            sqlif.omqp.exec("update workflow_event_types set description = %v where workflow_event_typeid = %v",
                            event{"desc"}, eventid);
        }

        omqmap_reload.events{eventid} = True;
    }

    processHookFunction(reference<auto> wf) {
        *hash<auto> hfi = getFunctionInfo(wf.hookfunction, "HOOK FUNCTION ERROR");
        softint fid = hfi.id;

        string b = sqlif.omqp.selectRow("select body from function_instance where function_instanceid = %v", fid).body;

        # get the workflow Program container
        WorkflowProgram p(options, NOTHING, PO_NO_USER_API);

        # import fake workflow function API
        qorus_load_fake_workflow_api_module(p);

        # parse hook function
        p.parse(b, "<hookfunction>");

        # retrieve hooks
        hash<auto> h = p.callFunction(hfi.name);

        # process one time init
        if (h.onetimeinitialization) {
            # assume version 1.0 for one time init function
            wf.onetimeinit = sprintf("%s:1.0", h.onetimeinitialization);
            # remove from the library list
            remove_from_list(\h.lib, h.onetimeinitialization);
        }

        # create function library
        if (h.lib)
            wf.functions = h.lib;

        #printf("hooks=%N\nwf.functions=%N\n", h, wf.functions);
    }

    *int checkWorkflow(softfloat fv, reference<auto> wf) {
        if (fv < 1.2) {
            if (exists wf.classes) {
                printf("%s:%s: class library not supported with format_version < 1.2\n", wf.name, wf.version);
                return -1;
            }
            if (exists wf.constants) {
                printf("%s:%s: constant library not supported with format_version < 1.2\n", wf.name, wf.version);
                return -1;
            }
            if (exists wf.functions) {
                printf("%s:%s: function library not supported with format_version < 1.2\n", wf.name, wf.version);
                return -1;
            }
            if (exists wf.attach) {
                printf("%s:%s: attach not supported with format_version < 1.2\n", wf.name, wf.version);
                return -1;
            }
            if (exists wf.detach) {
                printf("%s:%s: detach not supported with format_version < 1.2\n", wf.name, wf.version);
                return -1;
            }
            if (exists wf.initstep) {
                wf.attach = wf.initstep;
                delete wf.initstep;
            }
            if (exists wf.onetimeinit) {
                printf("%s:%s: onetimeinit not supported with format_version < 1.2\n", wf.name, wf.version);
                return -1;
            }
            if (exists wf.error_handler) {
                printf("%s:%s: error_handler not supported with format_version < 1.2\n", wf.name, wf.version);
                return -1;
            }
            if (exists wf.hookfunction)
                processHookFunction(\wf);
        } else {
            if (exists wf.initstep) {
                printf("%s:%s: initstep is not supported with format_version >= 1.2\n", wf.name, wf.version);
                return -1;
            }
            if (exists wf.hookfunction) {
                printf("%s:%s: hook functions are not allowed with format_version >= 1.2", wf.name, wf.version);
                return -1;
            }
        }
        if (fv >= 2.0) {
            if (exists wf.initflow) {
                printf("%s:%s: initflow is not applicable with format_version >= 2.0", wf.name, wf.version);
                return -1;
            }
            if (!exists wf.steps) {
                printf("%s:%s: 'steps' key required in workflow definition with format_version >= 2.0", wf.name, wf.version);
                return -1;
            }
        }
        if (fv >= 2.6) {
            hash testwf = wf - ValidWorkflowKeys;
            if (testwf.size() > 0) {
                warning("%s:%s: invalid keys found in workflow definition: %y", wf.name, wf.version, testwf.keys());
                return -1;
            }
        }
    }

    insertTags(string t, string name, int id, *hash<auto> uinfo) {
        if (uinfo.application) {
            checkApplications(t, name, uinfo.application);
        }

        #printf("%s_tags: id: %d %y\n", t, id, uinfo);
        map sqlif.omqp.exec("insert into %s_tags (%sid, tag, value) values (%v, %v, %v)", t, t, id, $1.key, $1.value), uinfo.pairIterator();
    }

    static showEx(hash<auto> ex) {
        if (ex.callstack) {
            printf("call stack:\n");
            foreach hash l in (ex.callstack)
                printf("  %s() called at %s (%s function)\n", l.function, get_ex_pos(l), l.type);
        }
    }

    # this is only used in refresh server. we assume that nobody cares if the
    # refresh (if used) fails. so we only print this out in verbose mode
    printResult(auto r) {
        if (!o.quiet)
            print("OK\n");
    }

    refreshServer() {
        try {
            # the following will fail if the system is down and cause all updates to be silently skipped
            string url = qrest.getURL();
            qrest.connect();

            hash<auto> info;

            # reload objects in the server if possible
            if (omqmap_reload) {
                try {
                    # convert hashes to lists
                    hash<auto> ah;
                    foreach hash<auto> i in (omqmap_reload.pairIterator()) {
                        ah{i.key} = (map (!StrMap{i.key} && $1.intp() ? $1.toInt() : $1), keys i.value);
                    }
                    if (!o.quiet) {
                        if (o.verbose) {
                            printf("resetting metadata and objects: %y: ", ah);
                        } else {
                            int count;
                            map count += $1.size(), ah.iterator();
                            printf("resetting metadata (%d object%s): ", count, count == 1 ? "" : "s");
                        }
                        flush();
                    }
                    *hash<auto> r = qrest.put("system/metadata?action=reload", ah, NOTHING, \info);
                    printResult(r);
                } catch (hash<ExceptionInfo> ex) {
                    if (o.quiet)
                        print("reload ");
                    stderr.print("failed: " + qrest.getRestErrorMessage(ex, info) + "\n");
                    # issue #2571: make sure that errors are flagged for the return code
                    oload::has_any_error = True;
                }
            }

            # send a REST reload cmd for connections
            foreach string conn_type in (keys reload_connections) {
                string api_path = "remote/" + conn_type + "/reload";

                if (!o.quiet) {
                    printf("sending REST PUT %s to %y: ", api_path, url);
                    flush();
                }
                try {
                    hash rvh = qrest.put(api_path, NOTHING, NOTHING, \info);
                    printResult(rvh);
                } catch (hash<ExceptionInfo> ex) {
                    if (o.quiet)
                        printf("%s connection reload ", conn_type);
                    stderr.print("failed: " + qrest.getRestErrorMessage(ex, info) + "\n");
                    # issue #2571: make sure that errors are flagged for the return code
                    oload::has_any_error = True;
                }
            }

            if (reload_errors) {
                if (!o.quiet) {
                    printf("sending REST PUT errors?action=reload to %y: ", url);
                    flush();
                }
                try {
                    hash<auto> r = qrest.put("errors?action=reload", NOTHING, NOTHING, \info);
                    printResult(r);
                } catch (hash<ExceptionInfo> ex) {
                    if (o.quiet)
                        printf("error reload ");
                    stderr.print("failed: " + qrest.getRestErrorMessage(ex, info) + "\n");
                    # issue #2571: make sure that errors are flagged for the return code
                    oload::has_any_error = True;
                }
            }

            if (reload_groups) {
                if (!o.quiet) {
                    printf("sending REST PUT system/rbac?action=reload to %y: ", url);
                    flush();
                }
                try {
                    string ok = qrest.put("system/rbac?action=reload", NOTHING, NOTHING, \info);
                    printResult(ok);
                } catch (hash<ExceptionInfo> ex) {
                    if (o.quiet)
                        printf("group reload ");
                    stderr.print("failed: " + qrest.getRestErrorMessage(ex, info) + "\n");
                    # issue #2571: make sure that errors are flagged for the return code
                    oload::has_any_error = True;
                }
            }
        } catch (hash<ExceptionInfo> ex) {
            printf("server down; no refresh performed\n");
        }
    }

    *int createWorkflowQore(Program p, string filename, *ReleaseFile f) {
        auto workflows       = p.getGlobalVariable("workflows");
        auto flows           = p.getGlobalVariable("flows");
        auto queues          = p.getGlobalVariable("queues");
        auto events          = p.getGlobalVariable("events");
        auto format_version  = p.getGlobalVariable("format_version");
        auto groups          = p.getGlobalVariable("groups");

        if (!workflows)
             throw "WORKFLOW-DEFINITION-ERROR", sprintf("Variable 'workflows' was not defined in file %s.", filename);

        # issue warning if "orders" structure present
        bool existsv;
        if (p.getGlobalVariable("orders", \existsv) || existsv)
            print("warning: orders no longer supported\n");

        # check format version
        if (!exists format_version) {
            printf("%s: required variable 'format_version' is not defined (supported versions: %y)\n",
                   filename, FormatVersions.keys());
            return -1;
        }

        if (!FormatVersions{format_version}) {
            printf("%s: is in unsupported format '%s', supported versions: %y\n",
                   filename, format_version, FormatVersions.keys());
            return -1;
        }

        format_version = float(format_version);

        # create queues
        if (exists queues) {
            createQueues(queues);
        }

        # create events
        if (exists events) {
            # check for minimum format_version
            if (format_version < 2.6) {
                printf("workflow events not supported with format_version < 2.6 (format_version given: %y)\n", format_version);
                return -1;
            }

            createEvents(events);
        }

        # create groups
        if (exists groups) {
            if (format_version < 2.6) {
                printf("interface groups not supported with format_version < 2.6 (format_version given: %y)\n", format_version);
                return -1;
            }

            createGroups(groups, !oload.o.verbose);
        }

        # process flows for backwards-compatibility
        {
            *list<string> fnl = keys flows;

            if (elements fnl) {
                if (format_version >= 2.0)
                    throw "WORKFLOW-DEFINITION-ERROR", sprintf("no flows may be defined with format_version >= 2.0, steps dependencies must be defined in the 'steps' key in the workflow definition; flows defined: %y", fnl);

                # move flow steps to workflows
                foreach string wfn in (keys workflows) {
                    foreach string wfv in (keys workflows{wfn}) {
                        if (!exists workflows{wfn}{wfv}.initflow)
                            throw "WORKFLOW-DEFINITION-ERROR", sprintf("workflow %s/%s does not define the 'initflow' tag", wfn, wfv);

                        (*string fn, *string fv) = (workflows{wfn}{wfv}.initflow =~ x/(.+):(.+)/);
                        if (!exists fn || !exists fv)
                            throw "WORKFLOW-DEFINITION-ERROR", sprintf("cannot parse 'initflow' tag=%y for flow name and version for workflow %s/%s", workflows{wfn}{wfv}.initflow, wfn, wfv);

                        if (!exists flows{fn}{fv})
                            throw "WORKFLOW-DEFINITION-ERROR", sprintf("workflow %s/%s references flow %s/%s that is not defined in the workflow definition file", wfn, wfv, fn, fv);

                        workflows{wfn}{wfv}.steps = flows{fn}{fv}.steps;
                    }
                }
            }
        }

        # get a data structure with unique steps for all workflows in this file
        hash<auto> steph;
        # create steps data structure from data in workflows
        foreach string wfn in (keys workflows) {
            foreach string wfv in (keys workflows{wfn}) {
                makeSteps(\steph, \workflows{wfn}{wfv});
            }
        }

        if (!steph)
            throw "WORKFLOW-DEFINITION-ERROR", sprintf("%s: does not contain any step definitions", filename);

        # create steps
        if (o.verbose && !o.quiet)
            printf("processing steps from %s: ", filename);
        bool has_metadata = createSteps(steph);

        if (o.verbose && !o.quiet)
            printf("processing workflows from %s: ", filename);

        # create WORKFLOWS table
        foreach string wf in (keys workflows) {
            foreach string version in (keys workflows{wf}) {
                if (exists workflows{wf}{version}.group && format_version < 2.6) {
                    printf("interface groups not supported with format_version < 2.6 (format_version given: %y)\n", format_version);
                    return -1;
                }

                if (!o.quiet && o.verbose) {
                    printf("%y: ", filename);
                }

                oload::validateObjectName(wf);

                on_success sqlif.omqp.commit();
                on_error sqlif.omqp.rollback();

                reference wfh = \workflows{wf}{version};

                wfh.name = wf;
                wfh.version = version;
                if (checkWorkflow(format_version, \wfh))
                    return -1;

                int workflowid = insertOrUpdateWorkflow(wfh, filename);
                wfh.workflowid = workflowid;

                createWorkflowSegments(wfh);

                createWorkflowLibrary(wfh, workflowid);

                createInterfaceOptions("workflow", wfh, wfh.options, workflowid);

                createCustomStatuses(wfh, workflowid);
                processWorkflowErrors(wfh);

                if (f) {
                    string source = sprintf("%y", workflows{wf}{version} - "workflowid");
                    f.add(wf, source, version, workflowid);
                }
            }
        }

        bool do_ok = True;
        # validate workflow now and create metadata if necessary
        if (has_metadata) {
            foreach string wf in (keys workflows) {
                foreach string version in (keys workflows{wf}) {
                    hash<auto> wfh = workflows{wf}{version};
                    WorkflowValidator val({"id": wfh.workflowid} + wfh{"name", "version"});
                    if (!o.quiet && o.verbose) {
                        printf("validated workflow %s/%s (%d) on load\n", wfh.name,
                            wfh.version, wfh.workflowid);
                        do_ok = False;
                    }
                    # remove workflow from validation hash
                    remove oload.valwh{wfh.workflowid};
                    # make sure that workflow does not get validated again
                    oload.already_validated_map.workflow{wfh.workflowid} = True;
                }
            }
        }
    }

    *int createWorkflowYaml(hash<auto> yaml_wf, string filename, *ReleaseFile release_file) {
        if (!o.quiet && o.verbose) {
            printf("%y: processing workflows from YAML workflow file: ", filename);
        }

        # create WORKFLOWS table
        oload::validateObjectName(yaml_wf.name);

        {
            on_success sqlif.omqp.commit();
            on_error sqlif.omqp.rollback();

            int workflowid = insertOrUpdateWorkflow(yaml_wf, filename);
            yaml_wf.workflowid = workflowid;

            createWorkflowSegments(yaml_wf, True);

            createWorkflowLibrary(yaml_wf, workflowid);

            createInterfaceOptions("workflow", yaml_wf, yaml_wf.options, workflowid);

            createCustomStatuses(yaml_wf, workflowid);
            processYamlWorkflowErrors(yaml_wf, dirname(filename) + Qore::DirSep);
        }

        if (release_file) {
            string source = sprintf("%y", yaml_wf - "workflowid");
            release_file.add(yaml_wf.name, source, version, yaml_wf.workflowid);
        }

        bool do_ok = True;
        # validate workflow now and create metadata if necessary
        WorkflowValidator val({"id": yaml_wf.workflowid} + yaml_wf{"name", "version"});
        if (!o.quiet && o.verbose) {
            printf("validated workflow %s/%s (%d) on load\n", yaml_wf.name, yaml_wf.version,
                yaml_wf.workflowid);
            do_ok = False;
        }
        # remove workflow from validation hash
        remove oload.valwh{yaml_wf.workflowid};
        # make sure that workflow does not get validated again
        oload.already_validated_map.workflow{yaml_wf.workflowid} = True;

        InterfaceConfigContainer::importWorkflowConfigItemValues(yaml_wf);
    }

    getMaps() {
        # create functionrmap
        *hash q = sqlif.omqp.select("select FUNCTION_INSTANCEID, NAME, VERSION, author, CREATED, FUNCTION_TYPE, PATCH from function_instance");

        delete frmap;
        context (q) {
            fmap.%function_instanceid = %%;

            frmap.%name.%version = {
                "type": %function_type,
                "version": %version,
                "author": %author,
                "id": %function_instanceid,
                "created": %created,
            };
        }

        # set lastversion for each function
        foreach string func in (keys frmap) {
            date d = date(0);
            string version;
            foreach string v in (keys frmap{func}) {
                if (frmap{func}{v}.created > d) {
                    d = frmap{func}{v}.created;
                    version = v;
                }
            }
            frmap{func}.lastversion = version;
        }

        # read in workflow name, version to workflowid mappings
        q = sqlif.omqp.select("select workflowid, name, version from workflows");
        context (q)
            workflowmap.%name.%version = %workflowid;

        # create smap
        q = sqlif.omqp.select("select name, version, serviceid, service_type, created from services");
        context (q)
            if (!exists smap.%name || %created > smap.%name.created)
            smap.%name = (
                "version"   : %version,
                "serviceid" : %serviceid,
                "created"   : %created,
                "type"      : %service_type,
            );

        # create qmap
        context (sqlif.omqp.select("select name, queueid from queues")) {
            qmap.%name = %queueid;
            qrmap.%queueid = %name;
        }

        # create emap
        context (sqlif.omqp.select("select name, workflow_event_typeid from workflow_event_types")) {
            emap.%name = %workflow_event_typeid;
            ermap.%workflow_event_typeid = %name;
        }

        # create constrmap
        context (sqlif.omqp.select("select CONSTANTID, NAME, VERSION, DESCRIPTION, CREATED, MODIFIED, PATCH from constants order by name, created")) {
            constrmap.%name.%version = %constantid;
            constrmap.%name.lastversion = %version;
            constmap.%constantid = %%;
        }
        #printf("constrmap=%N\n", constrmap);

        # create classrmap
        getClassMap();

        # create mapper maps
        context (sqlif.omqp.select("select mapperid, name, version, type, description, created, modified, patch from mappers order by name, created")) {
            mmap.%name.%version = mrmap.%mapperid = %%;
        }

        # create value map maps
        context (sqlif.omqp.select("select id, name, description, created, modified from value_maps order by name, created")) {
            vmmap.%name = vmrmap.%id = %%;
        }

        # create group maps
        context (sqlif.getGroups()) {
            gmap.%name = {
                "name"      : %name,
                "desc"      : %description,
                "id"        : int(%groupid),
                "workflows" : (),
                "services"  : (),
                "jobs"      : (),
                "mappers"   : (),
                "vmaps"     : (),
                "fsms"      : (),
                "pipelines" : (),
            };

            grmap.%groupid = %name;
        }

        # create step name map
        context (sqlif.omqp.select("select name, version, stepid, created from steps order by name, created")) {
            step_name_map.%name = %%;
        }

        # create class name map
        context (sqlif.omqp.select("select name, version, classid, created from classes order by name, created")) {
            class_name_map.%name = %%;
        }

        # setup service, workflow, job, mapper, and vmap maps
        code setupMap = sub(*hash gq, string type, string mapv, *string col) {
            if (!col)
                col = type + "id";
            type = type + "s";

            context (gq) {
                *string name = grmap.%groupid;
                if (exists name) {
                    auto id = %%{col};
                    if (id.toInt() == id) {
                        id = id.toInt();
                    }
                    gmap{name}{type} += id;

                    # add group name to lookup map
                    if (!exists self{mapv}{id})
                        self{mapv}{id} = ();
                    self{mapv}{id} += name;
                }
            }
        };

        # add services to group map
        setupMap(sqlif.getGroupServices(), "service", "gsmap");

        # add workflows to group map
        setupMap(sqlif.getGroupWorkflows(), "workflow", "gwmap");

        # add jobs to group map
        setupMap(sqlif.getGroupJobs(), "job", "gjmap");

        # add mappers to group map
        setupMap(sqlif.getGroupMappers(), "mapper", "gmmap");

        # add vmaps to group map
        setupMap(sqlif.getGroupVMaps(), "vmap", "gvmmap", "id");

        # add fsms to group map
        setupMap(sqlif.getGroupFsms(), "fsm", "gfmap", "fsm");

        # add pipelines to group map
        setupMap(sqlif.getGroupPipelines(), "pipeline", "gpmap", "pipeline");
    }

    getClassMap() {
        if (!classmap) {
            context (sqlif.omqp.select("select CLASSID, NAME, VERSION, DESCRIPTION, CREATED, MODIFIED, PATCH, LANGUAGE from classes order by name, created")) {
                classrmap.%name.%version = %classid;
                classrmap.%name.lastversion = %version;
                classmap.%classid = %%;
            }
        }
    }

    string getSchemaToolCmd() {
        if (exists schema_tool)
            return schema_tool;

        string dir;
        if (ENV.OMQ_DIR == "LSB")
            dir = "/usr/bin";
        else if (strlen(ENV.OMQ_DIR))
            dir = ENV.OMQ_DIR + DirSep + "bin";

        if (exists dir && my_is_executable(dir + DirSep + "schema-tool")) {
            #printf("got dir=%n\n", dir);
            schema_tool = dir + DirSep + "schema-tool";
            return schema_tool;
        }

        # look in the same directory as this script
        dir = get_script_dir();
        if (my_is_executable(dir + DirSep + "schema-tool")) {
            schema_tool = normalize_dir(dir) + DirSep + "schema-tool";
            return schema_tool;
        }

        # look in the path
        ENV.PATH =~ s/::+/:/g;
        ENV.PATH =~ s/(^:+)|(:+$)//;

        foreach dir in (split(":", ENV.PATH)) {
            if (my_is_executable(dir + DirSep + "schema-tool")) {
                schema_tool = normalize_dir(dir) + DirSep + "schema-tool";
                return schema_tool;
            }
        }

        stderr.printf("ERROR: cannot find schema-tool to execute SQL script\n");
        exit(1);

        # to avoid a warning
        return "";
    }

    runSchema(string dsname, Datasource ds, string file) {
        string driver = ds.getDriverName();

        # NOTE: all schema files must be in UTF-8 format
        # substitute driver name
        file = regex_subst(file, "%driver%", driver);

        if (!is_file(file))
            throw "MISSING-SCHEMA-FILE", sprintf("can't find schema file '%s'", file);

        # for options to the "schema-tool" command
        string opts;

        # pass through --verbose option
        if (o.verbose)
            opts += sprintf("-v=%d ", o.verbose);

        # add any data tablespace name
        if (o.dts{dsname})
            opts += sprintf("--data-ts=%s=%s ", dsname, o.dts{dsname});

        # add any index tablespace name
        if (o.its{dsname})
            opts += sprintf("--index-ts=%s=%s ", dsname, o.its{dsname});

        string st = getSchemaToolCmd();
        string cmd = sprintf("%s %s-r%s=\"%s\"", st, opts, dsname, file);

        if (!o.quiet)
            printf("launching: %s\n", cmd);

        system(cmd);
    }

    runSchemaFile(string dsname, string arg) {
        # check if file exists
        if (!exists stat(arg)) {
            printf("ERROR: schema file %y cannot be opened\n", arg);
            exit(1);
        }
        # get datasource
        Datasource ds = omqclient.getDatasource(dsname);
        if (!o.quiet)
            printf("running Qorus schema file %y in %s@%s\n", arg, ds.getUserName(), ds.getDBName());
        runSchema(dsname, ds, arg);
    }

    runSchemaFiles() {
        # check if all files exist
        foreach hash f in (\o.schema) {
            if (!exists stat(f.arg)) {
                stderr.printf("ERROR: schema file %y cannot be opened\n", f.arg);
                exit(1);
            }
            # get datasource
            f.ds = omqclient.getDatasource(f.dsname);
        }

        foreach hash f in (o.schema) {
            if (!o.quiet)
                printf("running Qorus schema file %y in %s@%s\n", f.arg, f.ds.getUserName(), f.ds.getDBName());
            runSchema(f.dsname, f.ds, f.arg);
        }
    }

    updateWFValidationLists(*hash wfsqlresult) {
        context (wfsqlresult) {
            if (!valwh.%workflowid && !already_validated_map.workflow{%workflowid}) {
                if (o.verbose) {
                    printf("Adding workflow %s %s to the validation list\n", %name, %version);
                }
                valwh.%workflowid = {
                    "id": %workflowid,
                    "name": %name,
                    "version": %version,
                    "added-recursive": True,
                };
                omqmap_reload.workflows.%workflowid = True;
            }
        }
    }

    updateSValidationLists(*hash<auto> ssqlresult) {
        context (ssqlresult) {
            if (!valsh.%serviceid && !already_validated_map.service{%serviceid}) {
                if (o.verbose)
                    printf("Adding service %s %s to the validation list\n", %name, %version);
                valsh.%serviceid = {
                    "id": %serviceid,
                    "type": %service_type.lwr(),
                    "name": %name,
                    "version": %version,
                    "added-recursive": True,
                };
                if (%language == "python") {
                    valsh.%serviceid.has_python = True;
                }
                omqmap_reload.services.%serviceid = True;
            }
        }
    }

    updateJValidationLists(*hash<auto> ssqlresult) {
        context (ssqlresult) {
            if (!valjh.%jobid) {
                if (o.verbose)
                    printf("Adding job %s %s to the validation list\n", %name, %version);
                valjh.%jobid = {
                    "id": %jobid,
                    "name": %name,
                    "version": %version,
                    "added-recursive": True,
                };
                if (%language == "python") {
                    valjh.%jobid.has_python = True;
                }
                omqmap_reload.jobs.%jobid = True;
            }
        }
    }

    updateMValidationLists(*hash<auto> sqlresult) {
        context (sqlresult) {
            if (!valmh.%mapperid) {
                if (o.verbose)
                    printf("Adding mapper %s %s to the validation list\n", %name, %version);
                valmh.%mapperid = (
                    "id": %mapperid,
                    "name": %name,
                    "version": %version,
                    "added-recursive": True,
                    );
                omqmap_reload.mappers.%mapperid = True;
            }
        }
    }

    updateFsmValidationLists(*hash<auto> sqlresult) {
        context (sqlresult) {
            if (!val_fsm_map.%name) {
                if (o.verbose)
                    printf("Adding FSM %y to the validation list\n", %name);
                val_fsm_map.%name = True;
                omqmap_reload.fsm{%name} = True;
            }
        }
    }

    updatePipelineValidationLists(*hash<auto> sqlresult) {
        context (sqlresult) {
            if (!val_pipeline_map.%name) {
                if (o.verbose)
                    printf("Adding pipeline %y to the validation list\n", %name);
                val_pipeline_map.%name = True;
                omqmap_reload.pipelines{%name} = True;
            }
        }
    }

    static synchronized AbstractTable getTable(string name) {
        if (!tables{name}) {
            tables{name} = (new Table(sqlif.omqp, name)).getTable();
        }
        return tables{name};
    }

    updateValidationLists() {
        string sql;
        *hash q;

        # issue
        # list of function IDs
        # issue #2005: ensure that fl and fll are assigned to avoid type errors
        softlist fl = map $1.toInt(), keys valfh;
        # list of potential library function IDs
        list fll = fl;
        map fll += $1.toInt(), keys valflh;

        # get latest version of all services
        softlist<auto> mysql_svc_list;
        code mysql_get_serviceid_list = list<auto> sub () {
            if (!exists mysql_svc_list)
                mysql_svc_list = sqlif.omqp.select("select serviceid from services s1, (select name, max(created) created from services group by name) s2 where s1.name = s2.name and s1.created = s2.created").serviceid;
            return mysql_svc_list;
        };

        AbstractTable workflows_table = oload::getTable("workflows");
        AbstractTable workflow_steps_table = oload::getTable("workflow_steps");

        AbstractTable steps_table = oload::getTable("steps");
        AbstractTable workflow_lib_table = oload::getTable("workflow_lib");

        AbstractTable step_lib_table = oload::getTable("step_lib");

        AbstractTable services_table = oload::getTable("services");
        AbstractTable service_lib_table = oload::getTable("service_lib");

        AbstractTable jobs_table = oload::getTable("jobs");
        AbstractTable job_lib_table = oload::getTable("job_lib");

        AbstractTable mappers_table = oload::getTable("mappers");
        AbstractTable mapper_lib_table = oload::getTable("mapper_lib");

        AbstractTable fsm_table = oload::getTable("fsm");
        AbstractTable fsm_lib_table = oload::getTable("fsm_lib");

        AbstractTable pipelines_table = oload::getTable("pipelines");
        AbstractTable pipeline_lib_table = oload::getTable("pipeline_lib");

        # check steps
        if (val_step_map) {
            q = workflows_table.select({
                "columns": (cop_distinct("workflowid"), "name", "version", "deprecated"),
                "where": {
                    "ws.stepid": op_in(map $1.toInt(), keys val_step_map),
                    "deprecated": op_ne(1),
                },
                "join": join_inner(workflow_steps_table, "ws"),
            });
            updateWFValidationLists(q);
        }

        if (fl) {
            q = workflows_table.select(("columns": (cop_distinct("workflowid"), "name", "version", "deprecated"),
                                          "where": wop_or(("errorfunction_instanceid": op_in(fl)),
                                                          ("attach_func_instanceid": op_in(fl)),
                                                          ("detach_func_instanceid": op_in(fl)),
                                                          ("onetimeinit_func_instanceid": op_in(fl)),
                                                          ("errhandler_func_instanceid": op_in(fl)),
                                                          ("s.stepfunction_instanceid": op_in(fl)),
                                                          ("s.validationfunction_instanceid": op_in(fl)),
                                                          ("s.endfunction_instanceid": op_in(fl)),
                                                          ("s.arrayfunction_instanceid": op_in(fl)),
                                                         ) + (
                                              "deprecated": op_ne(1),
                                          ),
                                          "join": (join_inner(workflow_steps_table, "ws") +
                                                   join_inner("ws", steps_table, "s", ("stepid": "stepid"))),
                                         ), \sql);
            updateWFValidationLists(q);
        }
        if (fll) {
            AbstractTable function_instance_table = oload::getTable("function_instance");

            q = workflows_table.select(("columns": (cop_distinct("workflowid"), "name", "version"),
                                          "where": (
                                              "fi.function_instanceid": op_in(fll),
                                              "wl.type": OT_FUNCTION,
                                              "deprecated": op_ne(1),
                                          ),
                                          "join": (join_inner(workflow_lib_table, "wl")) +
                                                   join_inner("wl", function_instance_table, "fi", ("name": "name"))),
                                         \sql);
            updateWFValidationLists(q);

            q = step_lib_table.select({
                "columns": ("wf.workflowid", "wf.name", "wf.version"),
                "where": {
                    "fi.function_instanceid": op_in(fll),
                    "type": OT_FUNCTION,
                },
                "join": join_inner(function_instance_table, "fi", {"name": "name"}) +
                        join_inner(workflow_steps_table, "ws", {"stepid": "stepid"}) +
                        join_inner("ws", workflows_table, "wf", {"workflowid": "workflowid"},
                                    {"deprecated": op_ne(1)})
            }, \sql);
            updateWFValidationLists(q);

            # check for functions in workflows through steps through FSMs
            q = step_lib_table.select({
                "columns": ("wf.workflowid", "wf.name", "wf.version"),
                "where": {
                    "fi.function_instanceid": op_in(fll),
                    "type": OT_FSM,
                    "fl.type": OT_FUNCTION,
                },
                "join": join_inner(fsm_lib_table, "fl", {"name": "fsm"})
                    + join_inner("fl", function_instance_table, "fi", {"name": "name"})
                    + join_inner(workflow_steps_table, "ws", {"stepid": "stepid"})
                    + join_inner("ws", workflows_table, "wf", {"workflowid": "workflowid"}, {"deprecated": op_ne(1)}),
            }, \sql);
            updateWFValidationLists(q);

            # check for functions in workflows through steps through pipelines
            q = step_lib_table.select({
                "columns": ("wf.workflowid", "wf.name", "wf.version"),
                "where": {
                    "fi.function_instanceid": op_in(fll),
                    "type": OT_FSM,
                    "pl.type": OT_FUNCTION,
                },
                "join": join_inner(pipeline_lib_table, "pl", {"name": "pipeline"})
                    + join_inner("pl", function_instance_table, "fi", {"name": "name"})
                    + join_inner(workflow_steps_table, "ws", {"stepid": "stepid"})
                    + join_inner("ws", workflows_table, "wf", {"workflowid": "workflowid"}, {"deprecated": op_ne(1)}),
            }, \sql);
            updateWFValidationLists(q);

            # this query only gets the latest version of services, thereby not adding older services to the validation list
            # use alternate approach for mysql, since mysql does not support SQL window functions
            if (sqlif.omqp.getDriverName() == "mysql") {
                q = services_table.select(("columns": (cop_distinct("serviceid"), "service_type", "name", "version", "created", "language"),
                                          "where": (
                                              "fi.function_instanceid": op_in(fll),
                                              "sl.type": OT_FUNCTION,
                                              "serviceid": op_in(mysql_get_serviceid_list()),
                                           ),
                                          "join": (join_inner(service_lib_table, "sl")) +
                                                   join_inner("sl", function_instance_table, "fi", ("name": "name")),),
                                       \sql);
            } else {
                q = services_table.select(("columns": ("serviceid", "service_type", "name", "version", "created",
                    cop_as(cop_over(cop_max("created"), "name"), "max_created"), "language"),
                                          "where": (
                                              "fi.function_instanceid": op_in(fll),
                                              "sl.type": OT_FUNCTION,
                                           ),
                                          "join": (join_inner(service_lib_table, "sl")) +
                                                   join_inner("sl", function_instance_table, "fi", ("name": "name")),
                                          "superquery": ("columns": (cop_distinct("serviceid"), "service_type", "name", "version", "language"),
                                                         "where": ("max_created": op_ceq("created"))),
                                       ),
                                       \sql);
            }
            updateSValidationLists(q);

            q = jobs_table.select(("columns": (cop_distinct("jobid"), "name", "version", "language"),
                                     "where": ("fi.function_instanceid": op_in(fll), "jl.type": OT_FUNCTION),
                                     "join": (join_inner(job_lib_table, "jl")) +
                                     join_inner("jl", function_instance_table, "fi", ("name": "name"))),
                                   \sql);
            updateJValidationLists(q);

            q = mappers_table.select(("columns": (cop_distinct("mapperid"), "name", "version"),
                                     "where": ("fi.function_instanceid": op_in(fll), "l.type": OT_FUNCTION),
                                     "join": (join_inner(mapper_lib_table, "l")) +
                                     join_inner("l", function_instance_table, "fi", ("name": "name"))),
                                    \sql);
            updateMValidationLists(q);

            q = fsm_table.select({
                "columns": (cop_distinct("name")),
                "where": {"fi.function_instanceid": op_in(fll), "l.type": OT_FUNCTION},
                "join": (join_inner(fsm_lib_table, "l")) +
                join_inner("l", function_instance_table, "fi", {"name": "name"}),
            }, \sql);
            updateFsmValidationLists(q);

            q = pipelines_table.select({
                "columns": (cop_distinct("name")),
                "where": {"fi.function_instanceid": op_in(fll), "l.type": OT_FUNCTION},
                "join": (join_inner(pipeline_lib_table, "l")) +
                join_inner("l", function_instance_table, "fi", {"name": "name"}),
            }, \sql);
            updatePipelineValidationLists(q);
        }

        # check classes
        if (valclh) {
            AbstractTable classes_table = oload::getTable("classes");
            AbstractTable class_dependencies_table = oload::getTable("class_dependencies");

            # issue #1704: find all depedendent classes
            # recursive queries are DB-dependent, and we don't have support in SqlUtil yet
            # (https://github.com/qorelanguage/qore/issues/3544)
            while (True) {
                list<int> classid_list = map $1.toInt(), keys valclh;
                *list<string> names = map classmap{$1}.name, keys valclh, classmap{$1};
                hash<auto> select_hash = {
                    "columns": "name",
                    "where": {
                        "classid": op_not(op_in(classid_list)),
                        "cd.dependson_class": op_in(names),
                    },
                    "join": join_inner(class_dependencies_table, "cd"),
                };
                names = classes_table.select(select_hash, \sql).name;
                if (!names) {
                    break;
                }
                map valclh{class_name_map{$1}.classid} = True, names, class_name_map{$1};
            }

            # list of class IDs
            *softlist cl = map int($1), keys valclh;

            # issue #1704: find steps with updated step classes
            q = workflows_table.select({"columns": (cop_distinct("workflowid"), "name", "version"),
                                          "where": {
                                              "s.step_classid": op_in(cl),
                                              "deprecated": op_ne(1),
                                          },
                                          "join": (join_inner(workflow_steps_table, "ws") +
                                                   join_inner("ws", steps_table, "s", {"stepid": "stepid"})),
                                         }, \sql);
            updateWFValidationLists(q);

            # issue #1704: find steps with updated step class dependencies
            q = workflows_table.select({"columns": (cop_distinct("workflowid"), "name", "version"),
                                          "where": {
                                              "cls.classid": op_in(cl),
                                              "deprecated": op_ne(1),
                                          },
                                          "join": (join_inner(workflow_steps_table, "ws") +
                                                   join_inner("ws", steps_table, "s", {"stepid": "stepid"}) +
                                                   join_inner("s", class_dependencies_table, "cd", {"step_classid": "classid"}) +
                                                   join_inner("cd", classes_table, "cls", {"dependson_class": "name"})
                                          ),
                                         }, \sql);
            updateWFValidationLists(q);

            # issue #1704: find workflows with updated library class dependencies
            q = workflows_table.select({"columns": (cop_distinct("workflowid"), "name", "version"),
                                          "where": {
                                              "cls2.classid": op_in(cl),
                                              "wl.type": OT_CLASS,
                                              "deprecated": op_ne(1),
                                          },
                                          "join": (join_inner(workflow_lib_table, "wl") +
                                                   join_inner("wl", classes_table, "cls1", {"name": "name"}) +
                                                   join_inner("cls1", class_dependencies_table, "cd", {"classid": "classid"}) +
                                                   join_inner("cd", classes_table, "cls2", {"dependson_class": "name"})
                                          ),
                                         }, \sql);
            updateWFValidationLists(q);

            q = workflows_table.select(("columns": (cop_distinct("workflowid"), "name", "version"),
                                          "where": (
                                              "c.classid": op_in(cl),
                                              "wl.type": OT_CLASS,
                                              "deprecated": op_ne(1),
                                          ),
                                          "join": (join_inner(workflow_lib_table, "wl") +
                                                   join_inner("wl", classes_table, "c", ("name": "name")))),
                                         \sql);
            updateWFValidationLists(q);
            #printf("q: %N\nsql: %s\n", q, sql);

            q = step_lib_table.select({"columns": ("wf.workflowid", "wf.name", "wf.version"),
                                        "where": {
                                            "cls2.classid": op_in(cl),
                                            "type": OT_CLASS,
                                        },
                                        "join": join_inner(classes_table, "cls1", {"name": "name"}) +
                                                join_inner("cls1", class_dependencies_table, "cd", {"classid": "classid"}) +
                                                join_inner("cd", classes_table, "cls2", {"dependson_class": "name"}) +
                                                join_inner(workflow_steps_table, "ws", {"stepid": "stepid"}) +
                                                join_inner("ws", workflows_table, "wf", {"workflowid": "workflowid"},
                                                            {"deprecated": op_ne(1)})
                                        }, \sql);
            updateWFValidationLists(q);

            q = step_lib_table.select({"columns": ("wf.workflowid", "wf.name", "wf.version"),
                                        "where": {
                                            "c.classid": op_in(cl),
                                            "type": OT_CLASS,
                                        },
                                        "join": join_inner(classes_table, "c", {"name": "name"}) +
                                                join_inner(workflow_steps_table, "ws", {"stepid": "stepid"}) +
                                                join_inner("ws", workflows_table, "wf", {"workflowid": "workflowid"},
                                                            {"deprecated": op_ne(1)})
                                        }, \sql);
            updateWFValidationLists(q);

            # check for classes in workflows through steps through FSMs
            q = step_lib_table.select({
                "columns": ("wf.workflowid", "wf.name", "wf.version"),
                "where": {
                    "c.classid": op_in(cl),
                    "type": OT_FSM,
                    "fl.type": OT_CLASS,
                },
                "join": join_inner(fsm_lib_table, "fl", {"name": "fsm"})
                    + join_inner("fl", classes_table, "c", {"name": "name"})
                    + join_inner(workflow_steps_table, "ws", {"stepid": "stepid"})
                    + join_inner("ws", workflows_table, "wf", {"workflowid": "workflowid"}, {"deprecated": op_ne(1)}),
            }, \sql);
            updateWFValidationLists(q);

            # check for classes in workflows through steps through pipelines
            q = step_lib_table.select({
                "columns": ("wf.workflowid", "wf.name", "wf.version"),
                "where": {
                    "c.classid": op_in(cl),
                    "type": OT_FSM,
                    "pl.type": OT_CLASS,
                },
                "join": join_inner(pipeline_lib_table, "pl", {"name": "pipeline"})
                    + join_inner("pl", classes_table, "c", {"name": "name"})
                    + join_inner(workflow_steps_table, "ws", {"stepid": "stepid"})
                    + join_inner("ws", workflows_table, "wf", {"workflowid": "workflowid"}, {"deprecated": op_ne(1)}),
            }, \sql);
            updateWFValidationLists(q);

            # this query only gets the latest version of services, thereby not adding older services to the validation list
            if (sqlif.omqp.getDriverName() == "mysql") {
                q = services_table.select(("columns": (cop_distinct("serviceid"), "service_type", "name", "version",
                    "created", "language",),
                                          "where": (
                                              "c.classid": op_in(cl),
                                              "sl.type": OT_CLASS,
                                              "serviceid": op_in(mysql_get_serviceid_list()),
                                           ),
                                          "join": (join_inner(service_lib_table, "sl")) +
                                                   join_inner("sl", classes_table, "c", ("name": "name")),                                       ),
                                       \sql);
            } else {
                q = services_table.select(("columns": ("serviceid", "service_type", "name", "version", "created",
                    cop_as(cop_over(cop_max("created"), "name"), "max_created"), "language"),
                                        "where": ("c.classid": op_in(cl), "sl.type": OT_CLASS),
                                        "join": (join_inner(service_lib_table, "sl")) +
                                                   join_inner("sl", classes_table, "c", ("name": "name")),
                                        "superquery": ("columns": (cop_distinct("serviceid"), "service_type",
                                            "name", "version", "language"),
                                                         "where": ("max_created": op_ceq("created"))),
                                       ),
                                       \sql);
            }
            updateSValidationLists(q);
            # check for classes in services through FSMs
            q = service_lib_table.select({
                "columns": ("svc.serviceid", "svc.service_type", "svc.name", "svc.version", "svc.language"),
                "where": {
                    "c.classid": op_in(cl),
                    "type": OT_FSM,
                    "fl.type": OT_CLASS,
                },
                "join": join_inner(fsm_lib_table, "fl", {"name": "fsm"})
                    + join_inner("fl", classes_table, "c", {"name": "name"})
                    + join_inner(services_table, "svc", {"serviceid": "serviceid"}),
            }, \sql);
            updateSValidationLists(q);

            # check for classes in services through pipelines
            q = service_lib_table.select({
                "columns": ("svc.serviceid", "svc.service_type", "svc.name", "svc.version", "svc.language"),
                "where": {
                    "c.classid": op_in(cl),
                    "type": OT_FSM,
                    "pl.type": OT_CLASS,
                },
                "join": join_inner(pipeline_lib_table, "pl", {"name": "pipeline"})
                    + join_inner("pl", classes_table, "c", {"name": "name"})
                    + join_inner(services_table, "svc", {"serviceid": "serviceid"}),
            }, \sql);
            updateSValidationLists(q);

            q = jobs_table.select(("columns": (cop_distinct("jobid"), "name", "version", "language"),
                                     "where": ("c.classid": op_in(cl), "jl.type": OT_CLASS),
                                     "join": (join_inner(job_lib_table, "jl") +
                                              join_inner("jl", classes_table, "c", ("name": "name")))),
                                    \sql);
            updateJValidationLists(q);

            # check for classes in jobs through FSMs
            q = job_lib_table.select({
                "columns": ("job.jobid", "job.name", "job.version", "job.language"),
                "where": {
                    "c.classid": op_in(cl),
                    "type": OT_FSM,
                    "fl.type": OT_CLASS,
                },
                "join": join_inner(fsm_lib_table, "fl", {"name": "fsm"})
                    + join_inner("fl", classes_table, "c", {"name": "name"})
                    + join_inner(jobs_table, "job", {"jobid": "jobid"}),
            }, \sql);
            updateJValidationLists(q);

            # check for classes in jobs through pipelines
            q = job_lib_table.select({
                "columns": ("job.jobid", "job.name", "job.version", "job.language"),
                "where": {
                    "c.classid": op_in(cl),
                    "type": OT_FSM,
                    "pl.type": OT_CLASS,
                },
                "join": join_inner(pipeline_lib_table, "pl", {"name": "pipeline"})
                    + join_inner("pl", classes_table, "c", {"name": "name"})
                    + join_inner(jobs_table, "job", {"jobid": "jobid"}),
            }, \sql);
            updateJValidationLists(q);

            # issue #1704: find jobs with updated library class dependencies
            q = mappers_table.select(("columns": (cop_distinct("mapperid"), "name", "version"),
                                     "where": {
                                         "cls2.classid": op_in(cl),
                                         "l.type": OT_CLASS,
                                     },
                                     "join": (
                                        join_inner(mapper_lib_table, "l")) +
                                        join_inner("l", classes_table, "cls1", {"name": "name"}) +
                                        join_inner("cls1", class_dependencies_table, "cd", {"classid": "classid"}) +
                                        join_inner("cd", classes_table, "cls2", {"dependson_class": "name"})
                                    ),
                                    \sql);
            updateMValidationLists(q);

            q = jobs_table.select({"columns": (cop_distinct("jobid"), "name", "version", "language"),
                                   "where": {"c.classid": op_in(cl), "jl.type": OT_CLASS},
                                   "join": (join_inner(job_lib_table, "jl") +
                                            join_inner("jl", classes_table, "c", ("name": "name")))},
                                    \sql);
            updateJValidationLists(q);

            q = fsm_table.select({
                "columns": (cop_distinct("name")),
                "where": {"c.classid": op_in(cl), "l.type": OT_CLASS},
                "join": (join_inner(fsm_lib_table, "l"))
                    + join_inner("l", classes_table, "c", {"name": "name"}),
            }, \sql);
            updateFsmValidationLists(q);

            q = pipelines_table.select({
                "columns": (cop_distinct("name")),
                "where": {"c.classid": op_in(cl), "l.type": OT_CLASS},
                "join": (join_inner(pipeline_lib_table, "l"))
                    + join_inner("l", classes_table, "c", {"name": "name"}),
            }, \sql);
            updatePipelineValidationLists(q);
        }

        # list of constant IDs
        *softlist conl = map int($1), keys valcoh;
        if (conl) {
            AbstractTable constants_table = oload::getTable("constants");

            q = workflows_table.select(("columns": (cop_distinct("workflowid"), "name", "version"),
                                          "where": (
                                                       "c.constantid": op_in(conl),
                                                       "wl.type": "CONSTANT",
                                                       "deprecated": op_ne(1),
                                                   ),
                                          "join": (join_inner(workflow_lib_table, "wl") +
                                                   join_inner("wl", constants_table, "c", ("name": "name")))),
                                         \sql);
            updateWFValidationLists(q);

            q = step_lib_table.select({"columns": ("wf.workflowid", "wf.name", "wf.version"),
                                        "where": {
                                            "c.constantid": op_in(conl),
                                            "type": "CONSTANT",
                                        },
                                        "join": join_inner(constants_table, "c", {"name": "name"}) +
                                                join_inner(workflow_steps_table, "ws", {"stepid": "stepid"}) +
                                                join_inner("ws", workflows_table, "wf", {"workflowid": "workflowid"},
                                                            {"deprecated": op_ne(1)})
                                        }, \sql);
            updateWFValidationLists(q);

            # this query only gets the latest version of services, thereby not adding older services to the validation list
            if (sqlif.omqp.getDriverName() == "mysql") {
                q = services_table.select(("columns": (cop_distinct("serviceid"), "service_type", "name", "version",
                    "created", "language",),
                                          "where": (
                                              "c.constantid": op_in(conl),
                                              "sl.type": "CONSTANT",
                                              "serviceid": op_in(mysql_get_serviceid_list()),
                                           ),
                                          "join": (join_inner(service_lib_table, "sl")) +
                                                   join_inner("sl", constants_table, "c", ("name": "name")),                                       ),
                                       \sql);
            } else {
                q = services_table.select(("columns": ("serviceid", "service_type", "name", "version", "created",
                    cop_as(cop_over(cop_max("created"), "name"), "max_created"), "language"),
                                          "where": ("c.constantid": op_in(conl), "sl.type": "CONSTANT"),
                                          "join": (join_inner(service_lib_table, "sl")) +
                                                   join_inner("sl", constants_table, "c", ("name": "name")),
                                          "superquery": ("columns": (cop_distinct("serviceid"), "service_type",
                                            "name", "version", "language"),
                                                         "where": ("max_created": op_ceq("created"))),
                                       ),
                                       \sql);
            }
            updateSValidationLists(q);

            q = jobs_table.select(("columns": (cop_distinct("jobid"), "name", "version", "language"),
                                     "where": ("c.constantid": op_in(conl), "jl.type": "CONSTANT"),
                                     "join": (join_inner(job_lib_table, "jl") +
                                              join_inner("jl", constants_table, "c", ("name": "name")))),
                                    \sql);
            updateJValidationLists(q);

            q = mappers_table.select(("columns": (cop_distinct("mapperid"), "name", "version"),
                                     "where": ("c.constantid": op_in(conl), "l.type": "CONSTANT"),
                                     "join": (join_inner(mapper_lib_table, "l")) +
                                     join_inner("l", constants_table, "c", ("name": "name"))),
                                    \sql);
            updateMValidationLists(q);

            q = pipelines_table.select({
                "columns": (cop_distinct("name")),
                "where": {"c.constantid": op_in(conl), "l.type": OT_CONSTANT},
                "join": (join_inner(pipeline_lib_table, "l"))
                    + join_inner("l", constants_table, "c", {"name": "name"}),
            }, \sql);
            updatePipelineValidationLists(q);

            q = fsm_table.select({
                "columns": (cop_distinct("name")),
                "where": {"c.constantid": op_in(conl), "l.type": OT_CONSTANT},
                "join": (join_inner(fsm_lib_table, "l"))
                    + join_inner("l", constants_table, "c", {"name": "name"}),
            }, \sql);
            updateFsmValidationLists(q);
        }

        # list of mapper IDs
        *softlist ml = map $1.toInt(), keys valmh;

        if (ml) {
            q = workflows_table.select(("columns": (cop_distinct("workflowid"), "name", "version"),
                                          "where": (
                                              "m.mapperid": op_in(ml),
                                              "deprecated": op_ne(1),
                                          ),
                                          "join": (join_inner(oload::getTable("workflow_mappers"), "m"))),
                                         \sql);
            updateWFValidationLists(q);

            AbstractTable step_mappers_table = oload::getTable("step_mappers");
            q = step_mappers_table.select({"columns": ("wf.workflowid", "wf.name", "wf.version"),
                                           "where": {
                                               "mapperid": op_in(ml),
                                           },
                                           "join": join_inner(workflow_steps_table, "ws", {"stepid": "stepid"}) +
                                                   join_inner("ws", workflows_table, "wf", {"workflowid": "workflowid"},
                                                              {"deprecated": op_ne(1)})
                                           }, \sql);
            updateWFValidationLists(q);

            AbstractTable service_mappers_table = oload::getTable("service_mappers");

            # this query only gets the latest version of services, thereby not adding older services to the validation list
            if (sqlif.omqp.getDriverName() == "mysql") {
                q = services_table.select(("columns": (cop_distinct("serviceid"), "service_type", "name", "version",
                    "created", "language",),
                                          "where": (
                                              "m.mapperid": op_in(ml),
                                              "serviceid": op_in(mysql_get_serviceid_list()),
                                           ),
                                          "join": (join_inner(service_mappers_table, "m")),
                                          ),
                                       \sql);
            } else {
                q = services_table.select(("columns": ("serviceid", "service_type", "name", "version", "created",
                    cop_as(cop_over(cop_max("created"), "name"), "max_created"), "language"),
                                          "where": ("m.mapperid": op_in(ml)),
                                          "join": (join_inner(service_mappers_table, "m")),
                                          "superquery": ("columns": (cop_distinct("serviceid"), "service_type",
                                            "name", "version", "language"),
                                                         "where": ("max_created": op_ceq("created"))),
                                       ),
                                       \sql);
            }
            updateSValidationLists(q);

            q = jobs_table.select(("columns": (cop_distinct("jobid"), "name", "version", "language"),
                                          "where": ("m.mapperid": op_in(ml)),
                                          "join": (join_inner(oload::getTable("job_mappers"), "m"))),
                                    \sql);
            updateJValidationLists(q);
        }

        # add FSMs where pipelines have been updated
        if (omqmap_reload.pipelines) {
            q = fsm_lib_table.select({
                "columns": cop_distinct("fsm"),
                "where": {"name": op_in(keys omqmap_reload.pipelines), "type": OT_PIPELINE},
            }, \sql);
            q.name = remove q.fsm;
            updateFsmValidationLists(q);
        }

        # check FSMs
        if (val_fsm_map) {
            # find all FSM interdepedencies
            # recursive queries are DB-dependent, and we don't have support in SqlUtil yet
            # (https://github.com/qorelanguage/qore/issues/3544)
            while (True) {
                *list<string> names = keys val_fsm_map;
                hash<auto> select_hash = {
                    "columns": "fsm",
                    "where": {
                        "name": op_in(names),
                        "type": OT_FSM,
                        "fsm": op_not(op_in(names)),
                    },
                };
                names = fsm_lib_table.select(select_hash, \sql).fsm;
                if (!names) {
                    break;
                }
                map val_fsm_map{$1} = True, names;
            }

            # check workflows through step libs
            q = step_lib_table.select({
                "columns": ("wf.workflowid", "wf.name", "wf.version"),
                "where": {
                    "name": op_in(keys val_fsm_map),
                    "type": OT_FSM,
                },
                "join": join_inner(workflow_steps_table, "ws", {"stepid": "stepid"})
                    + join_inner("ws", workflows_table, "wf", {"workflowid": "workflowid"}, {"deprecated": op_ne(1)}),
            }, \sql);
            updateWFValidationLists(q);

            # check services
            # this query only gets the latest version of services, thereby not adding older services to the validation list
            # use alternate approach for mysql, since mysql does not support SQL window functions
            if (sqlif.omqp.getDriverName() == "mysql") {
                q = services_table.select({
                    "columns": (cop_distinct("serviceid"), "service_type", "name", "version", "created", "language",),
                    "where": {
                        "sl.name": op_in(keys val_fsm_map),
                        "sl.type": OT_FSM,
                        "serviceid": op_in(mysql_get_serviceid_list()),
                    },
                    "join": join_inner(service_lib_table, "sl"),
                }, \sql);
            } else {
                q = services_table.select({
                    "columns": ("serviceid", "service_type", "name", "version", "created",
                        cop_as(cop_over(cop_max("created"), "name"), "max_created"), "language"),
                    "where": {
                        "sl.name": op_in(keys val_fsm_map),
                        "sl.type": OT_FSM,
                    },
                    "join": join_inner(service_lib_table, "sl"),
                    "superquery": ("columns": (cop_distinct("serviceid"), "service_type", "name", "version",
                        "language"),
                                    "where": ("max_created": op_ceq("created"))),
                }, \sql);
            }
            updateSValidationLists(q);

            # check jobs
            q = jobs_table.select({
                "columns": (cop_distinct("jobid"), "name", "version", "language"),
                "where": {
                    "jl.name": op_in(keys val_fsm_map),
                    "jl.type": OT_FSM,
                },
                "join": join_inner(job_lib_table, "jl"),
            }, \sql);
            updateJValidationLists(q);
        }

        # check pipelines
        if (val_pipeline_map) {
            # check workflows through step libs
            q = step_lib_table.select({
                "columns": ("wf.workflowid", "wf.name", "wf.version"),
                "where": {
                    "name": op_in(keys val_pipeline_map),
                    "type": OT_PIPELINE,
                },
                "join": join_inner(workflow_steps_table, "ws", {"stepid": "stepid"})
                    + join_inner("ws", workflows_table, "wf", {"workflowid": "workflowid"}, {"deprecated": op_ne(1)}),
            }, \sql);
            updateWFValidationLists(q);

            # check services
            # this query only gets the latest version of services, thereby not adding older services to the validation list
            # use alternate approach for mysql, since mysql does not support SQL window functions
            if (sqlif.omqp.getDriverName() == "mysql") {
                q = services_table.select({
                    "columns": (cop_distinct("serviceid"), "service_type", "name", "version", "created", "language",),
                    "where": {
                        "sl.name": op_in(keys val_pipeline_map),
                        "sl.type": OT_PIPELINE,
                        "serviceid": op_in(mysql_get_serviceid_list()),
                    },
                    "join": join_inner(service_lib_table, "sl"),
                }, \sql);
            } else {
                q = services_table.select({
                    "columns": ("serviceid", "service_type", "name", "version", "created",
                        cop_as(cop_over(cop_max("created"), "name"), "max_created"), "language"),
                    "where": {
                        "sl.name": op_in(keys val_pipeline_map),
                        "sl.type": OT_PIPELINE,
                    },
                    "join": join_inner(service_lib_table, "sl"),
                    "superquery": ("columns": (cop_distinct("serviceid"), "service_type", "name", "version",
                        "language"),
                                    "where": ("max_created": op_ceq("created"))),
                }, \sql);
            }
            updateSValidationLists(q);

            # check jobs
            q = jobs_table.select({
                "columns": (cop_distinct("jobid"), "name", "version", "language"),
                "where": {
                    "jl.name": op_in(keys val_pipeline_map),
                    "jl.type": OT_PIPELINE,
                },
                "join": join_inner(job_lib_table, "jl"),
            }, \sql);
            updateJValidationLists(q);
        }
    } # updateValidationLists

    bool validate() {
        if (!valwh && !valsh && !valjh && !valmh) {
            return True;
        }

        bool result = True;

        if (!o.quiet) {
            printf("Validating:");
            if (o.verbose) {
                print("\n");
            } else {
                print(" ");
            }
        }

        # workflows
        foreach hash<auto> h in (valwh.iterator()) {
            try {
                # temporarily clear "full" flag if checking objects added due to library dependencies
                FullOptionHelper foh(h);
                # create temporary test instance of workflow to see if it can be parsed
                WorkflowValidator val(h);
                if (!o.quiet) {
                    if (o.verbose) {
                        printf("workflow %s/%s (%d) validation successful\n", h.name, h.version, h.id);
                    } else {
                        mystat("V");
                    }
                }
            } catch (*hash<auto> ex) {
                result = False;
                if (o.quiet) {
                    stderr.printf("VALIDATION FAILED\n");
                } else {
                    stderr.printf("workflow %s/%s (%d) failed validation\n", h.name, h.version, h.id);
                }
                while (ex) {
                    if (ex.err =~ /^MISSING-/)
                        rethrow;
                    stderr.printf("%s: %s: %s\n", get_ex_pos(ex), ex.err, ex.desc);
                    oload::showEx(ex);
                    ex = ex.next;
                }
                # issue #2608: fix exit code in case of validation errors
                oload::has_any_error = True;
            }
        }

        # services
        foreach hash<auto> h in (valsh.iterator()) {
            try {
                # temporarily clear "full" flag if checking objects added due to library dependencies
                FullOptionHelper foh(h);
                # create temporary test instance of service to see if it can be parsed
                bool has_python = h.has_python ?? True;
                *string possible = !exists h.has_python ? "possible " : "";
                if (has_python) {
                    # issue #3571: if a service has a Python dependency, then validate it in another process
                    # in order to ensure that all modules can be loaded; some Python modules can only be loaded
                    # once per process

                    if (!oload.o.quiet && oload.o.verbose) {
                        printf("%s service %s v%s: validating externally due to %sdynamic Python dependency\n", h.type,
                            h.name, h.version, possible);
                    }

                    ExternalProcessHelper proc("SERVICE-VALIDATION-ERROR", "--validate-service=-");
                    # write serialized data to process's stdin
                    proc.write(h);
                    # wait for process to complete
                    proc.wait();
                } else {
                    ServiceValidator val(h);
                }
                if (!o.quiet) {
                    if (o.verbose) {
                        printf("%s service %s v%s (%d) validation successful\n", h.type, h.name, h.version, h.id);
                    } else {
                        mystat("V");
                    }
                }
            } catch (hash<ExceptionInfo> ex) {
                if (ex.err == "LOAD-MODULE-ERROR" && h.type == "system") {
                    if (o.quiet)
                        stderr.printf("warning: %s service %s/%s (%d) cannot be loaded (missing/incompatible libraries)\n", h.type, h.name, h.version, h.id);
                    else
                        stderr.printf("MISSING-FEATURE (check libraries and system for dependencies)\n");
                } else {
                    if (ex.err =~ /^MISSING-/)
                        rethrow;
                    stderr.printf("FAILED\n%s service %s v%s failed validation at %s: %s: %s\n",
                                  h.type, h.name, h.version, get_ex_pos(ex), ex.err, ex.desc);
                    oload::showEx(ex);
                }
                result = False;
                # issue #2608: fix exit code in case of validation errors
                oload::has_any_error = True;
            }
        }

        # jobs
        foreach hash<auto> h in (valjh.iterator()) {
            try {
                # temporarily clear "full" flag if checking objects added due to library dependencies
                FullOptionHelper foh(h);
                # create temporary test instance of job to see if it can be parsed
                bool has_python = h.has_python ?? True;
                *string possible = !exists h.has_python ? "possible " : "";
                if (has_python) {
                    # issue #3571: if a job has a Python dependency, then validate it in another process
                    # in order to ensure that all modules can be loaded; some Python modules can only be loaded
                    # once per process

                    if (!oload.o.quiet && oload.o.verbose) {
                        printf("job %s v%s: validating externally due to %sdynamic Python dependency\n",
                            h.name, h.version, possible);
                    }

                    ExternalProcessHelper proc("JOB-VALIDATION-ERROR", "--validate-job=-");
                    # write serialized data to process's stdin
                    proc.write(h);
                    # wait for process to complete
                    proc.wait();
                } else {
                    JobValidator val(h);
                }
                if (!o.quiet) {
                    if (o.verbose) {
                        printf("job %s/%s (%d) validation successful\n", h.name, h.version, h.id);
                    } else {
                        mystat("V");
                    }
                }
            } catch (*hash<ExceptionInfo> ex) {
                result = False;
                if (o.quiet)
                    stderr.printf("VALIDATION FAILED\n");
                else
                    stderr.printf("job %s/%s (%d) failed validation\n", h.name, h.version, h.id);
                while (ex) {
                    if (ex.err =~ /^MISSING-/)
                        rethrow;
                    stderr.printf("%s: %s: %s\n", get_ex_pos(ex), ex.err, ex.desc);
                    if (oload.o.showfullerror) {
                        oload::showEx(ex);
                    }
                    ex = ex.next;
                }
                # issue #2608: fix exit code in case of validation errors
                oload::has_any_error = True;
            }
        }

        # mappers
        foreach hash<auto> h in (valmh.iterator()) {
            try {
                # temporarily clear "full" flag if checking objects added due to library dependencies
                FullOptionHelper foh(h);
                # create temporary test instance of job to see if it can be parsed
                MapperValidator val(h);
                if (!o.quiet) {
                    if (o.verbose) {
                        printf("mapper %s/%s (%d) validation successful\n", h.name, h.version, h.id);
                    } else {
                        mystat("V");
                    }
                }
            } catch (*hash<ExceptionInfo> ex) {
                result = False;
                if (o.quiet)
                    stderr.printf("VALIDATION FAILED\n");
                else
                    stderr.printf("mapper %s/%s (%d) failed validation\n", h.name, h.version, h.id);
                while (ex) {
                    if (ex.err =~ /^MISSING-/)
                        rethrow;
                    stderr.printf("%s: %s: %s\n", get_ex_pos(ex), ex.err, ex.desc);
                    if (oload.o.showfullerror) {
                        oload::showEx(ex);
                    }
                    ex = ex.next;
                }
                # issue #2608: fix exit code in case of validation errors
                oload::has_any_error = True;
            }
        }

        if (!o.quiet && !o.verbose) {
            print(" OK\n");
        }

        #printf("oload::validate() returning %y\n", result);
        return result;
    }

    list<hash<auto>> analyzeGroups(list gl, string desc, string column, auto id, string mapv) {
        #printf("desc: %y col: %y id: %y mapv: %y (%y) gl: %y\n", desc, column, id, mapv, self{mapv}, gl);
        list<hash<auto>> d = ();

        # find new groups that the object wasn't already in
        foreach string gname in (gl) {
            *hash g = gmap{gname};
            if (!g) {
                stderr.printf("ERROR: %s references unknown group %y (known groups: %y)\n", desc, gname, gmap.keys());
                exit(1);
            }

            if (!inlist(gname, self{mapv}{id}))
                d += {"?": "ADD", "groupid": g.id, "desc": desc, "column": column, "id": id};
        }

        # find old groups the service is no longer in
        foreach string gname in (self{mapv}{id}) {
            softint gid = gmap{gname}.id;

            if (!inlist(gname, gl))
                d += {"?": "DEL", "groupid": gid, "desc": desc, "column": column, "id": id};
        }

        #printf("DBG: oload::analyzeGroups() d=%y\n");
        return d;
    }

    updateGroups(string type, string fn, list d, bool quiet = False) {
        if (!d)
            return;

        reload_groups = True;

        if (o.verbose && !o.quiet && !quiet) {
            printf("updating %s group lists from %s:\n", type, fn);
        }

        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        foreach hash h in (d) {
            if (h."?" == "ADD") {
                if (o.verbose)
                    printf("%s adding to group %s (%d)\n", h.desc, grmap.(h.groupid), h.groupid);
                else
                    mystat("+");

                switch (type) {
                    case "service": {
                        sqlif.addServicesToGroup(h.groupid, h.id);
                        break;
                    }
                    case "workflow": {
                        sqlif.addWorkflowsToGroup(h.groupid, h.id);
                        break;
                    }
                    case "job": {
                        sqlif.addJobsToGroup(h.groupid, h.id);
                        break;
                    }
                    case "mapper": {
                        sqlif.addMappersToGroup(h.groupid, h.id);
                        break;
                    }
                    case "value map": {
                        sqlif.addVMapsToGroup(h.groupid, h.id);
                        break;
                    }
                    case "fsm": {
                        sqlif.addFsmsToGroup(h.groupid, h.id);
                        break;
                    }
                    case "pipeline": {
                        sqlif.addPipelinesToGroup(h.groupid, h.id);
                        break;
                    }
                    default: throw "UNKNOWN-MEMBER", sprintf("unknown type: %y", type);
                }
            } else {
                if (o.verbose)
                    printf("%s removing from group %s (%d)\n", h.desc, grmap.(h.groupid), h.groupid);
                else
                    mystat("-");

                switch (type) {
                    case "service": {
                        sqlif.deleteServicesFromGroup(h.groupid, h.id);
                        break;
                    }
                    case "workflow": {
                        sqlif.deleteWorkflowsFromGroup(h.groupid, h.id);
                        break;
                    }
                    case "job": {
                        sqlif.deleteJobsFromGroup(h.groupid, h.id);
                        break;
                    }
                    case "mapper": {
                        sqlif.deleteMappersFromGroup(h.groupid, h.id);
                        break;
                    }
                    case "value map": {
                        sqlif.deleteVMapsFromGroup(h.groupid, h.id);
                        break;
                    }
                    case "fsm": {
                        sqlif.deleteFsmsFromGroup(h.groupid, h.id);
                        break;
                    }
                    case "pipeline": {
                        sqlif.deletePipelinesFromGroup(h.groupid, h.id);
                        break;
                    }
                    default: throw "UNKNOWN-MEMBER", sprintf("unknown type: %y", type);
                }
            }
        }

        if (!o.verbose && !o.quiet && !quiet)
            print(" OK\n");
    }

    static parseOptionValue(reference<auto> val) {
        if (val.typeCode() == NT_HASH && val.type.typeCode() == NT_STRING
            && ((val.size() == 2 && val.hasKey("value"))
                || val.size() == 1)) {
            if (val.type[0] == "*") {
                if (!exists val.value) {
                    val = NOTHING;
                    return;
                }
            }
            switch (val.type) {
                case "hash":
                case "list":
                case "date":
                    val = parse_yaml(val.value);
                    break;
                default:
                    val = val.value;
            }
        }
    }
}

class FullOptionHelper {
    private {
        *bool b;
    }
    public {}

    constructor(hash h) {
        b = oload.o.full;
        if (h."added-recursive")
            oload.o.full = False;
    }
    destructor() {
        oload.o.full = b;
    }
}

class OMQ::InterfaceContainerBase {
    public {
        # for warnings
        *hash warn;

        # map class name -> True
        hash<string, bool> loaded_classes;
        # map constant name -> True
        hash<string, bool> loaded_constants;

        # map class IDs -> True
        hash<string, bool> class_id_map;
    }

    private {
        # map of library objects; type -> name -> id
        hash<string, hash<string, int>> lib_map;
    }

    destructor() {
        if (warn) {
            oload::has_any_error = True;
        }
    }

    private loadClassToQorusProgram(softint id, QorusProgram pgm, *reference<hash<string, int>> class_deps) {
        if (class_id_map{id}) {
            return;
        }

        hash<auto> class_info = sqlif.omqp.selectRow("select * from classes where classid = %v", id);

        if (!class_info) {
            oload::error("unknown class: %s:%s", class_info.name, class_info.version);
        }

        if (loaded_classes{class_info.name}) {
            return;
        }

        hash<auto> tags = sqlif.getTags("class", id);
        tags += remove tags.sys;
        if ((!class_info.language || class_info.language == "qore")) {
            addWarnings(pgm.parsePending(class_info.body, sprintf("class %s:%s", class_info.name, class_info.version),
                                         oload.warningMask, tags.source, tags.offset));
        } else if (class_info.language == "java") {
            pgm.cacheJavaClass(class_info.name, class_info.language_info, NOTHING, tags.classpath);
        } else if (class_info.language == "python") {
            pgm.cachePythonClass(class_info.name, class_info.body, class_info.language_info, NOTHING, tags.module_path);
        } else {
            throw "UNSUPPORTED-LANGUAGE", sprintf("language %y is not supported; expecting qore, java, or python",
                class_info.language);
        }

        loaded_classes{class_info.name} = True;
        class_id_map{id} = True;

        loadClassDependencies(class_info.name, class_info.version, id, pgm, \class_deps);
    }

    private loadClassDependencies(string name, string version, softint id, QorusProgram pgm,
                                  *reference<hash<string, int>> class_deps) {
        # printf("DEBUG: loadClassDependencies for class with id: %y\n", id);
        class_deps{name} = id;

        *list deps = sqlif.omqp.select("select dependson_class from class_dependencies where classid = %v", id).
                                       dependson_class;
        foreach string required_name in (deps) {
            *string required_classversion = oload.classrmap{required_name}.lastversion;
            if (!exists required_classversion) {
                throw "MISSING-CLASS", sprintf("class %s v%s (%d) requires unknown class %y",
                    name, version, id, required_name);
            }
            softint required_classid = oload.classrmap{required_name}{required_classversion};
            loadClassToQorusProgram(required_classid, pgm, \class_deps);
        }
    }

    loadValidatorLibrary(hash<auto> q, QorusProgram pgm, *reference<hash<string, bool>> fh,
                         *reference<hash<string, int>> class_deps) {
        #printf("InterfaceContainerBase::loadValidatorLibrary() q: %N\n", q);

        # get constants
        context (q) {
            auto o;
            *hash<auto> oq;
            *hash<auto> th;
            # library object tags
            *hash<auto> tags;

            if (%type == OMQ::OT_CONSTANT) {
                o = oload.constrmap.%name;
                if (!exists o)
                    throw "UNKNOWN-LIBRARY-CONSTANT", sprintf("no constant '%s' can be loaded", %name);

                string ver = o.lastversion;
                if (oload.o.full) {
                    if (!exists oload.omap.constant.%name)
                        throw "MISSING-CONSTANT", sprintf("full release does not contain constant %y", %name);
                    if (!exists oload.omap.constant.%name{ver})
                        throw "MISSING-CONSTANT", sprintf("full release contains constant %y v%s but later version v%s is already in the DB and is not contained in this release", %name, oload.omap.constant.%name.firstKey(), ver);
                }

                softint constantid = o{ver};
                lib_map{OT_CONSTANT}{%name} = constantid;
                oq = sqlif.omqp.selectRow("select * from constants where constantid = %v", constantid);
                th = sqlif.getTags("constant", constantid).sys;

                if (loaded_constants{%name}) {
                    continue;
                }
                loaded_constants{%name} = True;
            } else if (%type == OMQ::OT_CLASS) {
                o = oload.classrmap.%name;
                if (!exists o)
                    throw "UNKNOWN-LIBRARY-CLASS", sprintf("no class '%s' can be loaded", %name);

                string ver = o.lastversion;
                if (oload.o.full) {
                    if (!exists oload.omap."class".%name)
                        throw "MISSING-CLASS", sprintf("full release does not contain class %y", %name);
                    if (!exists oload.omap."class".%name{ver})
                        throw "MISSING-CLASS", sprintf("full release contains class %y v%s but later version v%s is already in the DB and is not contained in this release", %name, oload.omap."class".%name.firstKey(), ver);
                }

                softint classid = o{ver};
                lib_map{OT_CLASS}{%name} = classid;
                oq = sqlif.omqp.selectRow("select * from classes where classid = %v", classid);
                tags = sqlif.getTags("class", classid);
                th = tags.sys;
                loadClassDependencies(%name, ver, classid, pgm, \class_deps);

                if (loaded_classes{%name}) {
                    continue;
                }

                loaded_classes{%name} = True;
                class_id_map{classid} = True;
            } else if (%type == OMQ::OT_FUNCTION) {
                o = oload.frmap.%name;
                if (!exists o)
                    throw "UNKNOWN-LIBRARY-FUNCTION", sprintf("no function '%s' can be loaded", %name);

                string ver = o.lastversion;
                if (oload.o.full) {
                    if (!exists oload.omap.function.%name)
                        throw "MISSING-FUNCTION", sprintf("full release does not contain function %y", %name);
                    if (!exists oload.omap.function.%name{ver})
                        throw "MISSING-FUNCTION", sprintf("full release contains function %y v%s but later version v%s is already in the DB and is not contained in this release", %name, oload.omap.function.%name.firstKey(), ver);
                }

                softint fiid = o{ver}.id;
                lib_map{OT_FUNCTION}{%name} = fiid;
                if (fh{fiid}) {
                    # issue #2282: skip functions that have already been loaded
                    continue;
                }
                fh{fiid} = True;
                #printf("loading %y (%y)\n", fiid, keys fh);
                oq = sqlif.omqp.selectRow("select * from function_instance where function_instanceid = %v", fiid);
                th = sqlif.getTags("function_instance", fiid).sys;
                #printf("%y: %N\n", %name, o);
                #printf("%y: %N\n", %name, th);
            } else if (%type == OMQ::OT_PIPELINE) {
                lib_map{OT_PIPELINE}{%name} = 0; # id is not needed
                DbInserter::checkLibraryObject("pipeline", "pipeline", %name);
                # nothing to parse - pipelines have no code
                continue;
            } else if (%type == OMQ::OT_FSM) {
                lib_map{OT_FSM}{%name} = 0; # id is not needed
                DbInserter::checkLibraryObject("fsm", "fsm", %name);
                # nothing to parse - fsm have no code
                continue;
            }

            if (!oq)
                oload::warning("unknown %s: %s", %type, %name);

            #printf("DBG: lib %y %y:%s oq: %y\n", %type, %name, o.lastversion, oq - "body");
            # reset parse options before each object is parsed

            if (!oq.language || oq.language == "qore") {
                addWarnings(pgm.parsePending(oq.body, sprintf("%s %s:%s", %type, %name, o.lastversion), oload.warningMask, th.source, th.offset));
            } else if (oq.language == "java") {
                pgm.cacheJavaClass(%name, oq.language_info, NOTHING, tags.classpath);
            } else if (oq.language == "python") {
                pgm.cachePythonClass(%name, oq.body, oq.language_info, NOTHING, tags.module_path);
            } else {
                throw "UNSUPPORTED-LANGUAGE", sprintf("language %y is not supported; expecting qore, java, or python",
                    oq.language);
            }
        }
    }

    static bool isPythonModuleErr(hash<ExceptionInfo> ex) {
        return ex.err == "builtins.ImportError" && ex.desc =~ /^Interpreter change detected/;
    }

    private checkClassConnectorsAndProcessor(QorusProgram pgm) {
        foreach string class_name in (keys lib_map{OT_CLASS}) {
            hash<auto> class_info = sqlif.omqp.selectRow("select * from classes where classid = %v", lib_map{OT_CLASS}{class_name});
            checkClassConnectors(pgm, class_info);
            checkClassProcessor(pgm, class_info);
        }
    }

    private checkClassConnectors(QorusProgram pgm, hash<auto> class_info) {
        if (!class_info.connectors) {
            return;
        }

        Reflection::Class cls = pgm.getClass(class_info.name);
        Reflection::Class observable_cls = pgm.getClass("Observable");

        foreach hash<auto> connector in (UserApi::deserializeQorusData(class_info.connectors)) {
            # event connectors do not need a method
            if (connector.type == "event") {
                continue;
            }
            try {
                auto method = cls.findMethod(connector.method);
                if (!method) {
                    throw "METHOD-ERROR", "no method info found";
                }
            } catch (hash<ExceptionInfo> ex) {
                if (ex.err == "METHOD-ERROR") {
                    oload::error("unknown method for the connector %y of the class %y", connector, class_info.name);
                }
                rethrow;
            }

            if (connector.type == "event" && !cls.getInheritanceAccess(observable_cls)) {
                oload::error("event connector %s::%s must inherit 'Observable'; inheritance list: %y",
                            class_info.name, connector.name, (map $1.getName(), cls.getClassHierarchy()));
            }
        }
    }

    private checkClassProcessor(QorusProgram pgm, hash<auto> class_info) {
        # may be an empty hash (valid processor)
        *hash<auto> processor = UserApi::deserializeQorusData(class_info.processor);
        if (!exists processor) {
            return;
        }

        Reflection::Class cls = pgm.getClass(class_info.name);
        Reflection::Class processor_cls = pgm.getClass("AbstractDataProcessor");
        if (!cls.getInheritanceAccess(processor_cls)) {
            oload::error("processor: %n of class %s must inherit 'AbstractDataProcessor'; inheritance list: %y",
                        processor, class_info.name, (map $1.getName(), cls.getClassHierarchy()));
        }
    }

    addWarnings(*hash we) {
        if (!we.val())
            return;

        if (we.err == "DEPRECATED" && !oload.coptions."warn-deprecated-api") {
            return;
        }
        if (!exists warn) {
            warn = we;
            return;
        }
        append(\warn, we);
    }

    *Program getProgram() {
    }

    private append(reference warn, *hash we) {
        if (!exists warn.next) {
            warn.next = we;
            return;
        }
        append(\warn.next, we);
    }
}

class OMQ::InterfaceConfigContainer inherits InterfaceContainerBase {
    private {
        # configuration item info
        # the key is prefix + name
        hash<string, hash<ConfigItemInfo>> config;

        # migrated values; item prefix + name -> value; only used when inserting new items
        hash<auto> new_values;

        string interfaceType;
        auto interfaceId;
        string interfaceIdColumn;
        string level;

        *softint workflowId; # only for steps

        # config items table for the current interface type
        AbstractTable configItemsTable;
        # config item values table
        AbstractTable configItemValuesTable;

        # interface config item tables
        hash<string, AbstractTable> configItemTables;

        const DefaultColumns = (
            "type", "description", "default_value", "allowed_values", "strictly_local",
            "config_group", "sensitive", "prefix",
        );

        const ConfigItemKeys = (
            "prefix",
            "name",
            "type",
            "description",
            "config_group",
            "allowed_values",
            "sensitive",
            "strictly_local",
            "default_value",
        );
    }
    public {
        # config output flag
        bool config_output = False;
    }

    constructor(string interface_type, auto interface_id, *softint workflow_id, bool id_int = True) {
        interfaceType = interface_type;
        interfaceId = interface_id;
        if (id_int) {
            interfaceId = interfaceId.toInt();
            interfaceIdColumn = interfaceType + "id";
        } else {
            interfaceIdColumn = interfaceType;
        }
        level = interfaceType + ":" + interfaceId;
        workflowId = workflow_id;

        configItemValuesTable = oload::getTable("config_item_values");

        configItemTables{"job"} = oload::getTable("job_config_items");
        configItemTables{"service"} = oload::getTable("service_config_items");
        configItemTables{"step"} = oload::getTable("step_config_items");
        configItemTables{"fsm"} = oload::getTable("fsm_config_items");
        configItemTables{"pipeline"} = oload::getTable("pipeline_config_items");

        configItemsTable = configItemTables{interfaceType};
    }

    # Clears config items only internally from config variable
    # WARNING: does not clear the database, use deleteConfigItems method instead
    clearConfigItems() {
        remove config;
    }

    *list addConfigItemsFromYaml(*string yaml, *hash<auto> old_value_map) {
        if (!yaml) {
            return;
        }

        *list config_items = UserApi::deserializeQorusData(yaml);
        foreach auto config_item in (config_items) {
            hash parents;
            addConfigItem(config_item{"name"}, prepareYamlConfigItem(config_item{"name"},
                          config_item, \parents), old_value_map);
        }
        return config_items;
    }

    hash<ConfigItemInfo> prepareYamlConfigItem(string name, hash config_item, reference<hash> parents) {
        remove config_item{"name"};
        remove config_item{"value"}; # will be processed separetely
        remove config_item{"value_true_type"}; # will be checked automatically
        remove config_item{"default_value_true_type"}; # will be checked automatically
        remove config_item{"is_value_templated_string"}; # will be checked automatically
        remove config_item{"is_default_value_templated_string"}; # will be checked automatically

        if (!config_item.hasKey("parent")) {
            return cast<hash<ConfigItemInfo>>(config_item);
        }

        hash<auto> parent_info = config_item{"parent"};

        string table_name = parent_info{"interface-type"} + (parent_info{"interface-type"} == "class" ? "es" : "s");
        *hash result = sqlif.omqp.selectRow(sprintf("select yaml_config_items from %s where name = %v and version = %v",
            table_name), parent_info{"interface-name"}, parent_info{"interface-version"});

        if (!result) {
            oload::error("unknown %s %s:%s while adding config item %y", parent_info{"interface-type"},
                         parent_info{"interface-name"}, parent_info{"interface-version"}, name);
        }

        string parent_id = parent_info{"interface-type"} + ":" + parent_info{"interface-name"} +
            parent_info{"interface-version"};

        if (parents{parent_id}) {
            oload::error("circular dependency (%s:%s:%s) when adding config item %y", parent_info{"interface-type"},
                         parent_info{"interface-name"}, parent_info{"interface-version"}, name);
        }
        parents{parent_id} = True;

        *list parent_config_items = UserApi::deserializeQorusData(result{"yaml_config_items"});
        if (!parent_config_items) {
            oload::error("parent %s %s:%s does not contain any config items", parent_info{"interface-type"},
                         parent_info{"interface-name"}, parent_info{"interface-version"});
        }

        foreach hash parent_config_item in (parent_config_items) {
            if (parent_config_item{"name"} == name) {
                parent_config_item += config_item - "parent"; # child info has higher priority

                # if parent has another parent do it again
                if (parent_config_item.hasKey("parent")) {
                    parent_config_item = prepareYamlConfigItem(name, parent_config_item, \parents);
                }

                remove parent_config_item{"value"};
                remove parent_config_item{"value_true_type"};
                remove parent_config_item{"default_value_true_type"};
                remove parent_config_item{"is_value_templated_string"};
                remove parent_config_item{"is_default_value_templated_string"};

                return cast<hash<ConfigItemInfo>>(parent_config_item - "name");
            }
        }

        oload::error("%s %s:%s does not contain config item: %y", parent_info{"interface-type"},
                        parent_info{"interface-name"}, parent_info{"interface-version"}, name);
    }

    # inserts/updates config item values defined in "config-items" tag within interface
    insertOrUpdateConfigItemValues(*list<auto> config_items) {
        if (!config_items) {
            return;
        }

        on_error configItemValuesTable.rollback();
        on_success configItemValuesTable.commit();

        foreach hash<auto> config_item in (config_items) {
            if (config_item.hasKey("value")) {
                InterfaceConfigContainer::insertOrUpdateConfigItemValue(config_item{"prefix"} + config_item{"name"},
                    self.level, config_item{"value"});
            }
        }
        # issue #3497: delete all local values not inserted or updated above
        hash wh = {
            "name": op_not(op_in(map $1.prefix + $1.name, config_items, $1.hasKey("value"))),
            "level": level,
        };
        if (!oload.o.override) {
            wh.manually_updated = 0;
        }
        *list<auto> names_to_delete = configItemValuesTable.select({
            "columns": "name",
            "where": wh,
        }).name;
        if (names_to_delete) {
            int rows = configItemValuesTable.del(wh);
            if (oload.o.verbose) {
                printf("deleted %d config item value%s on level %y\n", rows, rows == 1 ? "" : "s", level);
            }
            map oload::omqmap_reload.config_values{$1} = True, names_to_delete;
        }
    }

    private:internal list checkAndPrepareAllowedValues(string item_name, hash<ConfigItemInfo> item) {
        list allowed_values;

        if (item{"allowed_values"}.empty()) {
            oload::error("config item %y allowed_values is an empty list", item_name);
        }

        if (!oload::ConfigItemEnumTypes{item.type}) {
            oload::error("'allowed_values' property is not supported for the given type: %y of the %y config item, "
                         "remove the property or use one of the following types: %y",
                         item.type, item_name, keys oload::ConfigItemEnumTypes);
        }

        bool is_star_type = oload::ConfigItemBasicStarTypes.hasKey(item.type);
        bool has_nothing = False;
        foreach auto value in (item{"allowed_values"}) {
            string value_type = value.type();
            if (value_type == "nothing") {
                if (!is_star_type) {
                    oload::error("'allowed_values' property of the %y config item with type: %y contains NOTHING, "
                                 "change the type of the config item to a star type or remove the value from the "
                                 "'allowed_values' list", item_name, item.type);
                } else if (!has_nothing) {
                    has_nothing = True;
                    continue;
                } else {
                    oload::error("'allowed_values' property of the %y config item contains duplicate NOTHING values",
                                 item_name);
                }
            }

            value_type = value_type == "integer" ? "int": value_type;

            if (is_star_type) {
                value_type = "*" + value_type;
            }
            if (value_type != item.type) {
                oload::error("'allowed_values' property of the %y config item with type: %y contains value "
                             "with incompatible type: %y, change the type of the config item or remove the "
                             "value %y from the 'allowed_values' list", item_name, item.type, value_type, value);
            }
            allowed_values += value;
        }

        allowed_values = sort(allowed_values);

        auto previous_value = allowed_values.first();
        for (int i = 1; i < allowed_values.size(); i++) {
            if (previous_value === allowed_values[i]) {
                oload::error("'allowed_values' property of the %y config item contains duplicate values:"
                             " %y", item_name, previous_value);
            }
            previous_value = allowed_values[i];
        }

        if (has_nothing) {
            allowed_values += NOTHING;
        }

        if (!item.hasKey("default_value")) {
            return allowed_values;
        }

        if (inlist_hard(item{"default_value"}, allowed_values)) {
            return allowed_values;
        }

        oload::error("default value %N of the %y config item with type: %y is not an allowed value; allowed values: "
                        "%y", item{"default_value"}, item_name, item.type, allowed_values);
    }

    private:internal static configItemTypeError(string name, string type, auto value) {
        oload::error("configuration item %y with type %y cannot have default value %y with type %y", name, type,
                     value, value.type());
    }

    # Adds config items internally to config variable, checks if item does not already exist
    # also checks if type is OK
    addConfigItem(string item_name, hash<ConfigItemInfo> item, *hash<auto> old_value_map) {
        # printf("DEBUG %y -> %y\n", item_name, item);

        item_name = item.prefix + item_name;
        if (config{item_name}) {
            oload::error("config item %y has already been declared", item_name);
        }

        if (item.type && !oload::ConfigItemTypes{item.type} && !oload::ConfigItemStarTypes{item.type}) {
            oload::error("config item %y type %y is unknown; expecting one of: %y", item_name, item.type,
                         keys (oload::ConfigItemTypes + oload::ConfigItemStarTypes));
        }

        bool is_enum = item.hasKey("allowed_values");
        if (is_enum) {
            item.allowed_values = checkAndPrepareAllowedValues(item_name, item);
        }

        # check if the old value is compatible
        if (old_value_map.hasKey(item_name) && checkValueType(item, \old_value_map{item_name})) {
            # set new value for the config item; only used when inserting new config items
            new_values{item_name} = old_value_map{item_name};
        }

        if (!item.hasKey("default_value")) {
            config{item_name} = item;
            return;
        }

        if (!exists item.default_value && !oload::ConfigItemStarTypes{item.type}) {
            oload::error("config item %y default value is set to NOTHING while type is %y, "
                         "change the default value or use one of the following star types: %y", item_name, item.type,
                         keys oload::ConfigItemStarTypes);
        }

        # don't check the type if the default value is a templated string (except for enums)
        # or if the type is any
        if ((!is_enum && UserApi::isSingleTemplatedString(item.default_value)) ||
            item.type == "any") {

            config{item_name} = item;
            return;
        }

        if (!checkValueType(item, \item.default_value)) {
            InterfaceConfigContainer::configItemTypeError(item_name, item.type, item.default_value);
        }

        config{item_name} = item;
    }

    # returns True if the value is OK for the item's type, False if not
    static bool checkValueType(hash<ConfigItemInfo> item, reference<auto> val) {
        # do not check if the default value is NOTHING and the type is an "or nothing" type
        if (!exists val && oload::ConfigItemStarTypes{item.type}) {
            return True;
        }

        switch (item.type) {
            case "*int":
            case "int": {
                if (!val.intp()) {
                    return False;
                }
                *softint new_value = val;
                val = new_value;
                break;
            }

            case "*bool":
            case "bool": {
                if (!val.intp()) {
                    return False;
                }
                *softbool new_value = val;
                val = new_value;
                break;
            }

            case "*float":
            case "float": {
                # values that can be converted to an integer can also be converted to a float
                if (!val.intp()) {
                    return False;
                }
                *softfloat new_value = val;
                val = new_value;
                break;
            }

            case "*date":
            case "date": {
                # we allow conversion only from a string
                if (val.typeCode() != NT_DATE && val.typeCode() != NT_STRING) {
                    return False;
                }
                *softdate new_value = val;
                val = new_value;
                break;
            }

            case "*hash":
            case "hash": {
                if (val.typeCode() != NT_HASH) {
                    return False;
                }
                break;
            }

            case "*list":
            case "list": {
                if (val.typeCode() != NT_LIST) {
                    return False;
                }
                break;
            }

            default:
                if (!val.strp()) {
                    return False;
                }
                *softstring new_value = val;
                val = new_value;
                break;
        }

        return True;
    }

    # returns all config items for the current interface from the database
    private:internal *hash<auto> getConfigItems() {
        hash<auto> select_hash = {
            "columns": DefaultColumns + ("name", interfaceIdColumn),
            "where": {
                interfaceIdColumn: interfaceId,
            },
        };
        hash<string, hash<auto>> info;
        foreach hash<auto> row in (configItemsTable.select(select_hash).contextIterator()) {
            map delete row{$1}, keys row, row{$1} === NULL;
            row.strictly_local = row.strictly_local.toBool();
            row.sensitive = row.sensitive.toBool();
            info{row.name} = row - "name";
        }
        return info;
    }

    private:internal *hash<auto> getConfigItemValueOnAllLevels(string item_name, *int limit = NOTHING) {
        hash<auto> select_hash = {
            "columns": ("name", "level", "value"),
            "where": {
                "name": item_name
            },
        };
        select_hash += limit ? {"limit": limit} : {};
        return limit == 1
            ? configItemValuesTable.selectRow(select_hash)
            : configItemValuesTable.select(select_hash);
    }

    private:internal *hash<auto> getConfigItemValueOnCurrentLevel(string item_name) {
        hash<auto> select_hash = {
            "columns": ("name", "level", "value"),
            "where": {
                "level": level,
                "name": item_name,
            }
        };
        return configItemValuesTable.selectRow(select_hash);
    }

    private:internal *hash findFirstNonLocalConfigItem(string item_name) {
        foreach hash<auto> iterator in (configItemTables.pairIterator()) {
            AbstractTable table = iterator.value;
            hash select_hash = {
                "where": {
                    "name": item_name,
                    "strictly_local": 0,
                },
                "limit": 1,
            };

            # ignore config items for the same interface
            if (iterator.key == interfaceType) {
                select_hash."where" += {interfaceIdColumn: op_ne(interfaceId)};
            }

            *hash config_item = table.selectRow(select_hash);
            if (exists config_item) {
                config_item{"table_name"} = iterator.key;
                return config_item;
            }
        }
    }

    # deletes all config item values of the given workflow
    static int deleteAllWorkflowConfigItemValues(softint id) {
        return InterfaceConfigContainer::deleteConfigItemValuesOnLevel("workflow:" + id);
    }

    # deletes all config item values on the given level
    static int deleteConfigItemValuesOnLevel(string level) {
        AbstractTable config_item_values_table = oload::getTable("config_item_values");
        hash<auto> level_hash = {"level": level};
        InterfaceConfigContainer::exportConfigItemValues(level_hash);
        return config_item_values_table.del(level_hash);
    }

    private:internal auto parseConfigItemValue(auto val) {
        if (val.typeCode() == NT_LIST) {
            if (val.lsize() == 1) {
                val = val[0];
            } else {
                return map $1.val() ? parse_yaml($1) : $1, val;
            }
        }
        return val.val() ? parse_yaml(val) : val;
    }

    private:internal int deleteConfigItemValues(string item_name, hash<auto> config_item_info, bool redef_option) {
        int deleted = 0;

        code get_value = *hash<auto> sub (string item_name, softbool strictly_local) {
            return strictly_local
                ? getConfigItemValueOnCurrentLevel(item_name)
                : getConfigItemValueOnAllLevels(item_name);
        };

        *hash<auto> result = get_value(item_name, config_item_info.strictly_local);
        if (!result || !result.value.val()) {
            # Nothing to delete
            return deleted;
        }

        if (!redef_option) {
            oload::error("config item %y cannot be deleted because it has defined %slocal value(s) = %y on the %y "
                "level(s); call oload with the -A or --allow-redef option to forcibly delete values (this should "
                "only be done in a development system)",
                item_name, config_item_info.strictly_local ? "" : "non-", parseConfigItemValue(result.value),
                result.level);
        }

        hash level_name = {"name": item_name, "level": level};
        if (config_item_info.strictly_local) {
            exportConfigItemValues(level_name);
            deleted += configItemValuesTable.del(level_name);
        } else if (!config_item_info.strictly_local && !exists findFirstNonLocalConfigItem(item_name)) {
            # if config item is not strictly local then delete values on all levels in case
            # no other config item defines this config item
            exportConfigItemValues(level_name - "level");
            deleted += configItemValuesTable.del(level_name - "level");
        }
        oload::omqmap_reload.config_values{item_name} = True;
        return deleted;
    }

    private:internal int deleteConfigItemValues(hash<auto> config_items, bool redef_option) {
        int deleted = 0;
        map deleted += deleteConfigItemValues($1.key, $1.value, redef_option), config_items.pairIterator();
        return deleted;
    }

    # deletes all config items and their values if redef_option is True from the database
    int deleteConfigItems(*bool redef_option = False, *bool commit = True) {
        *hash<auto> config_items = getConfigItems();
        return config_items ? deleteConfigItems(config_items, redef_option, commit) : 0;
    }

    # deletes given config items from the database
    private:internal int deleteConfigItems(hash<auto> config_items, bool redef_option, bool commit = False) {
        # one commit / rollback per datasource
        on_error {
            if (commit) {
                configItemsTable.rollback();
            }
        }
        on_success {
            if (commit) {
                configItemsTable.commit();
            }
        }

        int deleted = deleteConfigItemValues(config_items, redef_option);
        deleted += configItemsTable.del({interfaceIdColumn: interfaceId, "name": op_in(keys config_items)});
        return deleted;
    }

    # inserts/updates config items from internal config variable into the database
    hash<string, int> insertUpdateConfigItems(*bool redef_option = False) {
        # one commit / rollback per datasource
        on_error {
            configItemsTable.rollback();
        }
        on_success {
            configItemsTable.commit();
        }

        # hash of existing config items
        *hash<auto> config_items = getConfigItems();

        hash<string, int> operations = {};
        foreach hash<auto> iterator in (config.pairIterator()) {
            string item_name = iterator.key;
            if (iterator.value.hasKey("default_value")) {
                iterator.value.default_value = UserApi::serializeQorusDataWithNothing(iterator.value.default_value);
            }

            if (*hash<auto> current_item_info = remove config_items{item_name}) {
                hash<string, int> res = updateConfigItem(item_name, current_item_info, iterator.value, redef_option);
                operations.unchanged += res.unchanged;
                operations.updated += res.updated;
                operations.deleted += res.deleted;
                if (res.updated || res.deleted) {
                    oload::omqmap_reload.config_values{item_name} = True;
                }
            } else {
                operations.inserted += insertConfigItem(item_name, iterator.value, redef_option).inserted;
                oload::omqmap_reload.config_values{item_name} = True;
            }
        }

        if (config_items) {
            operations.deleted += deleteConfigItems(config_items, redef_option);
        }
        return operations;
    }

    auto serializeDbConfigItemData(auto val) {
        if (!exists val) {
            return;
        }
        return UserApi::serializeQorusDataWithNothing(val);
    }

    private:internal hash<string, int> updateConfigItem(string item_name, hash<auto> current_item_info,
            hash<auto> new_item_info, bool redef_option) {
        hash<string, int> operations;

        hash<auto> new_item_info_db += new_item_info;
        if (exists new_item_info.allowed_values) {
            new_item_info_db.allowed_values = serializeDbConfigItemData(new_item_info.allowed_values);
        }
        if (new_item_info_db == current_item_info{ConfigItemKeys}) {
            ++operations.unchanged;
            return operations;
        }

        # strictly_local has been changed, 2 scenarios:
        # 1. new strictly_local == True: no need to check type and delete any values
        # 2. new strictly_local == False: check type and allowed_values since there can be some conficts and update
        #    all config items and their values if redef option is set to True

        if (new_item_info.strictly_local != current_item_info.strictly_local && !new_item_info.strictly_local) {
            checkConfigItemType(item_name, new_item_info, redef_option);
            checkConfigItemAllowedValues(item_name, new_item_info, redef_option);
            operations.deleted += deleteConfigItemValues({item_name: current_item_info}, redef_option);
        }

        if (new_item_info.type != current_item_info.type) {
            checkConfigItemType(item_name, new_item_info, redef_option);
            operations.deleted += deleteConfigItemValues({item_name: new_item_info}, redef_option);
        }

        if (new_item_info_db.allowed_values != current_item_info.allowed_values) {
            checkConfigItemAllowedValues(item_name, new_item_info, redef_option);
            operations.deleted += deleteConfigItemValues({item_name: new_item_info}, redef_option);
        }

        hash<auto> update_where = {
            "name": item_name,
            interfaceIdColumn: interfaceId,
        };

        hash<auto> update_hash = {
            "description": new_item_info.description,
            "type": new_item_info.type,
            "default_value": new_item_info.default_value,
            "strictly_local": new_item_info.strictly_local.toInt(),
            "config_group": new_item_info.config_group,
            "sensitive": new_item_info.sensitive.toInt(),
            "prefix": new_item_info.prefix,
            "allowed_values": new_item_info_db.allowed_values,
        };

        try {
            configItemsTable.update(update_hash, update_where);
        } catch (hash<ExceptionInfo> ex) {
            stderr.printf("error updating item %y: %s: %s: item config: %y", item_name, ex.err, ex.desc, new_item_info);
            rethrow;
        }
        ++operations.updated;
        return operations;
    }

    private:internal updateConfigItemInAllTables(hash<auto> update_where, hash<auto> update_hash) {
        foreach hash<auto> iterator in (configItemTables.pairIterator()) {
            AbstractTable table = iterator.value;
            table.update(update_hash, update_where);
        }
    }

    private:internal hash<string, int> insertConfigItem(string item_name, hash<auto> item_info, bool redef_option) {
        hash<string, int> operations;
        # printf("DEBUG: inserting config item: %s - %n\n", item_name, item_info);
        checkConfigItemType(item_name, item_info, redef_option);
        checkConfigItemAllowedValues(item_name, item_info, redef_option);

        hash insert_hash = {
            "name": item_name,
            "description": item_info.description,
            "type": item_info.type,
            "default_value": item_info.default_value,
            "strictly_local": item_info.strictly_local.toInt(),
            "config_group": item_info.config_group,
            "sensitive": item_info.sensitive.toInt(),
            "prefix": item_info.prefix,
            interfaceIdColumn: interfaceId,
        };

        insert_hash += item_info.allowed_values.val() ?
            {"allowed_values": UserApi::serializeQorusData(item_info.allowed_values)} : {};

        configItemsTable.insert(insert_hash);
        ++operations.inserted;

        # issue #3281: insert migrated values, if any
        if (new_values.hasKey(item_name)) {
            hash<auto> new_row = {
                "name": item_name,
                "level": level,
                "value": UserApi::serializeQorusDataWithNothing(new_values{item_name}),
            };
            configItemValuesTable.insert(new_row);
            ++operations.inserted;
        }

        return operations;
    }

    # if there is a config item defined by any interface with the same name and has strictly_local == False
    # then check if it has the same type as a new one config if not then exit with an error
    # if redef_option is True type will be updated for all config items with the same name and no error will
    # be thrown
    private:internal checkConfigItemType(string item_name, hash<auto> item_info, bool redef_option) {
        if (item_info.strictly_local == True) {
            # strictly local config items are OK, types will be checked in case of updates
            return;
        }

        *hash config_item = findFirstNonLocalConfigItem(item_name);
        if (!exists config_item) {
            return;
        }

        if (config_item.type == item_info.type) {
            return;
        }

        # workaround of https://git.qoretechnologies.com/qorus/issues/issues/3233
        if (redef_option) {
            hash<auto> update_where = {"name": item_name, "strictly_local": 0};
            hash update_hash = {"type": item_info{"type"}};
            updateConfigItemInAllTables(update_where, update_hash);
            return;
        }

        string table_name = config_item{"table_name"};
        string id_column = table_name + "id";
        oload::error("cannot create config item %s; the same item with a different type: %s is already defined"
                     " by %s with id %s; update config item or call oload with the -A or --allow-redef option"
                     " to forcibly update type for all config items with the same name", item_name, config_item.type,
                     table_name, config_item{id_column});
    }

    # if there is a config item defined by any interface with the same name and has strictly_local == False
    # then check if it has the same allowed values as a new one config if not then exit with an error
    # if redef_option is True allowed values will be updated for all config items with the same name and no error will
    # be thrown
    private:internal checkConfigItemAllowedValues(string item_name, hash<auto> item_info, bool redef_option) {
        if (item_info.strictly_local == True) {
            # strictly local config items are OK
            return;
        }

        *hash config_item = findFirstNonLocalConfigItem(item_name);
        if (!exists config_item) {
            return;
        }

        *string serialized_allowed_values = config_item{"allowed_values"};
        config_item{"allowed_values"} = deserialize_qorus_data(config_item{"allowed_values"});
        if (item_info{"allowed_values"} === config_item{"allowed_values"}) {
            return;
        }

        # workaround of https://git.qoretechnologies.com/qorus/issues/issues/3233
        if (redef_option) {
            hash<auto> update_where = {"name": item_name, "strictly_local": 0};
            hash<auto> update_hash = {"allowed_values": serialized_allowed_values};
            updateConfigItemInAllTables(update_where, update_hash);
            return;
        }

        string table_name = config_item{"table_name"};
        string id_column = table_name + "id";
        oload::error("cannot create config item %s with allowed values: %y; the same item with different allowed"
                     " values: %y is already defined by %s with id %s; update config item or call oload with the -A"
                     " or --allow-redef option to forcibly update allowed values for all config items with the"
                     " same name", item_name, item_info.allowed_values, config_item.allowed_values, table_name,
                     config_item{id_column});
    }

    # for importing config item values on the workflow level defined in the workflow definition file
    static importWorkflowConfigItemValues(hash workflow) {
        string level = "workflow:" + workflow.workflowid;
        foreach auto config_item_value in (workflow{"config-item-values"}) {
            InterfaceConfigContainer::insertOrUpdateConfigItemValue(
                config_item_value{"prefix"} + config_item_value{"name"}, level, config_item_value{"value"});
        }
    }

    # for importing global config item values defined in a separated file
    static importGlobalConfigItemValues(hash parsed_yaml, string file_name) {
        AbstractTable config_item_values_table = oload::getTable("config_item_values");
        on_error config_item_values_table.rollback();
        on_success config_item_values_table.commit();

        foreach auto global_config_item_value in (parsed_yaml{"global-config-item-values"}) {
            InterfaceConfigContainer::insertOrUpdateConfigItemValue(
                global_config_item_value{"prefix"} + global_config_item_value{"name"}, "global",
                global_config_item_value{"value"});
        }

        if (oload.o.verbose) {
            printf("config item values %s successfully imported\n", file_name);
        }
    }

    static importConfigItemValues(hash parsed_yaml, string file_name) {
        printf("importing config item values %s\n", file_name);
        if (oload.o.import_cfg_val_rest) {
            try {
                qrest.put("system/config", {"config-items": parsed_yaml{"config-item-values"}});
                printf("OK \n");
            } catch (hash<ExceptionInfo> ex) {
                # ignore connection refused errors (system down), otherwise output a warning msg
                if (ex.err != "SOCKET-CONNECT-ERROR") {
                    oload::warning("failed to import config item values from: %y: %s: %s", parsed_yaml, ex.err, ex.desc);
                }
            }
            return;
        }

        AbstractTable config_item_values_table = oload::getTable("config_item_values");
        on_error config_item_values_table.rollback();
        on_success config_item_values_table.commit();

        foreach auto config_item_value in (parsed_yaml{"config-item-values"}) {
            string interface_type = config_item_value{"interface-type"};

            string level = interface_type;
            if (interface_type != "global") {
                *string interface_name = config_item_value{"interface-name"};
                *string interface_version = config_item_value{"interface-version"};
                string id_column = interface_type + "id";

                if (!exists interface_name) {
                    oload.error("no interface name provided for the config item value: %y", config_item_value);
                }
                if (!exists interface_version) {
                    oload.error("no interface version provided for the config item value: %y", config_item_value);
                }

                *softint id = sqlif.omqp.selectRow(sprintf("select %s from %ss where name = %v and version = %v",
                                                           id_column, interface_type),
                                                   interface_name,
                                                   interface_version){id_column};

                if (!id) {
                    oload.error("cannot find %s %s:%s when importing config item value: %y",
                                interface_type, interface_name, interface_version, config_item_value);
                }

                level += ":" + id;
            }
            InterfaceConfigContainer::insertOrUpdateConfigItemValue(
                config_item_value{"prefix"} + config_item_value{"name"}, level, config_item_value{"value"});
        }
        if (oload.o.verbose) {
            printf("config item values %s successfully imported\n", file_name);
        }
    }

    private:internal static *hash findNonLocalConfigItem(string name) {
        list<auto> tables = (
            oload::getTable("job_config_items"),
            oload::getTable("step_config_items"),
            oload::getTable("service_config_items"),
            oload::getTable("fsm_config_items"),
            oload::getTable("pipeline_config_items"),
        );

        foreach AbstractTable table in (tables) {
            hash<auto> select_hash = {
                "columns": InterfaceConfigContainer::DefaultColumns,
                "where": {
                    "name": name,
                    "strictly_local": 0,
                },
                "limit": 1,
            };

            *hash<auto> config_item = table.selectRow(select_hash);
            if (exists config_item) {
                return config_item;
            }
        }
    }

    private:internal static checkConfigItemValueType(string name, hash<ConfigItemInfo> config_item,
                                                     reference<auto> value) {
        code print_ok = sub() {
            if (!oload.o.quiet && oload.o.verbose > 2) {
                printf("OK\n");
            }
        };

        if (!oload.o.quiet && oload.o.verbose > 2) {
            printf("checking config item %y value type: ", name);
        }

        if (!config_item{"allowed_values"} &&
            (UserApi::isSingleTemplatedString(value) || config_item{"type"} == "any")) {
            print_ok();
            return;
        }

        if (!InterfaceConfigContainer::checkValueType(config_item, \value)) {
            InterfaceConfigContainer::configItemTypeError(name, config_item{"type"}, value);
        }

        if (!config_item{"allowed_values"}) {
            print_ok();
            return;
        }

        foreach auto allowed_value in (config_item{"allowed_values"}) {
            if (value === allowed_value) {
                print_ok();
                return;
            }
        }
        oload::error("cannot set value %y for the configuration item %y the value is not allowed, allowed values: %y",
                     value, name, config_item{"allowed_values"});
    }

    private:internal static checkConfigItemValue(string name, string level, reference<auto> value) {
        (*string interface_type, auto interface_id) = level.split(":");

        *hash config_item;
        if (interface_type != "global" && interface_type != "workflow") {
            if (!exists interface_id) {
                oload::error("incorrect level %y when importing %y config item value %y", level, name, value);
            }
            string id_column = interface_type;
            if (interface_type != "fsm" && interface_type != "pipeline") {
                id_column += "id";
                interface_id = interface_id.toInt();
            }

            AbstractTable config_items = oload::getTable(sprintf("%s_config_items", interface_type));
            hash<auto> select_hash = {
                "columns": InterfaceConfigContainer::DefaultColumns,
                "where": {
                    "name": name,
                    id_column: interface_id,
                },
            };
            config_item = config_items.selectRow(select_hash);
        } else {
            config_item = InterfaceConfigContainer::findNonLocalConfigItem(name);
        }

        if (!config_item) {
            oload::error("unable to find config item %y when importing config item value %y with level %y", name,
                         value, level);
        }

        config_item{"allowed_values"} = UserApi::deserializeQorusData(config_item{"allowed_values"}) ?? list();
        config_item{"strictly_local"} = config_item{"strictly_local"}.toBool();
        config_item.sensitive = config_item.sensitive.toBool();

        InterfaceConfigContainer::checkConfigItemValueType(name, cast<hash<ConfigItemInfo>>(config_item), \value);
    }

    private:internal static insertOrUpdateConfigItemValue(string name, string level, auto value) {
        InterfaceConfigContainer::checkConfigItemValue(name, level, \value);

        *hash<auto> config_item_value = InterfaceConfigContainer::selectConfigItemValues({
            "name": name,
            "level": level,
        }, 1);
        string sval = UserApi::serializeQorusDataWithNothing(value);
        if (!config_item_value) {
            InterfaceConfigContainer::insertConfigItemValue(name, level, sval);
        } else {
            # do not update if the values are already identical
            if (sval == config_item_value.value) {
                if (!oload.o.quiet && oload.o.verbose > 2) {
                    printf("config item %y (level: %y) value does not need updating\n", name, level);
                    printf("sval: %y == %y\n", sval, config_item_value.value);
                }
                return;
            } else {
                InterfaceConfigContainer::updateConfigItemValue(name, level, sval, config_item_value);
            }
        }
    }

    private:internal static insertConfigItemValue(string name, string level, auto value) {
        AbstractTable config_item_values_table = oload::getTable("config_item_values");

        if (!oload.o.quiet && oload.o.verbose) {
            printf("inserting value for config item %y\n", name);
        }
        config_item_values_table.insert({"name": name, "level": level, "value": value});
        oload::omqmap_reload.config_values{name} = True;
    }

    private:internal static updateConfigItemValue(string name, string level, auto value, hash<auto> current_config_item_value) {
        AbstractTable config_item_values_table = oload::getTable("config_item_values");

        # issue #3463: only update the config item if it has not been manually updated
        if (current_config_item_value.manually_updated) {
            if (oload.o.override) {
                if (oload.o.verbose && !oload.o.quiet) {
                    printf("overriding value of manually updated config item %y (level: %y); ", name, level);
                }
            } else {
                if (oload.o.verbose && !oload.o.quiet) {
                    printf("not updating value of config item %y (level: %y); already updated manually; use -O to "
                        "override manual updates\n", name, level);
                }
                return;
            }
        } else if (!oload.o.quiet && oload.o.verbose) {
            printf("updating value of config item %y (level: %y)\n", name, level);
        }
        config_item_values_table.update({"value": value}, {"level": level, "name": name});
        oload::omqmap_reload.config_values{name} = True;
    }

    /* Exports config item values.
        @param config_item (optional): hash describing config item value to be exported, keys:
        - \c name: name with prefix of the config item
        - \c level: level of the config item value
        If not provided all config item values will be exported.
        @param file_name: name of the file to be exported to, if not provided then values will be exported to a file in
        temp directory with a generated name.
    */
    static exportConfigItemValues(*hash config_item, *string file_name) {
        *hash yaml = InterfaceConfigContainer::getConfigItemValues(config_item);
        if (!yaml) {
            return;
        }

        string yaml_string = make_yaml(yaml, YAML::BlockStyle);
        InterfaceConfigContainer::saveConfigItemValues(yaml_string, file_name);
    }

    private:internal static *hash<auto> selectConfigItemValues(*hash config_item, *int limit = NOTHING) {
        hash select_hash = {
            "columns": ("name", "level", "value", "manually_updated"),
        };

        if (exists limit) {
            select_hash{"limit"} = limit;
        }

        if (config_item) {
            select_hash{"where"} += config_item{"name"} ? {"name": config_item{"name"}} : {};
            select_hash{"where"} += config_item{"level"} ? {"level": config_item{"level"}} : {};
        }

        AbstractTable config_item_values_table = oload::getTable("config_item_values");
        return (limit == 1) ? config_item_values_table.selectRow(select_hash) : config_item_values_table.select(select_hash);
    }

    private:internal static *hash getConfigItemValues(*hash config_item) {
        *hash result = InterfaceConfigContainer::selectConfigItemValues(config_item);
        if (!result || !result.name) {
            printf("no config item values to export\n");
            return;
        }

        hash yaml = {
            "type": "config-item-values"
        };
        yaml{"config-items"} = ();
        context (result) {
            (*string type, *string id) = %level.split(":");

            hash config_item_value = {"name": %name, "interface-type": type,
                "value": UserApi::deserializeQorusData(%value)};

            if (exists id && id.intp()) {
                string id_column = type + "id";
                AbstractTable interface_table = oload::getTable(type + "s");
                *hash<auto> interface = interface_table.selectRow({
                    "columns": ("name", "version"),
                    "where": {
                        id_column: id.toInt()
                    }
                });

                if (interface) {
                    config_item_value += {
                        "interface-name": interface{"name"},
                        "interface-version": interface{"version"}
                    };
                }
            }

            yaml{"config-items"} += config_item_value;
        }

        return yaml;
    }

    private:internal static saveConfigItemValues(string yaml, *string save_to_file) {
        string file_name = save_to_file ?? tmp_location() + DirSep + "qorus_config_item_values_" +
                           get_random_string(10) + ".yaml";
        File file();
        file.open2(file_name, O_CREAT | O_WRONLY | O_TRUNC);
        file.write(yaml);

        printf("config item values have been exported to: %s\n", file_name);
    }

    static *hash<auto> getInterfaceConfigItemValues(string level) {
        AbstractTable configItemValuesTable = oload::getTable("config_item_values");
        return map {$1.name: deserialize_qorus_data($1.value)}, configItemValuesTable.select({
            "columns": ("name", "value"),
            "where": {
                "level": level,
            },
        }).contextIterator();
    }
}

# Qorus WorkflowValidator class

# declare global variables for WorkflowValidator
our DatasourcePool omqp;

class OMQ::WorkflowInterfaceContainerBase inherits public InterfaceContainerBase, public OMQ::WorkflowDef {
    public {
        # Program object to hold workflow functions
        WorkflowProgram pgm(oload.options, NOTHING, PO_NO_USER_API);
    }

    private {
        hash handler;

        # issue #2282 to ensure that functions are only loaded once; function instance ID -> True
        hash<string, bool> fh;

        # issue #1704: map of step class names to step base classes for Qore verification
        # cannot be static as the classes are retrieved from each Program container
        hash<string, Reflection::Class> java_class_map;

        # issue #1704: map classes to step types; stepid -> step type base class name
        hash<string, string> step_class_map;

        # issue #1704: static map of step class names to step base classes for Qore verification
        static hash<string, Reflection::Class> qore_class_map;

        # issue #3524: static map of step class names to step base classes for Qore verification
        static hash<string, Reflection::Class> python_class_map;

        # issue #2587: get a list of workflow modules
        list<string> workflow_module_list;

        # set of step classes: lang -> stepid (wf for the wf class) -> class name -> True
        hash<auto> class_map = {};
    }

    constructor(hash<auto> info) {
        workflowid = -1;
        # export system objects to workflow program
        pgm.importGlobalVariable("omqservice");
        createProgram();
    }

    constructor(int id) {
        self.workflowid = id;
        # export system objects to workflow program
        pgm.importGlobalVariable("omqservice");
    }

    init() {
        *hash<auto> q = sqlif.omqp.selectRow("select workflowid, name, version, patch, description, author, remote, "
            "manual_remote, workflow_modules, autostart, manual_autostart, sla_threshold, manual_sla_threshold, "
            "max_instances, enabled, code, language, language_info, class_name, has_detach, errorfunction_instanceid, "
            "attach_func_instanceid, detach_func_instanceid, onetimeinit_func_instanceid, errhandler_func_instanceid, "
            "deprecated, created, modified from workflows where workflowid = %v", workflowid);

        # check if workflow exists
        if (!q.name) {
            throw "NO-SUCH-WORKFLOW", sprintf("workflowid %d", workflowid);
        }

        # rename deprecated to depr
        q.depr = remove q.deprecated;

        # map fields to workflow hash
        foreach string col in (keys q) {
            if (q{col} !== NULL) {
                self{col} = q{col};
            }
        }

        if (workflow_modules) {
            workflow_module_list = map trim($1), workflow_modules.split(",");
        }

        # create program and import fake library functions for workflow program
        createProgram();
    }

    loadErrorFunction(string name, softint id) {
        if (!pgm.existsFunction(name)) {
            loadFunctionIds(id);
        }
    }

    private createProgram() {
        # import fake workflow API
        qorus_load_fake_workflow_api_module(pgm);

        # load any workflow modules
        foreach string mod in (workflow_module_list) {
            try {
                string feature_name = qorus_load_workflow_module(mod);
                pgm.loadModule(feature_name);
            } catch (hash<ExceptionInfo> ex) {
                oload::error("cannot setup Program for workflow validation; cannot load module %y specified "
                    "by the \"workflow-modules\" option; correct or remove this option and try again; error: "
                    "%s: %s: %s", mod, get_ex_pos(ex), ex.err, ex.desc);
            }
        }

        # load workflow library
        loadLibrary();

        # load workflow functions
        if (attach_func_instanceid) {
            if (self{"code"}) {
                oload::error("workflow %s v%s (%d): cannot define an attach function when a workflow class has been "
                    "defined", name, version, workflowid);
            }
            handler.attach = loadFunctionId(attach_func_instanceid);
        }

        if (detach_func_instanceid) {
            if (self{"code"}) {
                oload::error("workflow %s v%s (%d): cannot define an detach function when a workflow class has been "
                    "defined", name, version, workflowid);
            }
            handler.detach = loadFunctionId(detach_func_instanceid);
        }

        if (onetimeinit_func_instanceid) {
            if (self{"code"}) {
                oload::error("workflow %s v%s (%d): cannot define an one time init function when a workflow class "
                    "has been defined", name, version, workflowid);
            }
            handler.onetimeinit = loadFunctionId(onetimeinit_func_instanceid);
        }

        if (errhandler_func_instanceid) {
            if (self{"code"}) {
                oload::error("workflow %s v%s (%d): cannot define an error handler function when a workflow class "
                    "has been defined", name, version, workflowid);
            }
            handler.error_handler = loadFunctionId(errhandler_func_instanceid);
        }

        # cache step
        cacheSteps();

        createProgramImpl();
    }

    # id == wf for the workflow class, id == stepid for step classes
    private:internal cacheClass(softstring id, hash<auto> info, hash<auto> tags) {
        if (info.language == "qore") {
            addWarnings(pgm.parsePending(info{"code"}, info.label ?? info.classname, oload.warningMask,
                        tags.sys.source, tags.sys.offset));
            class_map.qore{id}{info.classname} = True;
        } else if (info.language == "java") {
            *string main_class_name = pgm.cacheJavaClass(info.classname, info.language_info, True, tags.classpath);
            if (main_class_name) {
                class_map.java{id}{main_class_name} = True;
            }
        } else if (info.language == "python") {
            pgm.cachePythonClass(info.classname, info."code", info.language_info, True, tags.module_path);
            class_map.python{id}{info.classname} = True;
        } else {
            oload.error("unknown or missing language %y for class %y", info.language, info.classname);
        }
    }

    Program getProgram() {
        return pgm;
    }

    parseCommit() {
        # parse workflow code
        if (self{"code"}) {
            if (!class_name) {
                class_name = name;
            }

            *string label;
            if (language == "qore") {
                label = sprintf("workflow: %s v%s (%d)", name, version, workflowid);
            }
            hash<auto> info = {
                "code": self{"code"},
                "label": label,
                "classname": class_name,
                "language_info": language_info,
                "language": language,
            };
            hash<auto> wf_tags = sqlif.getTags("workflow", workflowid);
            cacheClass("wf", info, wf_tags);
        }

        addWarnings(pgm.parseCommit(class_map, oload.warningMask));
        checkClassConnectorsAndProcessor(pgm);

        if (self{"code"}) {
            code check_wf_inheritance = sub (Reflection::Class obj_class, Reflection::Class inherited_class) {
                if (obj_class.getInheritanceAccess(inherited_class) == AC_PUBLIC) {
                    return;
                }

                throw "WORKFLOW-CLASS-ERROR",
                    sprintf("workflow %s v%s (%d) class %y does not inherit %y; inheritance list: %y",
                            name, version, workflowid, class_name, "QorusWorkflow",
                            (map $1.getName(), obj_class.getClassHierarchy()));
            };

            Reflection::Class workflow_base_cls;
            Reflection::Class obj_class;
            if (language == "qore") {
                # cache base class object
                if (!qore_class_map.QorusWorkflow) {
                    qore_class_map.QorusWorkflow = pgm.getClass("QorusWorkflow");
                }

                obj_class = pgm.getCreateClass("qore", class_name);
                check_wf_inheritance(obj_class, qore_class_map.QorusWorkflow);
                workflow_base_cls = pgm.getClass("OMQ::UserApi::Workflow::QorusWorkflow");
            } else if (language == "java") {
                # get base class object
                java_class_map.QorusWorkflow = pgm.getClass("OMQ::UserApi::Workflow::QorusWorkflow");

                obj_class = pgm.getCreateClass("java", class_name);
                check_wf_inheritance(obj_class, java_class_map.QorusWorkflow);
                workflow_base_cls = java_class_map.QorusWorkflow;
            } else if (language == "python") {
                # cache base class object
                python_class_map.QorusWorkflow = pgm.getClass("OMQ::UserApi::Workflow::QorusWorkflow");

                obj_class = pgm.getCreateClass("python", class_name);
                check_wf_inheritance(obj_class, python_class_map.QorusWorkflow);
                workflow_base_cls = pgm.getClass("QorusWorkflow");
            }

            # check if workflow class has a detach method
            bool really_has_detach = !obj_class.findNormalMethod("detachImpl").method.getClass().isEqual(workflow_base_cls);
            if (really_has_detach != has_detach) {
                if (oload.o.verbose) {
                    printf("marking workflow detach method = %y...\n", really_has_detach);
                }
                if (oload.o.verbose) {
                    printf("workflow %s:%s (%d) has a detach method\n", name, version, workflowid);
                }

                AbstractTable workflows = oload::getTable("workflows");
                on_error workflows.rollback();
                on_success workflows.commit();
                workflows.update({
                    "has_detach": really_has_detach.toInt(),
                }, {
                    "workflowid": workflowid,
                });
            }
        }

        # issue #1704: verify step classes
        foreach hash<auto> i in (step_class_map.pairIterator()) {
            hash<auto> steph = steps{i.key};

            softint stepid = i.key;

            # separated config container for each step
            # this allows to have the same config item in different steps
            # in case they are local then type can be different otherwise it should be the same
            InterfaceConfigContainer step_config_container("step", stepid, workflowid);

            *list yaml_config_items = step_config_container.addConfigItemsFromYaml(steph{"yaml_config_items"},
                oload.config_item_value_map.step{stepid});

            bool exists_config_items_yaml = yaml_config_items.toBool();
            bool insert_config_items = exists_config_items_yaml;

            # printf("DEBUG: parseCommit:pgm.lang_data: %y\n", pgm.lang_data);

            object step_object;
            if (!exists_config_items_yaml) {
                if (steph.language == "qore") {
                    # get base class object
                    qore_class_map{i.value} = pgm.getClass(i.value);

                    Reflection::Class obj_class = pgm.getCreateClass("qore", steph.classname);
                    if (obj_class.getInheritanceAccess(qore_class_map{i.value}) != AC_PUBLIC) {
                        throw "STEP-CLASS-ERROR", sprintf("step %s v%s (%d) class %y does not inherit %y; class name: %y, inheritance list: %y",
                            steph.name, steph.version, stepid, steph.classname, i.value, obj_class.getName(),
                            (map $1.getName(), obj_class.getClassHierarchy()));
                    }

                    if (oload.o.verbose > 2) {
                        printf("step %s v%s (%d) Qore class %y is a subclass of %y\n", steph.name, steph.version, stepid,
                            steph.classname, qore_class_map{i.value}.getPathName());
                    }

                    AbstractMethod m = obj_class.findMethod("getConfigItemsImpl").method;
                    if (!m.getClass().isEqual(qore_class_map{i.value})) {
                        # create step object to get config items
                        step_object = pgm.getCreateObject(stepid, "qore", steph.classname);

                        # get step configuration
                        *hash<string, hash<ConfigItemInfo>> config_items = step_object.getConfigItems();
                        if (config_items) {
                            step_config_container.clearConfigItems();
                            map step_config_container.addConfigItem($1.key, $1.value, oload.config_item_value_map.step{stepid}),
                                config_items.pairIterator();
                            insert_config_items = True;
                        }
                    }
                } else if (steph.language == "java") {
                    java_class_map{i.value} = pgm.getClass(i.value);

                    Reflection::Class obj_class = pgm.getCreateClass("java", steph.classname);
                    if (obj_class.getInheritanceAccess(java_class_map{i.value}) != AC_PUBLIC) {
                        throw "STEP-CLASS-ERROR", sprintf("step %s v%s (%d) class %y does not inherit %y; class name: %y, inheritance list: %y",
                            steph.name, steph.version, i.key, steph.classname, i.value, obj_class.getName(),
                            (map $1.getName(), obj_class.getClassHierarchy()));
                    }

                    if (oload.o.verbose > 2) {
                        printf("step %s v%s (%d) Java class %y is a subclass of %y\n", steph.name, steph.version, i.key,
                            steph.classname, java_class_map{i.value}.getName());
                    }

                    AbstractMethod m = obj_class.findMethod("getConfigItemsImpl").method;
                    if (!m.getClass().isEqual(java_class_map{i.value})) {
                        # create step object to get config items
                        step_object = pgm.getCreateObject(stepid, "java", steph.classname);

                        # get step configuration
                        *hash<string, object> config_items = step_object.getConfigItems();
                        if (config_items) {
                            step_config_container.clearConfigItems();
                            # make a configuration list from the ConfigItem objects
                            foreach auto iterator in (config_items.pairIterator()) {
                                hash<ConfigItemInfo> config_item = cast<hash<ConfigItemInfo>>({
                                    "description": iterator.value.description,
                                    "type": iterator.value.type,
                                    "strictly_local": iterator.value.strictly_local,
                                    "config_group": iterator.value.config_group,
                                    "sensitive": iterator.value.sensitive,
                                    "prefix": iterator.value.prefix,
                                });
                                config_item += iterator.value.is_default_value_set ?
                                    {"default_value": iterator.value.default_value} : {};
                                config_item += iterator.value.has_allowed_values ?
                                    {"allowed_values": iterator.value.allowed_values}: {};
                                step_config_container.addConfigItem(iterator.key, config_item,
                                    oload.config_item_value_map.step{stepid});
                            }
                            insert_config_items = True;
                        }
                    }
                } else if (steph.language == "python") {
                    # cache base class object
                    python_class_map{i.value} = pgm.getClass(i.value);

                    Reflection::Class obj_class = pgm.getCreateClass("python", steph.classname);
                    if (obj_class.getInheritanceAccess(python_class_map{i.value}) != AC_PUBLIC) {
                        throw "STEP-CLASS-ERROR", sprintf("step %s v%s (%d) class %y does not inherit %y; "
                            "class name: %y, inheritance list: %y",
                            steph.name, steph.version, stepid, steph.classname, i.value, obj_class.getName(),
                            (map $1.getName(), obj_class.getClassHierarchy()));
                    }

                    if (oload.o.verbose > 2) {
                        printf("step %s v%s (%d) Python class %y is a subclass of %y\n", steph.name, steph.version,
                            stepid, steph.classname, python_class_map{i.value}.getPathName());
                    }

                    AbstractMethod m = obj_class.findMethod("getConfigItemsImpl").method;
                    if (!m.getClass().isEqual(python_class_map{i.value})) {
                        # create step object to get config items
                        step_object = pgm.getCreateObject(stepid, "python", steph.classname);
                        # get step configuration
                        *hash<string, hash<ConfigItemInfo>> config_items = step_object.getConfigItems();
                        if (config_items) {
                            step_config_container.clearConfigItems();
                            map step_config_container.addConfigItem($1.key, $1.value,
                                oload.config_item_value_map.step{stepid}),
                                config_items.pairIterator();
                            insert_config_items = True;
                        }
                    }
                }
            }

            # issue #2880: update step metadata
            if (step_object) {
                # issue #3538: only update step metadata if the method is to be called is not the base class
                #              (QorusStepBase) method
                if (Class::getClass(step_object).findMethod("getStepMetadata").method.getClass().getName() !=
                    "QorusStepBase") {
                    *hash<auto> smd = step_object.getStepMetadata();
                    *string mdstr = smd ? serialize_qorus_data(smd) : NOTHING;

                    AbstractTable steps = oload::getTable("steps");
                    on_error steps.rollback();
                    on_success steps.commit();

                    steps.update({"userdata": mdstr}, {"stepid": i.key.toInt()});
                }
            }

            if (insert_config_items) {
                hash<string, int> op_hash = step_config_container.insertUpdateConfigItems(oload.o.redef);
                if (!!oload.o.quiet) {
                    if (!step_config_container.config_output && oload.o.verbose) {
                        print("creating config items...\n");
                        step_config_container.config_output = True;
                    }
                    if (oload.o.verbose) {
                        printf("step %s:%s (%d) configuration items: %y\n", steph.name, steph.version, stepid,
                            op_hash);
                    }
                }
                if (op_hash - "unchanged") {
                    oload::omqmap_reload.steps{stepid} = True;
                }

                step_config_container.insertOrUpdateConfigItemValues(yaml_config_items);
            } else {
                step_config_container.deleteConfigItems(oload.o.redef);
            }
        }
    }

    private createProgramImpl() {
    }

    private cacheSteps() {
        # load workflow step data
        hash<auto> need_steps = {};

        # get step dependencies for workflow
        *hash<auto> sq = sqlif.omqp.select("select stepid, dependson_stepid from workflow_steps where workflowid = %v",
            int(workflowid));

        # see which steps we need to retrieve from the DB
        foreach softstring step in (sq.stepid) {
            if (!exists steps{step}) {
                need_steps{step} = True;
            }
        }
        foreach softstring step in (sq.dependson_stepid) {
            if (!exists steps{step}) {
                need_steps{step} = True;
            }
        }

        if (need_steps) {
            loadStepLibrary(keys need_steps);
            loadSteps(keys need_steps);
        }
    }

    private loadStepLibrary(list stepids) {
        map loadValidatorLibrary(sqlif.omqp.select("select * from step_lib where stepid = %v order by load_order",
                                 int($1)), pgm, \fh),
            stepids;
    }

    private loadLibrary() {
        loadValidatorLibrary(sqlif.omqp.select("select * from workflow_lib where workflowid = %v order by load_order",
            int(workflowid)), pgm, \fh);
    }

    private string loadFunctionIntern(hash fq, softint id) {
        if (!regex(fq.body, fq.name + "[[:blank:]]*\\("))
            throw "MISSING-DECLARATION", sprintf("function %s/%s (%d) does not declare function %s()", fq.name,
                fq.version, id, fq.name);

        # bug 1180: when loading a full release, check step function presence as well
        if (oload.o.full) {
            if (!exists oload.omap.function.(fq.name))
                throw "MISSING-FUNCTION", sprintf("full release does not contain function %y", fq.name);
            if (!exists oload.omap.function.(fq.name).(fq.version))
                throw "MISSING-FUNCTION", sprintf("full release contains function %y v%s but later version v%s is "
                    "already in the DB and is not contained in this release", fq.name,
                    oload.omap.function.(fq.name).firstKey(), fq.version);
        }

        *hash<auto> th = sqlif.getTags("function_instance", id).sys;
        if (th.source)
            th.source = basename(th.source);

        # issue #2282: ensure functions are only loaded once
        if (!fh{id}) {
            #printf("loading fid: %y\n", id);
            fh{id} = True;
            addWarnings(pgm.parsePending(fq.body, fq.name, oload.warningMask, th.source, th.offset));
        }
        return fq.name;
    }

    # returns function name
    private string loadFunctionId(softint id) {
        *hash fq = sqlif.omqp.selectRow("select name, version, body, function_instanceid from function_instance "
            "where function_instanceid = %v", id);
        if (!fq.name)
            throw "FUNCTION-ERROR", sprintf("can't load function_instanceid %d: no such function", id);

        return loadFunctionIntern(fq, id);
    }

    private loadFunctionIds(softlist funclist) {
        AbstractTable fi = oload::getTable("function_instance");
        hash<auto> q = fi.select({
            "columns": (
                "name", "version", "body", "function_instanceid",
            ),
            "where": {
                "function_instanceid": op_in((map $1.toInt(), funclist)),
            },
        });

        # parse functions
        context (q) sortBy (get_index(funclist, %function_instanceid)) {
            # XXX FIXME: CHECK
            #if (!pgm.existsFunction(%name))
                loadFunctionIntern(%%, %function_instanceid);
        }
    }

    private string loadClassIntern(hash<auto> cq, softint id, softint step_id) {
        # issue #2282: ensure classes are only loaded once
        if (class_id_map{id}) {
            return cq.name;
        }
        class_id_map{id} = True;

        # run for qore & java classes
        if (!regex(cq.body, cq.name)) {
            throw "MISSING-DECLARATION", sprintf("class %s/%s (%d) does not declare class %s()",
                cq.name, cq.version, id, cq.name);
        }

        # bug 1180: when loading a full release, check class presence as well
        if (oload.o.full) {
            if (!exists oload.omap."class".(cq.name)) {
                throw "MISSING-CLASS", sprintf("full release does not contain class %y", cq.name);
            }
            if (!exists oload.omap."class".(cq.name).(cq.version))
                throw "MISSING-CLASS", sprintf("full release contains class %y v%s but later version v%s is already "
                    "in the DB and is not contained in this release", cq.name,
                    oload.omap."class".(cq.name).firstKey(), cq.version);
        }

        *hash<auto> tags = sqlif.getTags("class", id);
        if (tags.sys.source) {
            tags.sys.source = basename(tags.sys.source);
        }

        # load class dependencies
        foreach string required_name in (sqlif.omqp.select("select dependson_class from class_dependencies where "
            "classid = %v", id).dependson_class) {
            *string required_classversion = oload.classrmap{required_name}.lastversion;
            if (!exists required_classversion) {
                throw "MISSING-CLASS", sprintf("class %s v%s (%d) requires unknown class %y",
                    cq.name, cq.version, id, required_name);
            }
            softint required_classid = oload.classrmap{required_name}{required_classversion};
            loadClassIds({required_classid: step_id});
        }

        #printf("loading cid: %y\n", id);
        hash<auto> info = {
            "code": cq.body,
            "classname": cq.name,
            "language_info": cq.language_info,
            "language": cq.language,
        };
        cacheClass(step_id, info, tags);
        return cq.name;
    }

    private loadClassIds(hash<auto> class_step_map) {
        AbstractTable classes = oload::getTable("classes");
        list<int> classlist = map $1.toInt(), keys class_step_map;
        hash<auto> q = classes.select({
            "columns": ("name", "version", "body", "classid", "language", "language_info"),
            "where": {
                "classid": op_in(classlist),
            },
        });

        # parse class
        context (q) sortBy (get_index(classlist, %classid)) {
            loadClassIntern(%%, %classid, class_step_map{%classid});
        }
    }

    # issue #1704: create mapping from classid to step type
    private:internal addToStepClassMap(softstring stepid) {
        hash<auto> step = steps{stepid};
        bool array = step{"arraytype"} == ArraySeries;
        *string base_class = oload::StepClassBaseClasses{step{"steptype"}};
        if (!base_class) {
            throw "INVALID-STEP-TYPE", sprintf("step %s v%s (%d) has unknown step type %y; expecting one of %y",
                step{"name"}, step{"version"}, stepid, step{"steptype"}, keys oload::StepClassBaseClasses);
        }
        step_class_map{stepid} = base_class + (array ? "Array" : "") + "Step";
    }

    private:internal loadStepCode(softstring stepid) {
        hash<auto> step_tags = sqlif.getTags("step", stepid);
        cacheClass(stepid, steps{stepid}, step_tags);
        addToStepClassMap(stepid);
    }

    private loadSteps(list step_list) {
        AbstractTable t_steps = oload::getTable("steps");
        hash<auto> q = t_steps.select(("where": ("stepid": op_in((map $1.toInt(), step_list)))));

        # cache steps
        context (q) {
            # add name -> stepid mapping
            oload.stepmap.%name.%version = %stepid;

            # add all info to steps structure
            foreach string col in (keys q)
                if (col != "stepid") {
                    steps.%stepid{col} = %%{col};
                }
        }

        # printf("DEBUG: loadSteps:steps: %y\n", steps);

        # load all functions and classes not already cached
        try {
            list<softint> funclist();
            # classid -> stepid
            hash<auto> class_step_map = {};
            foreach softstring stepid in (step_list) {
                *hash<auto> code_info;

                if (steps{stepid}.stepfunction_instanceid) {
                    code_info = oload.fmap.(steps{stepid}.stepfunction_instanceid);
                    if (!pgm.existsFunction(code_info.name)) {
                        funclist += steps{stepid}.stepfunction_instanceid;
                    }
                } else if (steps{stepid}.step_classid) {
                    code_info = oload.classmap{steps{stepid}.step_classid};
                    steps{stepid}.classname = code_info.name;
                    steps{stepid}.language = code_info.language;
                    auto classid = steps{stepid}.step_classid;
                    class_step_map{classid} = stepid;

                    # issue #1704: create mapping from classid to step type
                    addToStepClassMap(stepid);
                } else if (steps{stepid}{"code"}) { # YAML steps
                    loadStepCode(stepid);
                }

                if (steps{stepid}.validationfunction_instanceid) {
                    code_info = oload.fmap.(steps{stepid}.validationfunction_instanceid);
                    if (!pgm.existsFunction(code_info.name)) {
                        funclist += steps{stepid}.validationfunction_instanceid;
                    }
                }

                if (steps{stepid}.endfunction_instanceid) {
                    code_info = oload.fmap.(steps{stepid}.endfunction_instanceid);
                    if (!pgm.existsFunction(code_info.name)) {
                        funclist += steps{stepid}.endfunction_instanceid;
                    }
                }

                if (steps{stepid}.arrayfunction_instanceid) {
                    code_info = oload.fmap.(steps{stepid}.arrayfunction_instanceid);
                    if (!pgm.existsFunction(code_info.name)) {
                        funclist += steps{stepid}.arrayfunction_instanceid;
                    }
                }

                if (steps{stepid}.queueid && !exists oload.qrmap.(steps{stepid}.queueid)) {
                    throw "INVALID-QUEUE", sprintf("queueid %d does not exist", steps{stepid}.queueid);
                }

                if (steps{stepid}.workflow_event_typeid && !exists oload.ermap.(steps{stepid}.workflow_event_typeid)) {
                    throw "INVALID-EVENT-TYPE", sprintf("event type id %d does not exist",
                        steps{stepid}.workflow_event_typeid);
                }
            }
            if (funclist) {
                loadFunctionIds(funclist);
            }
            if (class_step_map) {
                loadClassIds(class_step_map);
            }
        } catch (hash<ExceptionInfo> ex) {
            delete self;
            rethrow;
        }
    }
}

class OMQ::WorkflowValidator inherits public OMQ::WorkflowInterfaceContainerBase {
    private {
        *string errorfunctionname;
        *hash errors;
    }

    constructor(hash h) : WorkflowInterfaceContainerBase(h.id) {
        if (!oload.o.quiet && oload.o.verbose) {
            printf("validating workflow %s/%s (%d): ", h.name, h.version, h.id);
            flush();
        }

        init();

        if (exists warn)
            printf("%s\n", getWarningString(warn));

        #printf("self=%N\n", self);
        #exit(1);

        if (!oload.o.quiet) {
            if (oload.o.verbose) {
                printf("workflow %s/%s (%d): validation OK\n", h.name, h.version, h.id);
            } else {
                oload.mystat("V");
            }
        }
    }

    createProgramImpl() {
        # load and check error function if defined
        if (errorfunction_instanceid) {
            # setup workflow data
            hash fi = oload.fmap{errorfunction_instanceid};

            loadErrorFunction(fi.name, errorfunction_instanceid);
            errorfunctionname = fi.name;

            parseCommit();

            # bug 448: ignore runtime exceptions in the errorfunction during workflow validation as they may be
            # related to missing runtime features
            try {
                errors = pgm.callFunction(errorfunctionname);
            } catch (hash<ExceptionInfo> ex) {
                oload::warning("skipping error evaluation: errorfunction %y has thrown a runtime exception: %s: %s",
                    errorfunctionname, ex.err, ex.desc);
            }

            # check for errors with severity minor or less and an error code
            foreach string key in (keys errors) {
                if (errors{key}.typeCode() != NT_HASH)
                    throw "INVALID-ERROR-HASH", sprintf("error %y has a value of type '%s', expecting 'hash' "
                        "(value: %y)", key, type(errors{key}), errors{key});

                hash err = errors{key};

                if (exists err.severity) {
                    if (err.severity.typeCode() != NT_STRING)
                        throw "INVALID-ERROR-SEVERITY", sprintf("error %y has a 'severity' value of type '%s', "
                            "expecting 'string' (value: %y)", key, type(err.severity), err.severity);
                    if (OMQ::ErrorSeverityOrder.(err.severity) < 3 && exists err.status)
                        oload::warning("error %y has severity %y, so status %y will be ignored", key, err.severity,
                            err.status);
                }
                if (exists err.status) {
                    if (err.status.typeCode() != NT_STRING)
                        throw "INVALID-ERROR-STATUS", sprintf("error %y has a 'status' value of type '%s', expecting "
                            "'string' (value: %y)", key, type(err.status), err.status);
                    if (!inlist(err.status, (OMQ::StatError, OMQ::StatRetry, OMQ::StatCanceled))) {
                        oload::warning("error %y has invalid status %y; will be treated as %y", key, err.status,
                            OMQ::StatError);
                    }
                }
            }
        } else {
            parseCommit();
        }
    }
}

# ServiceValidator class

# dummy global vars to import in service program objects
our (hash services, hash SM);

class OMQ::ServiceContainer inherits OMQ::InterfaceConfigContainer {
    private {
        string type;
        string name;
        string version;
        softint id;

        string language = "qore";
        string class_name;
        bool class_based = False;
        bool java_from_qore = False;

        # hash of library classes; name -> True
        *hash<string, bool> class_lib;

        # hash of class dependencies of library classes; name -> id
        hash<string, int> class_deps;

        # hash of service modules
        *hash<string, bool> service_module_hash;

        # hash of service methods
        hash<string, bool> service_methods();

        # service class for class-based services
        Reflection::Class cls;

        # list of parent classes for class-based services, not including the QorusService class
        list<Reflection::Class> parent_classes();

        # the service's Program container
        ServiceProgram pgm;

        static string systemFakeServiceLib;

        const GlobalSysVars = "our Datasource $omq;
our DatasourcePool $omqp;
our hash<auto> $services;
our hash<auto> $SM;
our object $Qorus;
our object $sqlif;
our hash<auto> $api;";

        const GlobalVars = "our Datasource $omquser;\nour OMQ::QorusRemoteServiceHelper $omqservice;\n"
            "our hash<auto> $sysinfo;\n";

        const ServiceMethodsToIgnore = {
            "constructor": True,
            "destructor": True,
            "copy": True,
            "methodGate": True,
            "memberGate": True,
        };
    }

    constructor(hash<auto> info) : InterfaceConfigContainer("service", info.serviceid) {
        self += info{"type", "name", "version"};
        id = info.serviceid;
        setupProgram(type =~ /system/i, info);
    }

    constructor(string type, string name, string version, softint id) :
        InterfaceConfigContainer("service", id) {
        self.type = type;
        self.name = name;
        self.version = version;
        self.id = id;
    }

    load() {
        logStart();
        *hash<auto> q = sqlif.omqp.selectRow("select serviceid, service_type, name, version, description, class_name, "
            "service_modules, class_source, language, language_info, parse_options, yaml_config_items from services "
            "where serviceid = %v", id);
        if (!q.name) {
            error("%s service %s/%s (%d) does not exist", type, name, version, id);
        }

        if (q.class_source) {
            class_name = q.class_name ?? q.name;
        }
        language = q.language;

        hash<auto> svc_hash = q;

        string dname = sprintf("%s/%s", name, version);

        setupProgram(type =~ /system/i, svc_hash);

        q = sqlif.omqp.select("select * from service_lib where serviceid = %v order by load_order", id);
        class_lib = map {$1.name: True}, q.contextIterator(), $1.type == OT_CLASS;
        loadValidatorLibrary(q, pgm, NOTHING, \class_deps);

        # load service methods
        q = sqlif.omqp.select("select * from service_methods where serviceid = %v", id);
        # issue #3247: do not throw an error if there are no service methods here; the service may inherit methods;
        # this will be checked in service validation

        bool hasstart;
        bool hasstop;

        # make sure and parse init first, start second
        context (q) sortBy (%name == "init" ? 0 : %name == "start" ? 1 : 2) {
            service_methods{%name} = True;
            # parse in method source for function-based services
            if (!svc_hash.class_source) {
                *hash<auto> tags = sqlif.getTags("service_method", %service_methodid);
                addWarnings(pgm.parsePending(%body, sprintf("%s:%s", dname, %name), oload.warningMask, tags.sys.source,
                    tags.sys.offset));
            }
            if (%name == "start") {
                hasstart = True;
            } else if (%name == "stop") {
                hasstop = True;
            }
        }

        # make sure service is consistent
        if (hasstart && !hasstop) {
            error("start method defined but not stop method");
        }

        # load any service modules
        if (svc_hash.service_modules) {
            list<string> service_module_list = map trim($1), svc_hash.service_modules.split(",");
            foreach string mod in (service_module_list) {
                service_module_hash{mod} = True;
                try {
                    string feature_name = qorus_load_service_module(mod);
                    pgm.loadModule(feature_name);
                } catch (hash<ExceptionInfo> ex) {
                    if (oload.o.verbose) {
                        printf("%s\n", get_exception_string(ex));
                    }
                    error("cannot setup Program for service validation; cannot load module %y specified "
                        "by the \"service-modules\" option; correct or remove this option and try again; error: "
                        "%s: %s: %s", mod, get_ex_pos(ex), ex.err, ex.desc);
                }
            }
        }

        if (svc_hash.class_source) {
            class_based = True;
            loadClass(svc_hash, dname, q);
        } else {
            addWarnings(pgm.parseCommit(oload.warningMask));
            deleteConfigItems(oload.o.redef);
        }

        checkClassConnectorsAndProcessor(pgm);

        if (warn.val()) {
            printf("%s\n", getWarningString(warn));
        }

        logEnd();
    }

    loadClass(hash<auto> svc_hash, string dname, hash<auto> q) {
        *hash<auto> tags = sqlif.getTags("service", svc_hash.serviceid);
        if (svc_hash.language == "qore") {
            addWarnings(pgm.parsePending(svc_hash.class_source, dname, oload.warningMask, tags.sys.source,
                tags.sys.offset));
        } else if (svc_hash.language == "java") {
            pgm.cacheJavaClass(class_name, svc_hash.language_info, True, tags.classpath);
        } else if (svc_hash.language == "python") {
            pgm.cachePythonClass(class_name, svc_hash.class_source, svc_hash.language_info, True, tags.module_path);
        } else {
            throw "UNSUPPORTED-LANGUAGE", sprintf("language %y is not supported; expecting qore, java, or python",
                svc_hash.language);
        }

        addWarnings(pgm.parseCommit(oload.warningMask));

        Reflection::Class base_class;
        if (svc_hash.language == "qore") {
            base_class = svc_hash.service_type == "SYSTEM"
                ? pgm.getClass("OMQ::UserApi::Service::QorusSystemService")
                : pgm.getClass("OMQ::UserApi::Service::QorusService");
        } else if (svc_hash.language == "java") {
            base_class = svc_hash.service_type == "SYSTEM"
                ? pgm.getClass("OMQ::UserApi::Service::QorusSystemService")
                : pgm.getClass("OMQ::UserApi::Service::QorusService");
            java_from_qore = True;
        } else if (svc_hash.language == "python") {
            base_class = pgm.getClass("OMQ::UserApi::Service::QorusService");
        }

        try {
            cls = pgm.getClass(class_name);
            int access = cls.getInheritanceAccess(base_class);
            if (access != AC_PUBLIC) {
                error("%s %s service %s v%s (%d) class %y does not inherit %y; inheritance list: %y",
                    svc_hash.language, type, name, version, id,
                    class_name, base_class.getPathName(),
                    (map $1.getPathName(), cls.getClassHierarchy()));
            }

            # make a hash of public method names with a recursive scan of the hierarchy down to the base service class
            code get_methods = hash<string, AbstractMethod> sub (Reflection::Class c) {
                if (!c.isEqual(cls)) {
                    parent_classes += c;
                }
                hash<string, AbstractMethod> methods();
                foreach AbstractMethod m in (c.getMethods()) {
                    # ignore methods that should be ignored
                    if (ServiceMethodsToIgnore{m.getName()}) {
                        continue;
                    }
                    foreach AbstractMethodVariant v in (m.getVariants()) {
                        if (v.getModifiers() & AC_PUBLIC) {
                            # check for python private methods (__*)
                            if (svc_hash.language == "python" && m.getName() =~ /^__/) {
                                continue;
                            }
                            string name = m.getName();
                            if (name == "_copy") {
                                name = "copy";
                            }
                            methods{name} = m;
                            break;
                        }
                    }
                }
                # check publicly-inherited parent classes for public methods (if not the service base class)
                foreach hash<ClassAccessInfo> parent in (c.getParentClasses()) {
                    if (parent.access == AC_PUBLIC && !parent.cls.isEqual(base_class)) {
                        methods += get_methods(parent.cls);
                    }
                }

                return methods;
            };

            hash<string, AbstractMethod> methods = get_methods(cls);

            {
                # verify that all declared service methods are present in the class
                *list<string> missing_methods = map $1, q.name, !methods{$1};
                if (missing_methods) {
                    error("class-based %s service %s v%s (%d) does not implement "
                        "the following declared service methods as public class methods: %y ",
                        type, name, version, id, missing_methods);
                }
            }

            if (svc_hash.language != "python") {
                # process public class methods for Qore and Java services
                *hash<string, bool> service_method_hash = map {$1: True}, q.name;
                *list<string> missing_methods = map $1, keys methods, !service_method_hash{$1};
                processPublicClassMethods(methods{missing_methods});
            }

            if (methods{"start"} && !methods{"stop"}) {
                error("service class %y has a start() method but no stop() "
                    "method; a stop() method must be defined if the service has a start() method",
                    cls.getPathName());
            }

            # load YAML config items
            *list yaml_config_items = addConfigItemsFromYaml(svc_hash{"yaml_config_items"},
                oload.config_item_value_map.service{id});

            bool exists_config_items_yaml = yaml_config_items.toBool();
            bool local_config_items_processed = exists_config_items_yaml;

            # get configuration data from code if there are no config items defined in the YAML
            if (svc_hash.language == "qore" && !exists_config_items_yaml) {
                # only create the object if it really declares config items
                Reflection::Class config_base_class =
                    pgm.getClass("OMQ::UserApi::QorusConfigurationItemProvider");
                AbstractMethod m = cls.findMethod("getConfigItemsImpl").method;
                object obj;

                if (!m.getClass().isEqual(config_base_class)) {
                    # create the object to call the function
                    obj = cls.newObject();
                    *hash<string, hash<ConfigItemInfo>> config_items = obj.getConfigItems();
                    if (config_items) {
                        map addConfigItem($1.key, $1.value, oload.config_item_value_map.service{id}),
                            config_items.pairIterator();
                        local_config_items_processed = True;
                    }
                }
            } else if (svc_hash.language == "java" && !exists_config_items_yaml) {
                # get configuration
                object obj = cls.newObject();
                *hash<string, object> config_items = obj.getConfigItems();
                if (config_items) {
                    # make a configuration list from the ConfigItem objects
                    foreach auto iterator in (config_items.pairIterator()) {
                        hash<ConfigItemInfo> config_item = cast<hash<ConfigItemInfo>>({
                            "description": iterator.value.description,
                            "type": iterator.value.type,
                            "strictly_local": iterator.value.strictly_local,
                            "config_group": iterator.value.config_group,
                            "sensitive": iterator.value.sensitive,
                            "prefix": iterator.value.prefix,
                        });
                        config_item += iterator.value.is_default_value_set ?
                            {"default_value": iterator.value.default_value} : {};
                        config_item += iterator.value.has_allowed_values ?
                            {"allowed_values": iterator.value.allowed_values}: {};
                        addConfigItem(iterator.key, config_item, oload.config_item_value_map.service{id});
                    }
                    local_config_items_processed = True;
                }
            }

            if (local_config_items_processed) {
                if (!config_output && oload.o.verbose) {
                    print("creating config items...\n");
                    config_output = True;
                }
                hash<string, int> op_hash = insertUpdateConfigItems(oload.o.redef);
                if (oload.o.verbose) {
                    printf("%s service %s:%s (%d) configuration items: %y\n", type, name, version, id, op_hash);
                }
                insertOrUpdateConfigItemValues(yaml_config_items);
            } else {
                deleteConfigItems(oload.o.redef);
            }
        } catch (hash<ExceptionInfo> ex) {
            if (ex.err == "UNKNOWN-CLASS") {
                error("class-based %s service %s v%s (%d) does not implement a class named %y",
                    type, name, version, id, class_name);
            }
            rethrow;
        }
    }

    private processPublicClassMethods(hash<string, AbstractMethod> methods) {
        # this method intentionally left blank
    }

    Reflection::Class getClass() {
        return cls;
    }

    Program getProgram() {
        return pgm;
    }

    private setupProgram(bool sys, hash<auto> svc_hash) {
        # process parse options
        *int po;
        if (svc_hash.parse_options) {
            list<string> l = svc_hash.parse_options.split(",");
            trim l;
            foreach string opt in (l) {
                *int c = ParseOptionStringMap{opt};
                if (!c) {
                    error("cannot process unknown parse option %y", opt);
                }
                if (!(c & ServiceParseOptionMask)) {
                    error("service configuration references illegal parse option %y; valid parse options: %y", opt,
                        (map ParseOptionCodeMap.$1, ServiceParseOptionList));
                }
                po |= c;
            }
        }

        pgm = new ServiceProgram(sys, po | PO_NO_USER_API, oload.options);

        # export system objects to service program (read-only)
        if (sys) {
            # load in system service classes
            map pgm.importClass($1), SystemServiceClassList;

            string gv = GlobalSysVars;
            if (po & PO_ALLOW_BARE_REFS) {
                gv =~ s/\$//g;
            }

            pgm.parsePending(gv, "<gv>");
            our Tables tables;
            pgm.importGlobalVariable("tables", True);
        }

        string gv = GlobalVars;
        if (po & PO_ALLOW_BARE_REFS) {
            gv =~ s/\$//g;
        }

        pgm.parsePending(gv, "<gv>");

        # load fake service API
        qorus_load_fake_service_api_module(pgm);
    }

    private logStart() {
    }

    private logEnd() {
    }

    private error(string fmt) {
        throw "SERVICE-ERROR", vsprintf(fmt, argv);
    }
}

class OMQ::ServiceLoader inherits OMQ::ServiceContainer {
    private {
        QorusObjectParser::Parser parser();

        hash userTags;
        *string author;
    }

    constructor(string type, string name, string version, softint id, hash user_tags, *string author) :
            ServiceContainer(type, name, version, id) {
        parser.setEncoding(oload.encoding);
        userTags = user_tags;
        self.author = author;
        load();
    }

    private setupProgram(bool sys, hash<auto> svc_hash) {
        ServiceContainer::setupProgram(sys, svc_hash);
    }

    private:internal int insertMethods(list<QorusObjectParser::QorusObject> methods) {
        ServiceDbInserter inserter();
        int cnt = 0;
        foreach auto method in (methods) {
            if (!inserter.doesMethodExist(method.getTag("name"), id)) {
                inserter.checkMethod(method.getTags(), True);
                inserter.insertMethod(type, name, version, id, method.getTags(), userTags, True, author);
                ++cnt;
            }
        }
        return cnt;
    }

    private processPublicClassMethods(hash<string, AbstractMethod> methods) {
        # do not process Python service methods
        if (language == "python") {
            return;
        }

        # issue #3675 do not scan for declared methods anymore
        # scan sources for declared methods
        int methods_created = 0;
        foreach AbstractMethod m in (methods.iterator()) {
            if (language == "qore") {
                Reflection::Class mcls = m.getClass();
                string cname = mcls.getName();
                if (cname == class_name) {
                    # issue #3675: ignore undeclared public methods
                    continue;
                }
                if (class_lib{cname} || exists class_deps{cname}) {
                    # load source and scan for service methods
                    AbstractTable classes = oload::getTable("classes");

                    # printf("DEBUG: class_deps: %y\n", class_deps);
                    int classid = lib_map{OT_CLASS}{cname} ?? class_deps{cname};
                    *string src = classes.selectRow({"columns": "body", "where": {"classid": classid}}).body;
                    if (!src) {
                        # issue #3675: ignore methods without accessible source
                        continue;
                    }

                    list<QorusObjectParser::QorusObject> method_objects = parser.parseServiceMethods(cname,
                        new InputStreamLineIterator(new StringInputStream(src)), True, language);
                    methods_created += insertMethods(method_objects);

                    continue;
                }
                # see if class was provided by a service module
                if ((*string mod = mcls.getModuleName()) && service_module_hash{mod}) {
                    list<QorusObjectParser::QorusObject> method_objects = parser.parseServiceMethods(cname,
                        new FileLineIterator(get_module_hash(){mod}.filename), True, language);
                    methods_created += insertMethods(method_objects);

                    continue;
                }
            } else if (language == "java") {
                # issue #3675: ignore unannotated public methods
                continue;
            }
        }
        # ensure that the service has at least one method
        if (!service_methods && !methods_created) {
            throw "SERVICE-VALIDATION-ERROR", sprintf("%s service %s v%s (%d) has no methods defined", type, name,
                version, id);
        }
    }

    private cannotFindSourceError(string cname, *int classid) {
        string desc = sprintf("%y", cname);
        if (classid) {
            desc += sprintf(" (%d)", classid);
        }
        error("class-based %s service %s v%s (%d) cannot be loaded as the source for parent class %s cannot be found",
            type, name, version, id, desc);
    }
}

class OMQ::ServiceValidator inherits OMQ::ServiceContainer {
    constructor(hash<auto> h) : ServiceContainer(h.type, h.name, h.version, h.id) {
        load();
    }

    private logStart() {
        if (!oload.o.quiet && oload.o.verbose) {
            printf("validating %s service %s/%s (%d): ", type, name, version, id);
            flush();
        }
    }

    private logEnd() {
        if (!oload.o.quiet && oload.o.verbose) {
            if (config_output) {
                printf("%s service %s/%s (%d): validation ", type, name, version, id);
            }
            printf("OK\n");
        }
    }

    private processPublicClassMethods(hash<string, AbstractMethod> methods) {
        # this method intentionally left blank
    }
}

class OMQ::JobContainer inherits OMQ::InterfaceConfigContainer, OMQ::JobDef {
    private {
        # Program object to hold job logic
        JobProgram pgm(oload.options, NOTHING, PO_NO_USER_API);

        # job tags
        *hash<auto> job_tags;
    }

    constructor(hash<auto> info) : InterfaceConfigContainer("job", info.jobid) {
        self += info{"name", "version", "jobid"};

        /*
        hash<auto> q = {
            "class_name": info."class-name",
            "language": info.lang,
        };
        */
        createProgram();
    }

    constructor(string name, string version, softint id) :
        InterfaceConfigContainer("job", id) {
        self.name = name;
        self.version = version;
        self.jobid = id;

        logStart();
        init();
        logEnd();
    }

    init() {
        *hash<auto> q = sqlif.omqp.selectRow("select * from jobs where jobid = %v", jobid);

        # check if job exists
        if (!q) {
            throw "NO-SUCH-JOB", name;
        }

        # get job tags
        job_tags = sqlif.getTags("job", q.jobid);

        # map fields to job hash
        map self.$1 = (q.$1 === NULL ? NOTHING : q.$1), keys q;

        # create program and import fake library functions for job program
        createProgram();

        # load any job modules
        if (q.job_modules) {
            list<string> job_module_list = map trim($1), q.job_modules.split(",");
            foreach string mod in (job_module_list) {
                try {
                    string feature_name = qorus_load_job_module(mod);
                    pgm.loadModule(feature_name);
                } catch (hash<ExceptionInfo> ex) {
                    throw "JOB-ERROR", sprintf("cannot setup Program for job validation; cannot load module %y "
                        "specified by the \"job-modules\" option; correct or remove this option and try again; "
                        "error: %s: %s: %s", mod, get_ex_pos(ex), ex.err, ex.desc);
                }
            }
        }

        # parse job code
        if (q.language == "qore") {
            addWarnings(pgm.parsePending(q."code", "job code", oload.warningMask, job_tags.sys.source,
                job_tags.sys.offset));
        } else if (q.language == "java") {
            pgm.cacheJavaClass(q.class_name ?? name, q.language_info, True, job_tags.classpath);
        } else if (q.language == "python") {
            pgm.cachePythonClass(class_name ?? name, q."code", q.language_info, True, job_tags.module_path);
        } else {
            throw "UNSUPPORTED-LANGUAGE", sprintf("language %y is not supported; expecting qore, java, or python",
                q.language);
        }

        # commit parsing
        parseCommit();

        checkClassConnectorsAndProcessor(pgm);

        if (warn.val()) {
            printf("%s\n", getWarningString(warn));
        }
    }

    parseCommit() {
        pgm.loadModule("reflection");
        pgm.setParseOptions(PO_NEW_STYLE);

        if (class_based) {
            loadClass();
        } else {
            # function-based job
            pgm.parsePending("Reflection::Function sub _oload_qore_get_func(string name) { "
                "return Reflection::Function::forName(name); }", "<qore oload helper>");
            addWarnings(pgm.parseCommit(oload.warningMask));
            try {
                pgm.callFunction("_oload_qore_get_func", "run");
            } catch (hash<ExceptionInfo> ex) {
                if (ex.err == "UNKNOWN-FUNCTION") {
                    throw "JOB-ERROR", sprintf("function-based job %s v%s (%d) does not implement a 'run()' function",
                        name, version, jobid);
                }
                rethrow;
            }

            deleteConfigItems(oload.o.redef);
        }
    }

    Program getProgram() {
        return pgm;
    }

    private loadClass() {
        addWarnings(pgm.parseCommit(oload.warningMask));

        if (!class_name) {
            class_name = name;
        }
        Reflection::Class base_class;
        if (language == "qore") {
            base_class = pgm.getClass("OMQ::UserApi::Job::QorusJob");
        } else if (language == "java") {
            base_class = pgm.getClass("OMQ::UserApi::Job::QorusJob");
        } else if (language == "python") {
            base_class = pgm.getClass("OMQ::UserApi::Job::QorusJob");
        }

        try {
            Reflection::Class cls = pgm.getClass(class_name);
            int access = cls.getInheritanceAccess(base_class);
            if (access != AC_PUBLIC) {
                throw "JOB-ERROR", sprintf("job %s v%s (%d) class %y does not inherit %y; inheritance list: %y",
                    name, version, jobid, class_name, base_class.getPathName(),
                    (map $1.getName(), cls.getClassHierarchy()));
            }

            # load YAML config items
            *list des_yaml_config_items = addConfigItemsFromYaml(yaml_config_items);

            bool exists_config_items_yaml = des_yaml_config_items.toBool();
            bool local_config_items_processed = exists_config_items_yaml;

            # get job configuration data from code if there are no config items defined in the YAML
            if (language == "qore" && !exists_config_items_yaml) {
                # create the object to call the function
                object obj = cls.newObject();
                *hash<string, hash<ConfigItemInfo>> config_items = obj.getConfigItems();
                if (config_items) {
                    map addConfigItem($1.key, $1.value), config_items.pairIterator();
                    local_config_items_processed = True;
                }
            } else if (language == "java" && !exists_config_items_yaml) {
                # get configuration
                object obj = cls.newObject();
                *hash<string, object> config_items = obj.getConfigItems();
                if (config_items) {
                    # make a configuration list from the ConfigItem objects
                    foreach auto iterator in (config_items.pairIterator()) {
                        hash<ConfigItemInfo> config_item = cast<hash<ConfigItemInfo>>({
                            "description": iterator.value.description,
                            "type": iterator.value.type,
                            "strictly_local": iterator.value.strictly_local,
                            "config_group": iterator.value.config_group,
                            "sensitive": iterator.value.sensitive,
                            "prefix": iterator.value.prefix,
                        });
                        config_item += iterator.value.is_default_value_set ?
                            {"default_value": iterator.value.default_value} : {};

                        config_item += iterator.value.has_allowed_values ?
                            {"allowed_values": iterator.value.allowed_values}: {};
                        addConfigItem(iterator.key, config_item);
                    }
                    local_config_items_processed = True;
                }
            }

            if (local_config_items_processed) {
                if (!config_output && oload.o.verbose) {
                    print("creating config items...\n");
                    config_output = True;
                }
                hash<string, int> op_hash = insertUpdateConfigItems(oload.o.redef);
                if (oload.o.verbose) {
                    printf("job %s:%s (%d) configuration items: %y\n", name, version, jobid, op_hash);
                }
                insertOrUpdateConfigItemValues(des_yaml_config_items);
            } else {
                deleteConfigItems(oload.o.redef);
            }
        } catch (hash<ExceptionInfo> ex) {
            if (ex.err == "UNKNOWN-CLASS") {
                throw "JOB-ERROR", sprintf("class-based job %s v%s (%d) does not implement a class named %y",
                    name, version, jobid, class_name);
            }
            rethrow;
        }
    }

    private createProgram() {
        # import fake API
        qorus_load_fake_job_api_module(pgm);

        # export system objects to job program
        pgm.importGlobalVariable("omqservice");

        # load job library
        loadValidatorLibrary(sqlif.omqp.select("select * from job_lib where jobid = %v order by load_order", jobid),
            pgm);
    }

    private logStart() {
    }

    private logEnd() {
    }
}

# JobValidator class
class OMQ::JobValidator inherits public OMQ::JobContainer {
    constructor(hash<auto> h) : JobContainer(h.name, h.version, h.id) {
    }

    private logStart() {
        if (!oload.o.quiet && oload.o.verbose) {
            printf("validating job %s/%s (%d): ", name, version, jobid);
            flush();
        }
    }

    private logEnd() {
        if (!oload.o.quiet && oload.o.verbose) {
            if (config_output) {
                printf("job %s/%s (%d): validation ", name, version, jobid);
            }
            printf("OK\n");
        }
    }
}

class OMQ::MapperValidator inherits public OMQ::InterfaceContainerBase, public OMQ::MapperDef {
    constructor(hash<auto> h) {
        if (!oload.o.quiet && oload.o.verbose) {
            printf("validating mapper %s/%s (%d): ", h.name, h.version, h.id);
            flush();
        }

        mapperid = h.id;
        self += h.("name", "version");

        ClientMapperContainer mc;
        try {
            mc = oload.um.loadMapper(self, h.id);
        } catch (hash<ExceptionInfo> ex) {
            if (oload.options."debug-system") {
                error("%s", Util::get_exception_string(ex));
            } else {
                error("%s: %s", ex.err, ex.desc);
            }
        }

        if (!oload.o.quiet && oload.o.verbose) {
            printf("OK\n");
        }
    }

    error(string fmt) {
        throw "MAPPER-VALIDATION-ERROR", sprintf("MAPPER %s v%s (%d) %s", name, version, mapperid, vsprintf(fmt, argv));
    }
}

class PythonDependencyHelper {
    public {
        #! Keep an index of content; name -> version -> content for working with Java classes
        static hash<string, hash<string, hash<auto>>> content_index;

        #! Map of objects with already loaded Python dependencies: type -> name -> True
        static hash<string, bool> python_dep_map;
    }

    #! Returns True if the class has python dependencies
    static bool hasPythonDependencies(string class_name, string code_, *list<string> requires,
            reference<hash<auto>> dep_info) {
        if (code_ =~ /^import python/m) {
            dep_info = {
                "dynamic": True,
                "dep_name": class_name,
            };
            return True;
        }

        hash<string, bool> processed = {class_name: True};
        foreach string required in (requires) {
            if (JavaClassHelper::hasPythonDependencies(class_name, required, \processed, \dep_info)) {
                return True;
            }
        }

        return False;
    }

    #! Returns True if the pending class in the content index has python dependencies
    static bool hasPythonDependenciesInContentIndex(string class_name, string required,
            reference<hash<string, bool>> processed, reference<hash<auto>> dep_info) {
        hash<auto> info = content_index{required}.lastValue();
        if (info.metainfo.lang == "python") {
            return True;
        }

        if ((*string base_class = info.metainfo."base-class-name") &&
            JavaClassHelper::hasPythonDependencies(class_name, base_class, \processed, \dep_info)) {
            return True;
        }

        foreach string req in (info.metadata.required) {
            if (JavaClassHelper::hasPythonDependencies(class_name, req, \processed, \dep_info)) {
               return True;
            }
        }

        return False;
    }

    #! Returns True if the class has python dependencies
    static bool hasPythonDependencies(string class_name, string required,
            reference<hash<string, bool>> processed, reference<hash<auto>> dep_info) {
        *string reqversion = oload.classrmap{required}.lastversion;
        if (!exists reqversion) {
            # see if we have it in content_index
            if (content_index{required}) {
                return JavaClassHelper::hasPythonDependenciesInContentIndex(class_name, required, \processed, \dep_info);
            } else {
                throw "CLASS-DEPENDENCY-ERROR",
                    sprintf("class %y required by %y is not present in the DB or being currently loaded", required,
                            class_name);
            }
        }

        # check dependencies
        int classid = oload.classrmap{required}{reqversion};
        # load from the DB
        AbstractTable class_dependencies = oload::getTable("class_dependencies");
        hash<auto> sh = {
            "columns": ("dependson_class"),
            "where": {"classid": classid},
        };
        context (class_dependencies.select(sh)) {
            if (!processed{%dependson_class}
                && JavaClassHelper::hasPythonDependencies(class_name, %dependson_class, \processed, \dep_info)) {
                    return True;
            }
        }

        *hash<auto> current_info = content_index{required}{reqversion}.info;
        if (current_info.lang == "python") {
            dep_info.dep_name = required;
            return True;
        }

        AbstractTable classes = oload::getTable("classes");
        sh = {
            "columns": ("body", "language"),
            "where": {"classid": classid},
        };
        hash<auto> row = classes.selectRow(sh);
        if (row.language == "python") {
            dep_info.dep_name = required;
            return True;
        }
        if ((row.language == "qore" && row.body =~ /^%module-cmd(python)/)
            || (row.language == "java" && row.body =~ /^import python/)) {
            dep_info = {
                "dynamic": True,
                "dep_name": required,
            };
            return True;
        }

        return False;
    }
}

class DbInserter inherits PythonDependencyHelper {
    public {
    }

    private {
        #! True if the object has any python dependencies
        bool has_python = False;
    }

    *hash<string, hash<auto>> makeHash(*list<auto> objects) {
        return map {$1.name: $1}, objects;
    }

    insertObjects(string object_type, *list<auto> objects, *string name) {
        if (!objects) {
            return;
        }
        if (!oload.o.verbose && !oload.o.quiet) {
            string label = name ?? object_type;
            printf("processing %s%ss: ", label, label[-1] == "s" ? "e" : "");
        }

        hash<string, hash<auto>> ohash = makeHash(objects);

        while (ohash) {
            hash<auto> obj = remove ohash{ohash.firstKey()};

            *ReleaseFile release_file = oload.addRelease(object_type, obj{"yaml_source"}{"file"},
                obj{"yaml_source"}{"release"});

            code post_insert = sub (hash<auto> tags, softstring id, string file_name) {
                if (tags.groups) {
                    hash<auto> object_with_groups;
                    object_with_groups{id} = {
                        "type": object_type,
                        "name": tags.name,
                        "version": tags.version,
                        "groups": tags.groups,
                    };

                    oload.processGroups(object_with_groups, tags.yaml_source.file, !oload.o.verbose);
                }
            };

            insert(\ohash, obj, obj{"tags"}, obj{"yaml_source"}{"file"}, post_insert, release_file);
        }

        if (!oload.o.verbose && !oload.o.quiet) {
            print(" OK\n");
        }
    }

    insert(reference<hash<string, hash<auto>>> ohash, hash<auto> tags, hash<auto> user_tags,
            string file_name, code post_insert, *ReleaseFile release_file) {
        post_insert(tags, insertImpl(\ohash, tags, user_tags, file_name, post_insert, release_file), file_name);
    }

    abstract softstring insertImpl(reference<hash<string, hash<auto>>> ohash, hash<auto> tags, hash<auto> user_tags,
            string file_name, code post_insert, *ReleaseFile release_file);

    private static insertLibraries(string object_type, int id, hash<auto> tags) {
        # delete all library objects
        sqlif.omqp.exec("delete from %s_lib where %sid = %v", object_type, object_type, id);

        # mappers and vmaps are stored in separate tables
        hash<auto> library_types = QorusObjectParser::TagToLibraryType - ("mappers", "vmaps");
        foreach string tag in (keys library_types) {
            DbInserter::insertLibraries(object_type, id, tags{tag}, QorusObjectParser::TagToLibraryType{tag});
        }
    }

    private static insertLibraries(string object_type, int id, *list<auto> libraries, string library_type) {
        # save library objects
        int load_order = 0;
        map sqlif.omqp.exec("insert into %s_lib values (%v, %v, %v, %v)", object_type, id, library_type, $1,
                            load_order++),
            libraries;
    }

    private static *hash getLibraryObjectInfo(string table_name, string name) {
        AbstractTable table = oload::getTable(table_name);

        # separate name and version
        *string name_ = name;
        *string version = NOTHING;
        if (table_name == "mappers") {
            string reg = "(^[^:]+):(.+)$";
            (name_, version) = name.regexExtract(reg);
            if (!name_ || !version) {
                oload::error("mapper %s is not in the correct format: %s", name, reg);
            }
        }

        hash<auto> select_hash = {
            "where": {
                "name": name_,
            },
            "limit": 1,
        };

        if (exists version) {
            select_hash."where" += {"version": version};
        }

        return table.selectRow(select_hash);
    }

    static *hash checkLibraryObject(string table_name, string object_type, string name, *string dependent_object_type,
                                    *string dependent_object_name) {
        *hash object_info = DbInserter::getLibraryObjectInfo(table_name, name);
        if (!object_info) {
            oload::error("unknown %s: %s%s", object_type, name,
                dependent_object_type ? sprintf(" when loading %s: %s", dependent_object_type, dependent_object_name)
                                      : "");
        }
        return object_info;
    }

    static *string getFsmTriggers(reference<hash<auto>> tags) {
        hash<auto> info;
        foreach auto v in (\tags.fsm) {
            info{v.name} += v.triggers;
            v = v.name;
        }
        if (!info) {
            return;
        }
        return make_yaml(info);
    }
}

class PipelineProcessorConfigContainer inherits OMQ::InterfaceConfigContainer {
    private {
    }

    constructor(string pipeline, *hash<auto> processor_config_items) : InterfaceConfigContainer("pipeline", pipeline, NOTHING, False) {
        *list<hash<auto>> config_items;
        foreach hash<auto> i in (processor_config_items.pairIterator()) {
            foreach hash<auto> config_item in (i.value) {
                config_item.prefix = i.key + ":";
                config_items += config_item;

                hash<auto> parents;
                addConfigItem(config_item{"name"}, prepareYamlConfigItem(config_item{"name"},
                            config_item, \parents), oload.config_item_value_map.pipeline{pipeline});
            }
        }

        insertUpdateConfigItems(oload.o.redef);

        #printf("PCI: %N\n", processor_config_items);
        insertOrUpdateConfigItemValues(config_items);
    }
}

class PipelineDbInserter inherits DbInserter {
    public {
        const PipelineInputProviderTags = ("input-provider", "input-provider-options");
    }

    # inserts or updates the pipeline in the DB
    softstring insertImpl(reference<hash<string, hash<auto>>> ohash, hash<auto> tags, hash<auto> user_tags,
            string file_name, code post_insert, *ReleaseFile release_file) {
        if (!oload.o.quiet && oload.o.verbose) {
            printf("%y: ", file_name);
        }
        if (!tags.children) {
            throw "PIPELINE-ERROR", sprintf("%s: pipline %y has no elements", file_name, tags.name);
        }
        checkChildren(file_name, tags.name, tags.children);

        AbstractTable pipelines_table = oload::getTable("pipelines");
        AbstractTable pipeline_lib_table = oload::getTable("pipeline_lib");

        # remove config items from processors and save separately
        *hash<auto> processor_config_info = extractProcessorConfigItems(\tags.children);

        # one commit / rollback per datasource
        on_success pipelines_table.commit();
        on_error pipelines_table.rollback();

        hash<auto> opts;
        if (tags."input-provider") {
            opts = tags{PipelineInputProviderTags};
            # process options
            foreach hash<auto> i in (opts."input-provider-options".pairIterator()) {
                oload::parseOptionValue(\opts."input-provider-options"{i.key});
            }
        }
        hash<auto> info = {
            "children": serialize_qorus_data(tags.children),
            "description": tags.desc,
            "options": serialize_qorus_data(opts),
        };
        hash name_hash = {"name": tags.name};

        # update the pipeline if it exists already
        *string name = pipelines_table.selectRow({"columns": "name", "where": name_hash}).name;
        if (name) {
            if (!oload.o.quiet) {
                if (oload.o.verbose) {
                    printf("pipeline %y updating\n", name);
                } else {
                    oload.mystat(".");
                }
            }

            pipelines_table.update(info, name_hash);
        } else {
            if (!oload.o.quiet) {
                if (oload.o.verbose) {
                    printf("pipeline %y inserting\n", tags.name);
                } else {
                    oload.mystat("I");
                }
            }

            pipelines_table.insert(name_hash + info);
        }

        # insert libraries
        hash<auto> classes;
        hash<auto> mappers;
        checkLibraries(tags, tags.name, \classes, \mappers);

        pipeline_lib_table.del({"pipeline": tags.name});

        int load_order = 0;
        map pipeline_lib_table.insert({
                "pipeline": tags.name,
                "type": OT_CLASS,
                "name": $1,
                "load_order": load_order++,
            }),
            keys classes;

        load_order = 0;
        map pipeline_lib_table.insert({
                "pipeline": tags.name,
                "type": OT_MAPPER,
                "name": $1,
                "load_order": load_order++,
            }),
            keys mappers;

        # process config items
        PipelineProcessorConfigContainer sc(tags.name, processor_config_info);

        oload::omqmap_reload.pipelines{tags.name} = True;
        oload.val_pipeline_map{tags.name} = True;
        return tags.name;
    }

    # verify pipeline children
    private checkChildren(string file_name, string name, *list<auto> children) {
        if (children.size() > 1) {
            # ensure all children are queues
            hash<string, bool> eth = map {$1.type: True}, children;
            if (!eth.size() < 2 || (eth.size() == 1 && eth.firstKey() == "queue")) {
                return;
            }
            throw "PIPELINE-ERROR", sprintf("%s: pipeline %y has invalid children: %y; an element that has multiple "
                "children can only have \"queue\" as children", file_name, name, keys eth);
        }
        map checkChildren(file_name, name, $1.children), children;
    }

    # recursively extract all processor config items and return the hash of config items keyed by processor ID
    private *hash<auto> extractProcessorConfigItems(reference<auto> children) {
        *hash<auto> rv;
        foreach hash<auto> entry in (\children) {
            if (entry.type == "processor" && entry."config-items") {
                rv{entry.pid} = remove entry."config-items";
            }

            if (entry.children) {
                #printf("processing chilren: %y\n", entry.children);
                rv += extractProcessorConfigItems(\entry.children);
            }
        }
        #printf("extractProcessorConfigItems() returning: %y\n", rv);
        return rv;
    }

    private checkLibraries(hash<auto> tags, string pipeline, reference<hash<auto>> classes,
                           reference<hash<auto>> mappers) {
        foreach hash<auto> child in (tags.children) {
            if (child.type === "processor" && !classes.hasKey(child.name)) {
                *hash object_info = DbInserter::checkLibraryObject("classes", "class", child.name, "pipeline",
                                                                   pipeline);
                if (!exists UserApi::deserializeQorusData(object_info.processor)) {
                    oload::error("class %s is not a processor and hence cannot be used in the pipeline: %s",
                                 child.name, pipeline);
                }
                classes{child.name} = True;
            } else if (child.type === "mapper" && !mappers.hasKey(child.name)) {
                DbInserter::checkLibraryObject("mappers", "mapper", child.name, "pipeline", pipeline);
                mappers{child.name} = True;
            }
            checkLibraries(child, pipeline, \classes, \mappers);
        }
    }
}

class FsmStateConfigContainer inherits OMQ::InterfaceConfigContainer {
    private {
    }

    constructor(string fsm, *hash<auto> state_config_items) : InterfaceConfigContainer("fsm", fsm, NOTHING, False) {
        *list<hash<auto>> config_items;
        foreach hash<auto> i in (state_config_items.pairIterator()) {
            foreach hash<auto> config_item in (i.value) {
                config_item.prefix = i.key + ":";
                config_items += config_item;

                hash<auto> parents;
                addConfigItem(config_item{"name"}, prepareYamlConfigItem(config_item{"name"},
                            config_item, \parents), oload.config_item_value_map.fsm{fsm});
            }
        }

        insertUpdateConfigItems(oload.o.redef);

        #printf("SCI: %N\n", state_config_items);
        insertOrUpdateConfigItemValues(config_items);
    }
}

class FsmDbInserter inherits DbInserter {
    #! FSM dependencies, we want to commit after all FSM are loaded
    static commit(AbstractTable fsm_table) {
        # one commit per datasource
        fsm_table.commit();
    }

    #! FSM dependencies, we want to rollback all inserted FSM in case of error
    static error(AbstractTable fsm_table) {
        # one rollback per datasource
        fsm_table.rollback();
    }

    softstring insertImpl(reference<hash<string, hash<auto>>> ohash, hash<auto> tags, hash<auto> user_tags,
            string file_name, code post_insert, *ReleaseFile release_file) {
        if (!oload.o.quiet && oload.o.verbose) {
            printf("%y: ", file_name);
        }
        AbstractTable fsm_table = oload::getTable("fsm");
        AbstractTable fsm_lib_table = oload::getTable("fsm_lib");

        # remove config items from states and save separately
        hash<auto> state_config_info = processStates(\tags.states);

        hash<auto> fsm_info = {
            "description": tags.desc,
            "states": serialize_qorus_data(tags.states),
            "options": serialize_qorus_data(tags.options),
            "input_type": serialize_qorus_data(tags."input-type"),
            "output_type": serialize_qorus_data(tags."output-type"),
        };
        hash<auto> name_hash = {"name": tags.name};

        # update the FSM if it exists already
        *string name = fsm_table.selectRow({"columns": "name", "where": name_hash}).name;
        if (name) {
            if (!oload.o.quiet) {
                if (oload.o.verbose) {
                    printf("FSM %y updating\n", name);
                } else {
                    oload.mystat(".");
                }
            }

            fsm_table.update(fsm_info, name_hash);
        } else {
            if (!oload.o.quiet) {
                if (oload.o.verbose) {
                    printf("FSM %y inserting\n", tags.name);
                } else {
                    oload.mystat("I");
                }
            }

            fsm_table.insert(name_hash + fsm_info);
        }

        # insert libraries
        hash<auto> classes;
        hash<auto> mappers;
        hash<auto> pipelines;
        hash<auto> fsm;
        checkLibraries(tags, tags.name, tags.states, \classes, \mappers, \pipelines, \fsm);

        fsm_lib_table.del({"fsm": tags.name});

        int load_order = 0;
        map fsm_lib_table.insert({
            "fsm": tags.name,
            "type": OT_CLASS,
            "name": $1,
            "load_order": load_order++,
        }),
            keys classes;

        load_order = 0;
        map fsm_lib_table.insert({
            "fsm": tags.name,
            "type": OT_MAPPER,
            "name": $1,
            "load_order": load_order++,
        }),
            keys mappers;

        load_order = 0;
        map fsm_lib_table.insert({
            "fsm": tags.name,
            "type": OT_PIPELINE,
            "name": $1,
            "load_order": load_order++,
        }),
            keys pipelines;

        load_order = 0;
        map fsm_lib_table.insert({
            "fsm": tags.name,
            "type": OT_FSM,
            "name": $1,
            "load_order": load_order++,
        }),
            keys fsm;

        # process config items
        FsmStateConfigContainer sc(tags.name, state_config_info);

        oload::omqmap_reload.fsm{tags.name} = True;
        oload.val_fsm_map{tags.name} = True;
        return tags.name;
    }

    private hash<auto> processStates(reference<hash<auto>> states) {
        hash<auto> config = {};
        foreach string k in (keys states) {
            if (*list<auto> cl = remove states{k}."config-items") {
                config{states{k}.id} = cl;
            }
            # process block-config options
            foreach hash<auto> i in (states{k}."block-config".pairIterator()) {
                oload::parseOptionValue(\states{k}."block-config"{i.key});
            }
            if (states{k}.states) {
                config += processStates(\states{k}.states);
            }
        }

        return config;
    }

    private checkLibraries(hash<auto> tags, string fsm, *hash<auto> states, reference<hash<auto>> classes, reference<hash<auto>> mappers,
                           reference<hash<auto>> pipelines, reference<hash<auto>> dependent_fsm) {
        # make sure triggers are unique
        hash<auto> trig_map;
        foreach hash<auto> trigger in (tags.triggers) {
            string key = sprintf("%s-%s", trigger.firstKey(), trigger.firstValue());
            if (trig_map{key}) {
                throw "TRIGGER-ERROR", sprintf("multiple triggers %y for Finite State Machine", key);
            }
            if (trigger.hasKey("class")) {
                DbInserter::checkLibraryObject("classes", "class", trigger."class", "fsm", fsm);
                classes{trigger."class"} = True;
            }
        }

        code checkTransitions = sub (*list transitions) {
            foreach hash<auto> transition in (transitions) {
                if (transition.hasKey("condition") && transition.condition.typeCode() == NT_HASH) {
                    DbInserter::checkLibraryObject("classes", "class", transition.condition."class", "fsm", fsm);
                    classes{transition.condition."class"} = True;
                }
            }
        };

        foreach hash<auto> state_iterator in (states.iterator()) {
            if (state_iterator.action) {
                switch (state_iterator.action.type) {
                    case "connector":
                        DbInserter::checkLibraryObject("classes", "class",
                            state_iterator.action.value."class", "fsm", fsm);
                        classes{state_iterator.action.value."class"} = True;
                        break;

                    case "mapper":
                        checkLibraryObject("mappers", "mapper", state_iterator.action.value, "fsm", fsm);
                        mappers{state_iterator.action.value} = True;
                        break;

                    case "pipeline":
                        DbInserter::checkLibraryObject("pipelines", "pipeline", state_iterator.action.value, "fsm",
                            fsm);
                        pipelines{state_iterator.action.value} = True;
                        break;

                    case "apicall":
                    case "search-single":
                    case "search":
                    case "create":
                    case "update":
                    case "delete":
                        break;

                    default:
                        throw "ACTION-ERROR", sprintf("invalid action %y in FSM; expecting one of \"connector\", "
                            "\"mapper\", \"pipeline\", \"apicall\", \"search\", \"search-single\", \"create\", "
                            "\"update\", or \"delete\"",
                            state_iterator.action.type);
                }
            } else if (state_iterator.type == "fsm") {
                dependent_fsm{state_iterator.name} = True;
                # don't check here if FSM exists - it might be given in one command
                # that is checked after all FSM are inserted
                oload.fsm_dependencies{state_iterator.name} = fsm;
            } else if (state_iterator.states) {
                checkLibraries(tags, fsm, state_iterator.states, \classes, \mappers, \pipelines, \dependent_fsm);
            }

            checkTransitions(state_iterator.transitions);
        }
    }
}

class TypeDbInserter inherits DbInserter {
    *hash<string, hash<auto>> makeHash(*list<auto> objects) {
        return map {$1.path: $1}, objects;
    }

    softstring insertImpl(reference<hash<string, hash<auto>>> ohash, hash<auto> tags, hash<auto> user_tags,
            string file_name, code post_insert, *ReleaseFile release_file) {
        if (!oload.o.quiet && oload.o.verbose) {
            printf("%y: ", file_name);
        }
        # normalize the type path before saving
        tags.path = QorusDataProviderTypeHelper::normalizeTypePath(tags.path);
        oload.registerObject("type", tags.path, "-", file_name);

        #printf("t: %y ut: %y\n", tags, user_tags);
        # issue #3740 remote dots in field and path names
        hash<auto> typeinfo = tags.typeinfo;
        removeDots(\typeinfo.fields);

        # get the type object to insert
        binary type_data = QorusDataProviderTypeHelper::getType(typeinfo).serialize();

        # get table object
        AbstractTable types = oload::getTable("data_types");

        # update the type if it exists already
        *string old_path = types.selectRow({"columns": "path", "where": {"path": tags.path}}).path;

        on_success types.commit();
        on_error types.rollback();

        hash<auto> type_data_hash = {
            "path": tags.path,
            "typeinfo": type_data,
        };

        if (old_path) {
            if (!oload.o.quiet) {
                if (oload.o.verbose) {
                    printf("type %y updating\n", tags.path);
                } else {
                    oload.mystat(".");
                }
            }

            types.update(type_data_hash, {"path": tags.path});
        } else {
            if (!oload.o.quiet) {
                if (oload.o.verbose) {
                    printf("type %y inserting\n", tags.path);
                } else {
                    oload.mystat("I");
                }
            }

            types.insert(type_data_hash);
        }

        oload::omqmap_reload.types{tags.path} = True;
        return tags.path;
    }

    private static removeDots(reference<hash<auto>> fields) {
        foreach hash<auto> i in (fields.pairIterator()) {
            if (i.key =~ /\\\./) {
                string key = i.key;
                key =~ s/\\\././g;
                fields{key} = remove fields{i.key};
                i.key = key;
            }

            if (i.value.name =~ /\\\./) {
                fields{i.key}.name =~ s/\\\././g;
            }

            if (i.value.path =~ /\\\./) {
                fields{i.key}.path =~ s/\\\././g;
            }

            if (i.value.fields) {
                TypeDbInserter::removeDots(\fields{i.key}.fields);
            }
        }
    }
}

class FunctionDbInserter inherits DbInserter {
    softstring insertImpl(reference<hash<string, hash<auto>>> ohash, hash tags, hash user_tags, string file_name,
            code post_insert, *ReleaseFile release_file) {
        if (!oload.o.quiet && oload.o.verbose) {
            printf("%y: ", file_name);
        }

        oload.registerObject("function", tags{"name"}, tags{"version"}, file_name);

        softint id;

        # update function if it exists already
        *hash q = sqlif.omqp.selectRow("select function_instanceid from function_instance where name = %v and version = %v",
                                       tags{"name"}, tags{"version"});

        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        if (q.function_instanceid) {
            id = q.function_instanceid;
            if (!oload.o.quiet) {
                if (oload.o.verbose) {
                    printf("function %s:%s (%d) updating (%s)\n",
                        tags{"name"}, tags{"version"}, id, tags{"type"});
                } else {
                    oload.mystat(".");
                }
            }

            # delete all tags
            sqlif.omqp.exec("delete from function_instance_tags where function_instanceid = %v", id);

            sqlif.omqp.exec("update function_instance set function_type = %v, patch = %v, description = %v, author = %v, body = %v where function_instanceid = %v",
                            tags{"type"}, tags{"patch"}, tags{"desc"}, tags{"author"}, tags{"code"}, id);

            oload.valfh{id} = True;
        } else { # otherwise insert new function
            id = get_next_sequence_value("seq_function_instance");
            #printf("%s\n", sql);
            if (!oload.o.quiet) {
                if (oload.o.verbose) {
                    printf("function %s:%s (%d) inserting (%s)\n",
                        tags{"name"}, tags{"version"}, id, tags{"type"});
                } else {
                    oload.mystat("I");
                }
            }
            sqlif.omqp.exec("insert into function_instance (function_instanceid, function_type, name, version, patch, description, author, body) values (%v, %v, %v, %v, %v, %v, %v, %v)",
                            id, tags{"type"}, tags{"name"}, tags{"version"}, tags{"patch"}, tags{"desc"},
                            tags{"author"}, tags{"code"});

            oload.valflh{id} = True;

            # add function reverse reference
            oload.frmap{tags{"name"}}{tags{"version"}} = id;
            oload.frmap{tags{"name"}}.lastversion = tags{"version"};
        }

        # create audit msg
        oload.audit.sourceLoadedEvent(NOTHING, NOTHING, NOTHING, NOTHING, file_name, "function", tags{"name"},
                                      tags{"version"}, id,
                                      sprintf("offset: %d type: %s", user_tags."_offset", tags{"type"}));

        oload::omqmap_reload.functions{id} = True;

        # insert function tags
        oload.insertTags("function_instance", tags{"name"}, id, user_tags);

        # add release component
        if (release_file) {
            hash<auto> ch = tags + (
                "source": tags{"code"},
            );
            string source = sprintf("%y", ch);
            release_file.add(tags{"name"}, source, tags{"version"}, id);
        }

        return id;
    }
}

class ConstantDbInserter inherits DbInserter {
    softstring insertImpl(reference<hash<string, hash<auto>>> ohash, hash tags, hash user_tags, string file_name,
            code post_insert, *ReleaseFile release_file) {
        if (!oload.o.quiet && oload.o.verbose) {
            printf("%y: ", file_name);
        }

        oload.registerObject("constant", tags{"name"}, tags{"version"}, file_name);

        softint id;

        # run in single transaction in case the object is rejected due to tags
        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        # update constant if it exists already
        *hash<auto> q = sqlif.omqp.selectRow("select constantid from constants where name = %v and version = %v",
                                       tags{"name"}, tags{"version"});
        if (q.constantid) {
            id = q.constantid;
            if (!oload.o.quiet) {
                if (oload.o.verbose) {
                    printf("constant %s:%s (%d) updating\n", tags{"name"}, tags{"version"}, id);
                } else {
                    oload.mystat(".");
                }
            }

            # delete all tags
            sqlif.omqp.exec("delete from constant_tags where constantid = %v", id);

            sqlif.omqp.exec("update constants set patch = %v, description = %v, author = %v, body = %v where constantid = %v",
                            tags{"patch"}, tags{"desc"}, tags{"author"}, tags{"code"}, id);
        } else { # otherwise insert new constant
            id = get_next_sequence_value("seq_constants");
            #printf("%s\n", sql);
            if (!oload.o.quiet) {
                if (oload.o.verbose)
                    printf("constant %s:%s (%d) inserting\n", tags{"name"}, tags{"version"}, id);
                else
                    oload.mystat("I");
            }
            sqlif.omqp.exec("insert into constants (constantid, name, version, patch, description, author, body) values (%v, %v, %v, %v, %v, %v, %v)",
                            id, tags{"name"}, tags{"version"}, tags{"patch"}, tags{"desc"}, tags{"author"},
                            tags{"code"});

            # add constant reverse reference
            oload.constrmap{tags{"name"}}{tags{"version"}} = id;
            oload.constrmap{tags{"name"}}.lastversion = tags{"version"};
        }
        oload.valcoh{id} = True;

        # create audit msg
        oload.audit.sourceLoadedEvent(NOTHING, NOTHING, NOTHING, NOTHING, file_name, "constant", tags{"name"},
                                      tags{"version"}, id, sprintf("offset: %d", user_tags{"_offset"}));

        oload::omqmap_reload.constants{id} = True;

        # insert constant tags
        oload.insertTags("constant", tags{"name"}, id, user_tags);

        # add release component
        if (release_file) {
            hash<auto> ch = tags + (
                "source": tags{"code"},
            );
            string source = sprintf("%y", ch);
            release_file.add(tags{"name"}, source, tags{"version"}, id);
        }

        return id;
    }
}

class FsHelper {
    private:internal {
        # hash of file names; path -> True
        hash<string, bool> file_hash;

        # list of objects to delete
        list<hash<auto>> del_list();

        # classpath hash; path -> True
        hash<string, bool> classpath_hash;

        #! injected classes
        hash<string, binary> injected_classes;
    }

    destructor() {
        foreach hash<auto> del_hash in (new ListReverseIterator(del_list)) {
            if (del_hash.type == "file") {
                unlink(del_hash.path);
            } else {
                rmdir(del_hash.path);
            }
        }
    }

    *hash<string, binary> getInjectedClasses() {
        return injected_classes;
    }

    addInjectedClasses(hash<auto> lang_info) {
        *string package = lang_info.package;
        if (package) {
            package =~ s/\//./g;
            package += ".";
        }
        self.injected_classes += map {
            package + $1.key: $1.value,
        }, lang_info.class_data.pairIterator();
    }

    # add a path to unlink in the destructor
    add(string path) {
        path = check(path);
        del_list += {
            "type": "file",
            "path": path,
        };
    }

    # creates a directory and deletes it in the destructor
    mkdir(string path) {
        path = check(path);
        Qore::mkdir(path);
        del_list += {
            "type": "dir",
            "path": path,
        };
    }

    # adds an element to the classpath
    addClasspath(string path) {
        path = normalize_dir(path);
        classpath_hash{path} = True;
    }

    string getClasspath(*string other_classpath) {
        string classpath = sprintf("%s/jar/qorus-common.jar:%s/jar/qorus-workflow.jar:%s/jar/qorus-service.jar:"
            "%s/jar/qorus-job.jar:%s/jar/qore-jni.jar",
            ENV.OMQ_DIR, ENV.OMQ_DIR, ENV.OMQ_DIR, ENV.OMQ_DIR, ENV.OMQ_DIR);
        map classpath += ":" + $1, keys classpath_hash;
        if (other_classpath) {
            # do environment variable substitution for $OMQ_DIR
            other_classpath = replace(other_classpath, "$OMQ_DIR", ENV.OMQ_DIR);
            classpath += ":" + other_classpath;
        }
        return classpath;
    }

    private string check(string path) {
        path = normalize_dir(path);
        if (file_hash{path}) {
            throw "FILE-ERROR", sprintf("path %y has already been used", path);
        }
        file_hash{path} = True;
        return path;
    }
}

class StepDbInserter inherits DbInserter {
    softstring insertImpl(reference<hash<string, hash<auto>>> ohash, hash tags, hash user_tags, string file_name,
            code post_insert, *ReleaseFile release_file) {
        *string userdata;
        if (tags."user-metadata") {
            if (!tags."user-interaction") {
                if (tags{"steptype"} != ExecAsync) {
                    oload.error("ERROR: step %s:%s has type %y, however type %y is required to enable user interaction",
                        tags{"name"}, tags{"version"}, tags{"steptype"}, ExecAsync);
                }
                tags."user-interaction" = True;
            }
            userdata = serialize_qorus_data(tags."user-metadata");
        }

        if (tags{"user-interaction"} && tags{"steptype"} != ExecAsync) {
            oload.error("ERROR: step %s:%s has type %y, however type %y is required to enable user interaction",
                tags{"name"}, tags{"version"}, tags{"steptype"}, ExecAsync);
        }

        *int queueid;
        if (tags{"queue"}) {
            queueid = oload.qmap.(tags{"queue"});
            if (!exists queueid) {
                oload.error("step %s:%s error: queue %s does not exist", tags{"name"}, tags{"version"},
                            tags{"queue"});
            }
        } else if (tags{"steptype"} == ExecAsync) {
            oload.error("async step %s:%s: queue tag must be defined", tags{"name"}, tags{"version"});
        }

        *int eventid;
        if (tags{"event"}) {
            eventid = oload.emap.(tags{"event"});
            if (!exists eventid) {
                oload.error("step %s:%s error: event %s does not exist", tags{"name"}, tags{"version"}, tags{"event"});
            }
        } else if (tags{"steptype"} == ExecEvent) {
            oload.error("event step %s:%s: event tag must be defined", tags{"name"}, tags{"version"});
        }

        # language info, if any
        *string lang_info;
        if (tags{"lang"} == "java") {
            # we need to compile any Java source to byte code for insertion in the DB
            lang_info = JavaClassHelper::compileJavaSource("step", tags{"name"}, tags{"version"}, tags{"code"},
                user_tags, tags, cast<*list<string>>(tags{"classes"}));
        }

        # issue #3485: get any FSM triggers from the fsm_triggers column
        *string fsm_triggers = getFsmTriggers(\tags);

        *hash<auto> stepinfo = sqlif.omqp.selectRow("select * from steps where name = %v and version = %v",
            tags{"name"}, tags{"version"});

        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        *softint stepid = stepinfo{"stepid"};
        if (stepid) { # update existing step
            if (oload.o.verbose) {
                printf("updating step %s:%s (%d)\n", tags{"name"}, tags{"version"}, stepid);
            } else {
                oload.mystat(".");
            }

            # check for dangerous/illegal step redefinitions
            if (!oload.o.redef) {
                if (tags{"steptype"} != stepinfo{"steptype"}) {
                    oload.error("ERROR: step %s:%s (%d) type is currently: %y, new definition: %y", tags{"name"},
                                tags{"version"}, stepid, stepinfo{"steptype"}, tags{"steptype"});
                }
                if (tags{"arraytype"} != stepinfo{"arraytype"}) {
                    oload.error("ERROR: step %s:%s (%d) array type is currently: %y, new definition: %y",
                                tags{"name"}, tags{"version"}, stepid, stepinfo{"arraytype"}, tags{"arraytype"});
                }
            }

            sqlif.omqp.exec("update steps
                set steptype = %v, description = %v, author = %v, arraytype = %v, queueid = %v, user_interaction = %v,
                userdata = %v, workflow_event_typeid = %v, yaml_config_items = %v, classname = %v, code = %v,
                language = %v, language_info = %v, yaml_fsm_triggers = %v,
                step_classid = null, stepfunction_instanceid = null,
                validationfunction_instanceid = null, endfunction_instanceid = null, arrayfunction_instanceid = null
                where stepid = %v",
                tags{"steptype"}, tags{"desc"}, tags{"author"}, tags{"arraytype"},
                oload::intOrNull(queueid), tags{"user-interaction"}.toInt(), userdata, oload::intOrNull(eventid),
                tags{"config-items"}, tags{"class-name"} ?? tags{"name"}, tags{"code"}, tags{"lang"},
                lang_info, fsm_triggers, int(stepid));

            # clear step tags
            sqlif.omqp.exec("delete from step_tags where stepid = %v", stepid);

            # delete step mappers when updating
            sqlif.omqp.exec("delete from step_mappers where stepid = %v", stepid);

            # delete step value maps when updating
            sqlif.omqp.exec("delete from step_vmaps where stepid = %v", stepid);
        } else {  # insert new step
            stepid = get_next_sequence_value("seq_steps");
            if (oload.o.verbose) {
                printf("inserting step %s:%s (%d)\n", tags{"name"}, tags{"version"}, stepid);
            } else {
                oload.mystat("I");
            }

            sqlif.omqp.exec("insert into steps
                (stepid, steptype, name, version, description, author, arraytype, queueid,
                 user_interaction, userdata, workflow_event_typeid, yaml_config_items, classname, code, language,
                 language_info, yaml_fsm_triggers) values (%v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v,
                 %v, %v)",
                int(stepid), tags{"steptype"}, tags{"name"}, tags{"version"}, tags{"desc"}, tags{"author"},
                tags{"arraytype"}, oload::intOrNull(queueid), tags{"user-interaction"}.toInt(), userdata,
                oload::intOrNull(eventid), tags{"config-items"}, tags{"class-name"} ?? tags{"name"}, tags{"code"},
                tags{"lang"}, lang_info, fsm_triggers);

            # issue #3281: make entry in old config item value map for value migration
            *softint old_id = oload.step_name_map{tags{"name"}}.stepid;
            if (old_id) {
                *hash<auto> config_item_values = InterfaceConfigContainer::getInterfaceConfigItemValues("step:" + old_id);
                if (config_item_values) {
                    oload.config_item_value_map.step{stepid} = config_item_values;
                }
            }
        }

        *list mappers = oload.getMapperList(tags{"mappers"});
        tags{"mappers"} = ();
        # insert any step mappers
        foreach hash mapper in (mappers) {
            sqlif.omqp.exec("insert into step_mappers (stepid, mapperid) values (%v, %v)", stepid,
                            mapper{"mapperid"});
            if (oload.o.verbose) {
                printf("step %s:%s (%d) adding mapper %s v%s (%d)\n", tags{"name"}, tags{"version"}, stepid,
                    mapper{"name"}, mapper{"version"}, mapper{"mapperid"});
            } else {
                oload.mystat("I");
            }

            # save mappers for release hash
            tags{"mappers"} += mapper.("name", "version");
        }

        *list value_maps = oload.getVMapList(tags{"vmaps"});
        tags{"vmaps"} = ();

        # insert any step value maps
        foreach hash value_map in (value_maps) {
            sqlif.omqp.exec("insert into step_vmaps (stepid, id) values (%v, %v)", stepid, value_map{"id"});
            if (oload.o.verbose) {
                printf("step %s:%s (%d) adding value map %s (%d)\n", tags{"name"}, tags{"version"}, stepid,
                    value_map{"name"}, value_map{"id"});
            } else {
                oload.mystat("I");
            }

            # save value maps for release hash
            tags{"vmaps"} += value_map.("name",);
        }

        # insert tags
        oload.insertTags("step", tags{"name"}, stepid, user_tags);

        DbInserter::insertLibraries("step", stepid, tags);

        oload::omqmap_reload.steps{stepid} = True;
        oload.stepmap{tags{"name"}}{tags{"version"}} = stepid;

        # add to validation map
        oload.val_step_map{stepid} = True;

        return stepid;
    }
}

class ClassDbInserter inherits DbInserter {
    private {
        # classes that depend on other classes; dependent class -> required class -> True
        static hash<string, hash<string, bool>> deps;

        # reverse dependencies; required class -> dependent class -> True
        static hash<string, hash<string, bool>> reverse_deps;
    }

    # now process classes with dependencies
    static finalize() {
        while (True) {
            if (!deps) {
                break;
            }
            int write_count = 0;
            foreach string cname in (keys deps) {
                if (!deps{cname}) {
                    # write all versions of the class, in case there is more than one being loaded
                    foreach string version in (keys JavaClassHelper::content_index{cname}) {
                        hash<auto> class_hash = JavaClassHelper::content_index{cname}{version};
                        ClassDbInserter::write(class_hash.metainfo, class_hash.uinfo, class_hash.filename,
                                               class_hash.buffer, class_hash.release_file);
                    }
                    ++write_count;
                }
            }
            if (!write_count) {
                # remove classes already in the DB
                AbstractTable classes = oload::getTable("classes");
                hash<auto> sh = {
                    "columns": cop_distinct("name"),
                    "where": {"name": op_in(keys reverse_deps)},
                };
                foreach string name in (classes.select(sh).name) {
                    ClassDbInserter::removeReverseDependency(name);
                    ++write_count;
                }
            }

            if (!write_count) {
                stderr.printf("ERROR: the following unknown classes were listed as class dependencies: %y\n"
                    "Remove these dependencies or include them in the load set\n",
                    keys reverse_deps);
                exit(1);
            }
        }
    }

    *hash<string, hash<auto>> makeHash(*list<auto> objects) {
        return map {$1.name ?? $1."class-name": $1}, objects;
    }

    softstring insertImpl(reference<hash<string, hash<auto>>> ohash, hash<auto> tags, hash<auto> user_tags,
            string file_name, code post_insert, *ReleaseFile release_file) {
        if (!oload.o.quiet && oload.o.verbose) {
            printf("%y: ", file_name);
        }

        if (!tags{"name"}) {
            tags{"name"} = tags{"class-name"};
        }

        if (!tags{"name"}) {
            oload.error("missing name in class definition");
        }

        oload.registerObject("class", tags{"name"}, tags{"version"}, file_name);

        # process class dependencies; for java we have to process dependencies here or compilation will fail
        if (tags{"requires"}) {
            # if the class has not been loaded, then save it for later
            #bool found_all = True;
            foreach string cls in (tags{"requires"}) {
                *string classversion = oload.classrmap{cls}.lastversion;
                # issue #3901: load class immediately if required and try again
                if (!classversion && (*hash<auto> obj = remove ohash{cls})) {
                    if (obj.author.typeCode() == NT_LIST) {
                        obj{"author"} = obj{"author"}.join("; ");
                    }
                    insert(\ohash, obj, obj.tags, obj.yaml_source.file ?? obj.file_name, post_insert, obj.release_file);
                    classversion = oload.classrmap{cls}.lastversion;
                }

                # issue #3294: allow languages to be mixed with "requires"
                if (!classversion
                    && (!JavaClassHelper::content_index{cls}{classversion} ||
                        (tags{"lang"} == "java" &&
                        !JavaClassHelper::content_index{cls}{classversion}.info.lang_hash))) {
                    throw "CLASS-DEPENDENCY-ERROR", sprintf("missing class %y%s which is a dependency for "
                        "class %y v%s; remove this dependency or add the class to the load set",
                        cls, exists classversion ? sprintf(" (version %s)", classversion) : "",
                        tags.name, tags.version);
                }
            }
        }

        return ClassDbInserter::write(tags, user_tags, file_name, tags{"code"}, release_file);
    }

    static private removeReverseDependency(string name) {
        foreach string cls in (keys reverse_deps{name}) {
            remove deps{cls}{name};
        }
        remove reverse_deps{name};
    }

    static private removeDependency(string name) {
        if (reverse_deps{name}) {
            ClassDbInserter::removeReverseDependency(name);
        }
        # remove class dependencies
        remove deps{name};
    }

    static private int write(hash<auto> tags, hash<auto> user_tags, string file_name, string code_,
                             *ReleaseFile release_file) {
        # remove from dependency structures
        ClassDbInserter::removeDependency(tags{"name"});

        # language info, if any
        *string lang_info;
        if (tags{"lang"} == "java") {
            # we need to compile any Java source to byte code for insertion in the DB
            lang_info = JavaClassHelper::compileJavaSource("class", tags{"name"}, tags{"version"}, code_, user_tags,
                tags, cast<*list<string>>(tags{"requires"}));
        }

        softint id;

        # run in single transaction in case the object is rejected due to tags
        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        # update class if it exists already
        *hash<auto> q = sqlif.omqp.selectRow("select classid from classes where name = %v and version = %v",
                                             tags{"name"}, tags{"version"});

        if (q.classid) {
            id = q.classid;

            if (!oload.o.quiet) {
                if (oload.o.verbose) {
                    printf("class %s:%s (%d) updating\n", tags{"name"}, tags{"version"}, id);
                } else {
                    oload.mystat(".");
                }
            }

            # delete all tags
            sqlif.omqp.exec("delete from class_tags where classid = %v", id);

            # delete any old dependencies
            sqlif.omqp.exec("delete from class_dependencies where classid = %v", id);

            sqlif.omqp.exec("update classes set patch = %v, description = %v, author = %v, body = %v, language = %v, "
                "language_info = %v, yaml_config_items = %v, connectors = %v, processor = %v where classid = %v",
                tags{"patch"}, tags{"desc"}, tags{"author"}, code_, tags{"lang"}, lang_info, tags{"config-items"},
                tags{"class-connectors"}, tags{"processor"}, id);

            # update language info in map if necessary
            if (oload.classmap{id}.language != tags{"lang"}) {
                oload.classmap{id}.language = tags{"lang"};
            }

            #printf("CHECKING o: %y b: %y\n%s", oload.o.validate, code_ =~ /getConfigItemsImpl/, code_);

            # check if validation is not enabled, enable validation if the class is a step class
            # so confiuration can be loaded if necessary
            if (!oload.o.validate) {
                AbstractTable steps = oload::getTable("steps");
                if (steps.selectRow({"where": {"step_classid": id}, "limit": 1})) {
                    oload.o.validate = True;
                }
            }
        } else { # otherwise insert new class
            id = get_next_sequence_value("seq_classes");
            #printf("%s\n", sql);
            if (!oload.o.quiet) {
                if (oload.o.verbose) {
                    printf("class %s:%s (%d) inserting\n", tags{"name"}, tags{"version"}, id);
                } else {
                    oload.mystat("I");
                }
            }

            sqlif.omqp.exec("insert into classes (classid, name, version, patch, description, author, body, language, "
                "language_info, yaml_config_items, connectors, processor) values "
                "(%v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v)", id, tags{"name"}, tags{"version"}, tags{"patch"},
                tags{"desc"}, tags{"author"}, code_, tags{"lang"}, lang_info, tags{"config-items"},
                tags{"class-connectors"}, tags{"processor"});

            # add class reverse reference
            oload.classrmap{tags{"name"}}{tags{"version"}} = id;
            oload.classrmap{tags{"name"}}.lastversion = tags{"version"};
        }

        # add forward info
        oload.classmap{id} = {
            "name": tags{"name"},
            "version": tags{"version"},
            "language": tags{"lang"},
        };

        oload.valclh{id} = True;

        # create audit msg
        oload.audit.sourceLoadedEvent(NOTHING, NOTHING, NOTHING, NOTHING, file_name, "class", tags{"name"},
                                      tags{"version"}, id, sprintf("offset: %d", user_tags{"_offset"}));

        oload::omqmap_reload.classes{id} = True;

        # insert class tags
        oload.insertTags("class", tags{"name"}, id, user_tags);

        # insert class dependencies
        map sqlif.omqp.exec("insert into class_dependencies (classid, dependson_class) values (%v, %v)", id, $1),
            tags{"requires"};

        # add release component
        if (release_file) {
            hash<auto> ch = tags + (
                "source": code_,
            );
            string source = sprintf("%y", ch);
            release_file.add(tags{"name"}, source, tags{"version"}, id);
        }

        return id;
    }
}

#! used to create generic objects with no declared API for Java sources
class QorusUniversalJavaProgram inherits QorusProgram {
    constructor(int java_api = QPJ_New) : QorusProgram(True, CommonParseOptions | PO_NO_INHERIT_USER_CLASSES
            | PO_NO_INHERIT_USER_FUNC_VARIANTS | PO_NO_INHERIT_GLOBAL_VARS | PO_NO_INHERIT_USER_CONSTANTS
            | PO_NO_INHERIT_USER_HASHDECLS,
            QorusServerDefines) {
        # import fake APIs
        qorus_load_fake_universal_api_module(self);

        # export system objects to program
        importGlobalVariable("omqservice");

        QorusProgram::initJava(java_api);
        WorkflowProgram::staticInitJava(self, java_api);
        ServiceProgram::staticInitJava(self, java_api);
        JobProgram::staticInitJava(self, java_api);
        MapperProgram::staticInitJava(self, java_api);
    }
}

#! used to create interface module program objects
public class InterfaceModuleProgram inherits Program {
    constructor(*int po) : Program(PO_NO_INHERIT_GLOBAL_VARS
        |PO_NO_THREAD_CONTROL
        |PO_NO_PROCESS_CONTROL
        |PO_NO_INHERIT_USER_FUNC_VARIANTS
        |PO_NO_INHERIT_USER_CLASSES
        |PO_NO_INHERIT_USER_HASHDECLS
        |PO_NO_INHERIT_PROGRAM_DATA
        |po) {
    }
}

class JavaClassHelper inherits PythonDependencyHelper {
%ifndef NO_JNI
    static private loadJavaClassForCompilation(
            string name,
            QorusProgram pgm,
            string classname,
            string required,
            FsHelper fshelper,
            reference<hash<string, bool>> loading,
            reference<hash<string, bool>> loaded) {
        if (loading{required}) {
            throw "CLASS-DEPENDENCY-ERROR",
                sprintf("circular dependency when loading class %y dependency %y (dependency of %y)",
                        loading.firstKey(), required, loading.lastKey());
        }
        if (loaded{required}) {
            return;
        }
        loading{required} = True;

        *string reqversion = oload.classrmap{required}.lastversion;
        int classid;
        if (!exists reqversion) {
            # see if we have it in content_index
            if (content_index{required}) {
                reqversion = content_index{required}.lastKey();
            } else {
                throw "CLASS-DEPENDENCY-ERROR",
                    sprintf("class %y required by %y is not present in the DB or being currently loaded", required,
                            classname);
            }
        }

        # load dependencies
        classid = oload.classrmap{required}{reqversion};
        # load from the DB
        AbstractTable class_dependencies = oload::getTable("class_dependencies");
        hash<auto> sh = {
            "columns": ("dependson_class"),
            "where": {"classid": classid},
        };
        context (class_dependencies.select(sh)) {
            if (!loaded{%dependson_class}) {
                JavaClassHelper::loadJavaClassForCompilation(name, pgm, required, %dependson_class, fshelper,
                    \loading, \loaded);
            }
        }

        hash<auto> lang_info;
        {
            *hash<auto> current_info = content_index{required}{reqversion}.info;
            if (current_info) {
                lang_info = current_info.lang_hash;
            } else {
                AbstractTable classes = oload::getTable("classes");
                sh = {
                    "columns": ("language_info"),
                    "where": {"classid": classid},
                };
                hash<auto> row = classes.selectRow(sh);
                if (row.language_info) {
                    lang_info = parse_yaml(row.language_info);
                }
            }
        }
        if (!lang_info.class_data) {
            JavaClassHelper::compileOtherToJava(name, pgm, classid, required, reqversion);
        } else {
            fshelper.addInjectedClasses(lang_info);
            *string classpath = oload::getTable("class_tags").selectRow({
                "columns": "value",
                "where": {
                    "classid": classid,
                    "tag": "classpath",
                },
            }).value;
            if (classpath) {
                fshelper.addClasspath(classpath);
            }
        }

        loaded{required} = True;
    }

    #! Returns a language_info hash for Qore or Python code
    static compileOtherToJava(string name, QorusProgram pgm, int classid, string class_name, string version) {
        AbstractTable classes = oload::getTable("classes");
        hash<auto> sh = {
            "columns": ("body", "language", "language_info",),
            "where": {"classid": classid},
        };
        hash<auto> row = classes.selectRow(sh);

        hash<auto> tags = sqlif.getTags("class", classid);
        tags += remove tags.sys;

        if (row.language == "qore") {
            pgm.parsePending(row.body, sprintf("class %s:%s", class_name, version), oload.warningMask, tags.source,
                tags.offset);
        } else if (row.language == "python") {
            pgm.cachePythonClass(class_name, row.body, row.language_info, NOTHING, tags.module_path);
            python_dep_map{name} = True;
        } else {
            throw "UNSUPPORTED-LANGUAGE", sprintf("language %y is not supported; expecting \"qore\" or \"python\"",
                row.language);
        }
    }
%endif

    #! Returns a language_info string for the DB
    static string compileJavaSource(string type, string class_name, string version, string code_,
            hash<auto> user_tags, hash<auto> info, *list<string> requires) {
%ifdef NO_JNI
        throw "NO-JAVA", "the jni module is not available";
%else
        # issue #3707: compile in a separate process in case there are Python dependencies that can only be loaded
        # once in a process
        hash<auto> python_dep_info;
        if (!JavaClassHelper::hasPythonDependencies(class_name, code_, requires, \python_dep_info)) {
            return JavaClassHelper::compileJavaSourceIntern(type, class_name, version, code_, user_tags, info, requires);
        }

        if (!oload.o.quiet && oload.o.verbose) {
            if (python_dep_info.dynamic) {
                printf("%s class %s v%s: compiling Java class externally due to dynamic Python dependency in "
                    "class %y\n", type, class_name, version, python_dep_info.dep_name);
            } else {
                printf("%s class %s v%s: compiling Java class externally due to Python class dependency %y\n",
                    type, class_name, version, python_dep_info.dep_name);
            }
        }

        # get temporary file name
        string input_filename = sprintf("%s%soload-%d-input-%s.txt", tmp_location(), DirSep, getpid(),
            get_random_string());
        hash<auto> opt = {
            "type": type,
            "class_name": class_name,
            "version": version,
            "code": code_,
            "user_tags": user_tags,
            "info": info,
            "requires": requires,
            "content_index": content_index,
            "filename": input_filename,
        };

        ExternalProcessHelper proc("JAVA-COMPILATION-ERROR", "--compile-java=-");
        on_exit unlink(input_filename);
        #printf("created child oload process with PID %d\n", proc.id());

        proc.write(opt);
        proc.wait();

        string rv = File::readTextFile(input_filename);
        if (!oload.o.quiet && oload.o.verbose) {
            printf("%s class %s v%s: done compiling Java class externally (%d bytes)\n", type, class_name, version,
                rv.size());
        }
        return rv;
    }

    #! Returns a language_info string for the DB
    static string compileJavaSourceIntern(string type, string class_name, string version, string code_,
            hash<auto> user_tags, hash<auto> info, *list<string> requires) {
        # get package name
        *string package = (code_ =~ x/^package ([^;]+);/m)[0];
        # create source file with the proper name
        # issue #2683: use a unique directory to avoid collisions with other oload processes
        string dir = tmp_location() + DirSep + get_random_string();
        #string sfn = dir + DirSep + class_name + ".java";
        hash<string, CompilerOutput> compiler_output;
        QorusProgram pgm;
        int java_api;
        {
            # unlink all class files at the end of compilation
            FsHelper fshelper();

            # add/create directory
            fshelper.mkdir(dir);

            # add blank lines so that Java errors report the same source line
            if (user_tags._offset) {
                code_ = strmul("\n", user_tags._offset) + code_;
            }

            # get binary name for object
            string bin_name;
            if (package) {
                bin_name = package + ".";
            }
            bin_name += class_name;

            ArrayList options();
            if (oload.o.verbose) {
                options.add("-Xdiags:verbose");
                options.add("-Xlint:unchecked");
                options.add("-Xlint:deprecation");
            }

            if (!java_api) {
                java_api = QorusProgram::QPJ_New;
            } else {
                java_api = info.java_api == "new" ? QorusProgram::QPJ_New : QorusProgram::QPJ_Old;
            }
            pgm = JavaClassHelper::getContainerForType(type, info, java_api, class_name, version);

            hash<string, string> sources = {
                bin_name: code_,
            };

            # process dependencies
            if (requires) {
                # get unique name
                string name = sprintf("%s/%s/%s", type, info.name, info.version);

                # hash of objects being loaded to catch recursive dependencies
                hash<string, bool> loading = {class_name: True};
                # hash of objects already loaded
                hash<string, bool> loaded;
                foreach string required in (requires) {
                    # get compiled byte code
                    JavaClassHelper::loadJavaClassForCompilation(name, pgm, class_name, required, fshelper,
                        \loading, \loaded);
                }
                pgm.parseCommit();
            }

            # setup classpath
            options.add("-cp");
            options.add(fshelper.getClasspath(user_tags.classpath));

            compiler_output = cast<hash<string, CompilerOutput>>(pgm.javaCompile(sources, options,
                fshelper.getInjectedClasses(), fshelper.getClasspath(user_tags.classpath), java_api));
        }

        # get a map from binary names to byte code
        hash<auto> class_data = map {
            $1.key: $1.value.file.openInputStream().readAllBytes(),
        }, compiler_output{sort(keys compiler_output)}.pairIterator();

        # log results
        if (oload.o.verbose) {
            foreach hash<auto> i in (class_data.pairIterator()) {
                if (i.key =~ /\$/) {
                    printf("%s %s:%s: compiled inner class %y (%d bytes)\n",
                        type, class_name, version, i.key, i.value.size());
                } else {
                    printf("%s %s:%s: compiled source (%d bytes) to binary class %y (%d bytes)\n",
                        type, class_name, version, code_.size(), i.key, i.value.size());
                }
            }
        }

        # convert dots to slashes in package name
        if (package) {
            package =~ s/\./\//g;
        }

        # create language_info data
        hash<auto> lang_hash += {
            "class_data": class_data,
            "package": package,
        };
        if (java_api == QorusProgram::QPJ_New) {
            lang_hash.java_api = "new";
        }
        if (pgm.compat_qore_module_imports) {
            lang_hash.compat_imports = True;
        }

        # save byte code for possible use in a dependency
        content_index{class_name}{version}.info = {
            "lang": "java",
            "lang_hash": lang_hash,
        };

        # create and return YAML string for insertion in the DB
        return make_yaml(lang_hash);
%endif
    }

    static QorusProgram getContainerForType(string type, hash<auto> info, int java_api, string name, string ver) {
        QorusProgram pgm;

        switch (type) {
            case "workflow":
            case "step": {
                WorkflowInterfaceContainerBase container(info);
                pgm = container.getProgram();
                break;
            }

            case "service": {
                ServiceContainer container(info);
                pgm = container.getProgram();
                break;
            }

            case "job": {
                JobContainer container(info);
                pgm = container.getProgram();
                break;
            }

            default: {
                pgm = new QorusUniversalJavaProgram(java_api);
                break;
            }
        }

        pgm.setScriptPath(sprintf("%s %s v%s", type, name, ver));
        return pgm;
    }
}

class ValueMapDbInserter inherits DbInserter {
    softstring insertImpl(reference<hash<string, hash<auto>>> ohash, hash tags, hash user_tags, string file_name,
            code post_insert, *ReleaseFile release_file) {
        #printf("value map: %y\n", info);
        AbstractTable vs = oload::getTable("value_maps");
        AbstractTable vsv = oload::getTable("value_map_values");

        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        tags{"throws_exception"} = (tags{"throws_exception"} ?? tags{"exception"}).toInt();

        *softint id;
        {
            tags{"description"} = remove tags{"desc"};
            hash m_meta = tags.(vs.describe().keys());
            id = vs.selectRow(("columns": "id", "where": {"name": tags{"name"}})).id;
            if (!id) {
                if (vs.hasReturning()) {
                    id = vs.insert(m_meta, {"returning": "id"}).id;
                } else {
                    vs.insert(m_meta);
                    id = vs.selectRow({"columns": "id", "where": {"name": tags{"name"}}}).id;
                }

                if (oload.o.verbose) {
                    printf("value map %s (%d) inserting\n", tags{"name"}, id);
                } else {
                    oload.mystat("I");
                }
            } else {
                if (oload.o.verbose) {
                    printf("value map %s (%d) updating\n", tags{"name"}, id);
                } else {
                    oload.mystat(".");
                }

                vs.upsert(m_meta + {"id": id});
            }
        }

        # values
        vsv.del({"value_map_id": id});
        hash opts = {
            "fields": {
                "0": "string",
                "1": (tags{"valuetype"} == "date") ? {"type": tags{"valuetype"}, "format": tags{"dateformat"}}
                                                   : tags{"valuetype"},
                "2": "*string"
            },
        };

        int cnt = 0;
        list sl;

        if (exists tags{"code"}) {
            CsvUtil::CsvDataIterator it(tags{"code"}, opts);
            # source list to calculate the hash based on a defined key order
            while (it.next()) {
                hash rec = it.getValue();
                string key = rec."0";
                string val = vmap_value(tags, \rec."1");
                int enabled = (!exists it.getValue()."2" ? True : parse_boolean(rec."2")).toInt(); # toInt() for pgsql

                # add source for the release file hash
                if (release_file) {
                    sl += {
                        "key": key,
                        "src": sprintf("%y(%d) = %s\n", key, enabled, trim(val)),
                    };
                }

                hash<auto> row = {
                    "value_map_id": id,
                    "keyname": key,
                    "value": val,
                    "enabled": enabled,
                };
                vsv.insert(row);
                ++cnt;
            }
        } else if (exists tags{"value-maps"}) {
            # YAML syntax
            foreach auto iterator in (tags{"value-maps"}.pairIterator()) {
                # add source for the release file hash
                if (release_file) {
                    auto ignored;
                    sl += {
                        "key": iterator.key,
                        "src": sprintf("%y(%d) = %s\n", iterator.key, iterator.value.enabled.toInt(),
                                       trim(vmap_value(tags, \ignored))),
                    };
                }

                hash<auto> row = {
                    "value_map_id": id,
                    "keyname": iterator.key,
                    "value": iterator.value.value,
                    "enabled": iterator.value.enabled.toInt(),
                };
                vsv.insert(row);
                ++cnt;
            }
        } else {
            oload.error("no value map data are provided, please correct %s and try again", file_name);
        }

        vs.update({"mapsize": cnt}, {"id": id});

        # make / update entry in maps for mapper
        oload.vmmap.(tags{"name"}) = oload.mrmap.(id) = {
            "name"      : tags{"name"},
            "id"        : id,
            "created"   : now_us(),
        };

        # create audit msg
        oload.audit.sourceLoadedEvent(NOTHING, NOTHING, NOTHING, NOTHING, file_name, "vmap", tags{"name"}, NOTHING,
                                      id);

        # add release component
        if (release_file) {
            # create a source string for the release file hash
            string src = sprintf("name: %y desc: %y author: %y valuetype: %y dateformat: %y throws_exception: %y\n",
                                 tags{"name"}, tags{"desc"}, tags{"author"}, tags{"valuetype"}, tags{"dateformat"},
                                 boolean(tags{"throws_exception"}));

            map src += $1.src, sort(sl, int sub (hash l, hash r) {return l.key <=> r.key;});
            release_file.add(tags{"name"}, src, NOTHING, id);
        }

        oload::omqmap_reload.vmaps{id} = True;

        return id;
    }
} # class ValueMapDbInserter

class MapperDbInserter inherits DbInserter {
    softstring insertImpl(reference<hash<string, hash<auto>>> ohash, hash tags, hash user_tags, string file_name,
            code post_insert, *ReleaseFile release_file) {
        string mappersrc;
        string optsrc;

        if (tags{"is_yaml"}) {
            # issue #3740: re-escape any escaped dots in the "name" values
            foreach hash<auto> i in (tags.fields.pairIterator()) {
                if (i.value.name =~ /\\\./) {
                    i.value.name =~ s/\\\./\\\\\./g;
                    tags.fields{i.key}.name = i.value.name;
                }
            }
            mappersrc = serialize_qorus_data(tags{"fields"});
            # process options
            foreach hash<auto> i in (tags.options.pairIterator()) {
                if (i.key == "mapper-input" || i.key == "mapper-output") {
                    continue;
                }
                oload::parseOptionValue(\tags.options{i.key});
            }
            optsrc = serialize_qorus_data(tags{"options"});
        } else {
            # re-create mapper src
            mappersrc = foldl $1 + "\n" + $2,
                              (map sprintf("FIELD: %s: %s", $1.key, $1.value), tags{"fields"}.pairIterator());

            # re-create option src
            optsrc = foldl $1 + "\n" + $2,
                           (map sprintf("OPTION: %s: %s", $1.key, $1.value), tags{"options"}.pairIterator());

        }

        # convert context to a string
        if (tags."context") {
            tags."context" = sprintf("%s:%s", tags."context".iface_kind, tags."context".name);
            if (tags."context".version) {
                tags."context" += ":" + tags."context".version;
            }
        }

        # run in single transaction in case the object is rejected due to tags
        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        *softint id = sqlif.omqp.selectRow("select mapperid from mappers where name = %v and version = %v",
                                           tags{"name"}, tags{"version"}).mapperid;
        if (id) {
            if (oload.o.verbose) {
                printf("mapper %s:%s (%d) updating\n", tags{"name"}, tags{"version"}, id);
            } else {
                oload.mystat(".");
            }

            sqlif.omqp.exec("update mappers set patch=%v, description=%v, author=%v, parse_options=%v, type=%v, "
                "fields=%v, options=%v, is_yaml=%v, context=%v where mapperid=%v",
                            tags{"patch"}, tags{"desc"}, tags{"author"}, tags{"parse_options"}, tags{"type"},
                            mappersrc, optsrc, tags{"is_yaml"}.toInt(), tags."context", id);

            # delete all mapper tags
            sqlif.omqp.exec("delete from mapper_tags where mapperid = %v", id);
        } else {
            id = get_next_sequence_value("seq_mappers");
            if (oload.o.verbose) {
                printf("mapper %s:%s (%d) inserting\n", tags{"name"}, tags{"version"}, id);
            } else {
                oload.mystat("I");
            }

            sqlif.omqp.exec("insert into mappers (mapperid, name, version, patch, description, author, type, "
                "parse_options, fields, options, is_yaml, context) values (%v, %v, %v, %v, %v, %v, %v, %v, %v, %v, "
                "%v, %v)", id, tags{"name"}, tags{"version"}, tags{"patch"}, tags{"desc"}, tags{"author"},
                tags{"type"}, tags{"parse_options"}, mappersrc, optsrc, tags{"is_yaml"}.toInt(), tags."context");
        }

        # flag for mappers to be reloaded
        oload::omqmap_reload.mappers{id} = True;

        # make / update entry in maps for mapper
        oload.mmap.(tags{"name"}).(tags{"version"}) = oload.mrmap{id} = {
            "name"      : tags{"name"},
            "version"   : tags{"version"},
            "patch"     : tags{"patch"},
            "mapperid"  : id,
            "created"   : now_us(),
            "type"      : tags{"type"},
        };

        # create audit msg
        oload.audit.sourceLoadedEvent(NOTHING, NOTHING, NOTHING, NOTHING, file_name, "mapper", tags{"name"},
                                      tags{"version"}, id);

        oload.valmh{id} = {
            "id": id,
            "name": tags{"name"},
            "version": tags{"version"},
        };

        # insert tags
        oload.insertTags("mapper", tags{"name"}, id, user_tags);

        # add release component
        if (release_file) {
            release_file.add(tags{"name"}, optsrc + mappersrc, tags{"version"}, id);
        }

        DbInserter::insertLibraries("mapper", id, tags);
        DbInserter::insertLibraries("mapper", id, tags{"mapper-code"}, OT_CLASS);
        return id;
    }
}

class ServiceDbInserter inherits DbInserter {
    checkMethod(hash<auto> method_tags, bool class_based) {
        if (class_based) {
            if (!exists method_tags{"name"}) {
                oload.error("no service method name was given for this code block");
            }
        }

        if ((!method_tags{"code"}.val() && !class_based) || !method_tags{"name"}.val()) {
            oload.error("service methods must have a name and a body (name: %y, lock: %y, body.size(): %d, info: %y)",
                        method_tags{"name"}, method_tags{"lock"}, method_tags{"code"}.size(), method_tags);
        }
    }

    bool doesMethodExist(string name, int serviceid) {
        *softint service_methodid = sqlif.omqp.selectRow("select service_methodid from service_methods where "
            "name = %v and serviceid = %v", name, serviceid).service_methodid;
        return (exists service_methodid);
    }

    insertMethod(string type, string name, string version, int serviceid, hash method_tags,
                 hash user_tags, bool class_based, *string author) {
        # run in single transaction in case the object is rejected due to method_tags
        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        method_tags{"code"} = class_based ? NOTHING : method_tags{"code"};
        method_tags{"author"} = method_tags{"author"} ?? author;
        ServiceDbInserter::insertMethod(type, name, version, serviceid, method_tags, user_tags);
    }

    private:internal insertMethodInternal(hash service_tags, hash user_tags, int serviceid, hash method_tags,
                                          string file_name) {
        checkMethod(method_tags, service_tags{"class-based"});

        # throw an error if the method has been defined twice
        if (doesMethodExist(method_tags{"name"}, serviceid)) {
            # warn about duplicate method variants for existing members nor defined with the original variant
            if (service_tags{"class-based"}) {
                oload::warning("service method variant for the %s() method not defined with the original "
                                "definition; move this method variant next to the original method definition to "
                                "avoid seeing this warning", method_tags{"name"});
                return;
            }
            #printf("%N\n", (map sprintf("%s() %s:%d", $1.function, $1.file, $1.line), get_thread_call_stack()[1..]));
            oload.error("method '%s.%s.%s()' defined twice", tolower(service_tags{"servicetype"}), service_tags{"name"},
                        method_tags{"name"});
        } else {
            insertMethod(service_tags{"servicetype"}, service_tags{"name"}, service_tags{"version"}, serviceid,
                         method_tags, user_tags, service_tags{"class-based"} ?? False, service_tags{"author"});
        }
    }

    private:internal int insertService(hash tags, hash user_tags, string file_name) {
        if (oload.o.verbose && !oload.o.quiet) {
            printf("%y: ", file_name);
        }
        if (!exists tags{"remote"}) {
            tags{"remote"} = tags{"servicetype"} == "SYSTEM" ? False : oload.coptions.remote;
        }

        oload.registerObject(tags{"servicetype"} + " service", tags{"name"}, tags{"version"}, file_name);

        # prepare service_modules value
        string service_modules_string;
        if (tags{"modules"}) {
            service_modules_string = tags{"modules"}.typeCode() == NT_STRING
                ? tags{"modules"}
                : tags{"modules"}.join(",");
        }

        string api_manager;
        if (tags."api-manager") {
            # process any file options as service resources
            {
                # get factory object
                AbstractDataProviderFactory factory = DataProvider::getFactoryEx(tags."api-manager".factory);
                *hash<auto> opts =
                    factory.getInfo().provider_info.constructor_options{keys tags."api-manager"."provider-options"};
                # make a hash of current resources
                *hash<string, bool> rsrc_map = map {$1: True}, tags.resource;
                string base_dir = normalize_dir(dirname(file_name));
                foreach hash<auto> i in (opts.pairIterator()) {
                    foreach AbstractDataProviderType type in (i.value.type) {
                        if (type.getTags().from_location) {
                            string val = normalize_dir(tags."api-manager"."provider-options"{i.key}.value, base_dir);
                            val = get_relative_path(base_dir, val);
                            val =~ s/\.\///;
                            if (!rsrc_map{val}) {
                                rsrc_map{val} = True;
                            }
                            tags."api-manager"."provider-options"{i.key}.value = "resource://" + val;
                        }
                    }
                }
                if (rsrc_map) {
                    tags.resource = keys rsrc_map;
                }
            }

            # verify that all methods exists and add all FSMs as library dependencies
            *hash<string, bool> method_map = map {$1.name: True}, tags.methods;
            *hash<string, bool> fsmmap = map {$1.name: True}, tags.fsm;
            foreach hash<auto> ep in (tags."api-manager".endpoints) {
                if (ep.type == "method") {
                    if (!method_map{ep.value}) {
                        oload.error("endpoint %y in service %y refers to non-existent method %y; known methods: %y",
                            ep.endpoint, tags.name, ep.value, keys method_map);
                    }
                } else if (ep.type == "fsm") {
                    if (!tags.fsm) {
                        tags.fsm = ();
                    }
                    if (!fsmmap{ep.value}) {
                        tags.fsm += {"name": ep.value};
                        fsmmap{ep.value} = True;
                    }
                }
            }

            api_manager = make_yaml(tags."api-manager");
        }

        string events;
        foreach hash<auto> event in (\tags.events) {
            # process any file options as service resources
            if (event.type == "factory" && event.options) {
                # get factory object
                AbstractDataProviderFactory factory = DataProvider::getFactoryEx(event.name);
                *hash<auto> opts =
                    factory.getInfo().provider_info.constructor_options{keys event.options};
                # make a hash of current resources
                *hash<string, bool> rsrc_map = map {$1: True}, tags.resource;
                string base_dir = normalize_dir(dirname(file_name));
                foreach hash<auto> i in (opts.pairIterator()) {
                    foreach AbstractDataProviderType type in (i.value.type) {
                        if (type.getTags().from_location) {
                            string val = normalize_dir(event.options{i.key}.value, base_dir);
                            val = get_relative_path(base_dir, val);
                            val =~ s/\.\///;
                            if (!rsrc_map{val}) {
                                rsrc_map{val} = True;
                            }
                            event.options{i.key}.value = "resource://" + val;
                        }
                    }
                }
                if (rsrc_map) {
                    tags.resource = keys rsrc_map;
                }
            }

            # verify that all methods exists and add all FSMs as library dependencies
            *hash<string, bool> method_map = map {$1.name: True}, tags.methods;
            *hash<string, bool> fsmmap = map {$1.name: True}, tags.fsm;
                if (event.handler.type == "method") {
                    if (!method_map{event.handler.value}) {
                        oload.error("event handler in service %y refers to non-existent method %y; known methods: %y",
                            tags.name, event.handler.value, keys method_map);
                    }
                } else if (event.handler.type == "fsm") {
                    if (!tags.fsm) {
                        tags.fsm = ();
                    }
                    if (!fsmmap{event.handler.value}) {
                        tags.fsm += {"name": event.handler.value};
                        fsmmap{event.handler.value} = True;
                    }
                }

            events = make_yaml(tags.events);
        }

        # run in single transaction in case the object is rejected due to tags
        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        # issue #281: old config item values for migration
        *hash<auto> config_item_values;

        if (tags{"servicetype"} == "SYSTEM") {
            # feature 1264: delete all other system services with the same name before installing
            *list<auto> l = sqlif.omqp.select("select serviceid from services where name = %v and service_type = %v and "
                "version != %v", tags{"name"}, tags{"servicetype"}, tags{"version"}).serviceid;

            if (l) {
                # issue #3281: get config item values of last service
                softint old_id = oload.smap{tags.name}.serviceid;
                config_item_values = InterfaceConfigContainer::getInterfaceConfigItemValues("service:" + old_id);

                AbstractTable t = oload::getTable("service_methods");
                *list ml = t.select({
                    "columns": "service_methodid",
                    "where": {"serviceid": op_in(l)},
                }).service_methodid;

                if (oload.o.verbose) {
                    printf("%s service %s: deleting %d definition%s (%d method%s) from %sexisting/incompatible service%s: ",
                           tags{"servicetype"}, tags{"name"}, l.size(),
                           l.size() == 1 ? "" : "s", ml.size(),
                           ml.size() == 1 ? "" : "s",
                           l.size() == 1 ? "an " : "",
                           l.size() == 1 ? "" : "s");
                    flush();
                }

                # delete library references
                t = oload::getTable("service_lib");
                t.del(("serviceid": op_in(l)));

                # delete mappers
                t = oload::getTable("service_mappers");
                t.del(("serviceid": op_in(l)));

                # delete value maps
                t = oload::getTable("service_vmaps");
                t.del(("serviceid": op_in(l)));

                # delete method tags
                t = oload::getTable("service_method_tags");
                t.del(("service_methodid": op_in(ml)));

                # delete all methods
                t = oload::getTable("service_methods");
                t.del(("serviceid": op_in(l)));

                # delete all file resources
                t = oload::getTable("service_file_resources");
                t.del(("serviceid": op_in(l)));

                # delete group entries
                t = oload::getTable("group_services");
                t.del(("serviceid": op_in(l)));

                # delete audit events
                t = oload::getTable("audit_events");
                t.del(("serviceid": op_in(l)));

                # delete service state data
                t = oload::getTable("service_state_data");
                t.del(("serviceid": op_in(l)));

                # delete service config item data creating config items
                map oload::deleteItems("service", $1, True), l;

                t = oload::getTable("config_item_values");
                map t.del({"level": "service:" + $1}), l;

                # delete service tags
                t = oload::getTable("service_tags");
                t.del(("serviceid": op_in(l)));

                # delete service options
                t = oload::getTable("service_options");
                t.del({"serviceid": op_in(l)});

                # delete service entries
                t = oload::getTable("services");
                t.del(("serviceid": op_in(l)));

                if (oload.o.verbose) {
                    print("done\n");
                }

                # mark deleted system services for reset
                map oload::omqmap_reload.services{$1} = True, l;
            }
        }

        # issue #3751: check for python dependencies
        if (tags.lang == "python") {
            has_python = True;
        } else if (tags."class-name" && tags."code") {
            # only check class-based services
            hash<auto> python_dep_info;
            has_python = JavaClassHelper::hasPythonDependencies(tags."class-name", tags."code",
                cast<*list<string>>(tags.classes), \python_dep_info);
        }

        # issue #3485: get any FSM triggers from the fsm_triggers column
        *string fsm_triggers = getFsmTriggers(\tags);

        *hash<auto> q = sqlif.omqp.selectRow("select serviceid, autostart, manual_autostart, remote, manual_remote from "
            "services where name = %v and version = %v and service_type = %v", tags{"name"}, tags{"version"},
            tags{"servicetype"});

        *softint serviceid = q.serviceid;

        # the following tags are unsupported in the Community Edition
        remove tags{
            "scaling-min-replicas", "scaling-max-replicas", "scaling-cpu", "scaling-memory",
            "container-cpu-request", "container-memory-request", "container-cpu-limit", "container-memory-limit"
        };

        if (serviceid) { # update existing service definition
            q.autostart = boolean(q.autostart);
            # should we ignore the autostart flag (if it's been updated manually already)
            bool ia = q.manual_autostart && boolean(tags{"autostart"}) != q.autostart;

            if (ia && oload.o.override) {
                oload::warning("overriding autostart value %y for service %s", q.autostart, tags{"name"});
                ia = False;
            }

            # should we ignore the remote value (if it's been updated manually already)
            bool ignore_remote = q.manual_remote && tags{"remote"} != q.remote;

            if (ignore_remote && oload.o.override) {
                oload::warning("overriding remote value %y for service %s", q.remote, tags{"name"});
                ignore_remote = False;
            }

            if (!oload.o.quiet) {
                if (oload.o.verbose) {
                    printf("%s service %s:%s (%d) remote: %y%s autostart: %y%s: updating\n", tags{"servicetype"},
                        tags{"name"}, tags{"version"}, serviceid,
                        tags{"remote"}, ignore_remote ? " (ignoring: manually updated)": "",
                        tags{"autostart"}, ia ? " (ignoring: manually updated)" : "");
                } else {
                    oload.mystat(".");
                }
            }
            # class source and language_info are updated if necessary when the service is finalized
            sqlif.omqp.exec("update services set patch=%v, description=%v, author=%v, autostart=%v, "
                "remote=%v, parse_options=%v, language=%v, language_info=NULL, class_source=NULL, class_name=%v, "
                "service_modules=%v, yaml_config_items=%v, yaml_fsm_triggers=%v, api_manager=%v, events=%v "
                "where serviceid=%v",
                tags{"patch"}, tags{"desc"}, tags{"author"}, ia ? int(q.autostart) : int(tags{"autostart"}),
                ignore_remote ? int(q.remote) : tags{"remote"}.toInt(), tags{"parse_options"}, tags{"lang"},
                tags{"class-name"}, service_modules_string, tags{"config-items"}, fsm_triggers, api_manager, events,
                serviceid);

            # delete all service tags
            sqlif.omqp.exec("delete from service_tags where serviceid = %v", serviceid);

            # delete service mappers when updating
            sqlif.omqp.exec("delete from service_mappers where serviceid = %v", serviceid);

            # delete service value maps when updating
            sqlif.omqp.exec("delete from service_vmaps where serviceid = %v", serviceid);

            # delete all service resources
            sqlif.omqp.exec("delete from service_file_resources where serviceid = %v", serviceid);

            # delete service authentication labels
            # do not delete all, only those which are no longer defined in QSD file
            deleteAuthLabels(tags{"authlabels"}, serviceid);
        } else {
            serviceid = get_next_sequence_value("seq_services");
            if (!oload.o.quiet) {
                if (oload.o.verbose) {
                    printf("%s service %s:%s (%d) remote: %y autostart: %y: inserting\n", tags{"servicetype"},
                        tags{"name"}, tags{"version"}, serviceid, tags{"remote"}, tags{"autostart"});
                } else {
                    oload.mystat("I");
                }
            }

            sqlif.omqp.exec("insert into services (serviceid, service_type, name, version, patch, description, "
                "author, parse_options, language, class_name, service_modules, autostart, manual_autostart, "
                "remote, yaml_config_items, yaml_fsm_triggers, api_manager, events) "
                "values (%v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, 0, %v, %v, %v, %v, %v)",
                serviceid, tags{"servicetype"}, tags{"name"}, tags{"version"}, tags{"patch"}, tags{"desc"},
                tags{"author"}, tags{"parse_options"}, tags{"lang"}, tags{"class-name"}, service_modules_string,
                int(tags{"autostart"}), int(tags{"remote"}), tags{"config-items"}, fsm_triggers, api_manager, events);

            # issue #3281: make entry in old config item value map for value migration
            if (tags.servicetype != "SYSTEM") {
                *softint old_id = oload.smap{tags.name}.serviceid;
                if (old_id) {
                    config_item_values = InterfaceConfigContainer::getInterfaceConfigItemValues("service:" + old_id);
                }
            }
            if (config_item_values) {
                oload.config_item_value_map.service{serviceid} = config_item_values;
            }

            # make entry in smap for new service
            oload.smap.(tags{"name"}) = {
                "version"   : tags{"version"},
                "patch"     : tags{"patch"},
                "serviceid" : serviceid,
                "created"   : now_us(),
                "type"      : tags{"servicetype"},
            };
        }

        # create / update service options
        oload.createInterfaceOptions("service", tags, NOTHING, serviceid);

        # save service in list of services created/updated
        oload::omqmap_reload.services{serviceid} = True;

        *list mappers = oload.getMapperList(tags{"mappers"});
        tags{"mappers"} = ();
        # insert any service mappers
        foreach hash mapper in (mappers) {
            sqlif.omqp.exec("insert into service_mappers (serviceid, mapperid) values (%v, %v)", serviceid,
                            mapper{"mapperid"});
            if (oload.o.verbose) {
                printf("%s service %s:%s (%d) adding mapper %s v%s (%d)\n", tags{"servicetype"}, tags{"name"},
                       tags{"version"}, serviceid, mapper{"name"}, mapper{"version"}, mapper{"mapperid"});
            } else {
                oload.mystat("I");
            }

            # save mappers for release hash
            tags{"mappers"} += mapper.("name", "version");
        }

        *list<auto> value_maps = oload.getVMapList(tags{"vmaps"});
        tags{"vmaps"} = ();

        # insert any service value maps
        foreach hash<auto> value_map in (value_maps) {
            sqlif.omqp.exec("insert into service_vmaps (serviceid, id) values (%v, %v)", serviceid, value_map{"id"});
            if (oload.o.verbose) {
                printf("%s service %s:%s (%d) adding value map %s (%d)\n", tags{"servicetype"}, tags{"name"},
                       tags{"version"}, serviceid, value_map{"name"}, value_map{"id"});
            } else {
                oload.mystat("I");
            }

            # save value maps for release hash
            tags{"vmaps"} += value_map.("name",);
        }

        # create audit msg
        oload.audit.sourceLoadedEvent(NOTHING, NOTHING, NOTHING, serviceid, file_name, "service", tags{"name"},
            tags{"version"}, serviceid);

        oload.valsh{serviceid} = {
            "id": serviceid,
            "type": tags{"servicetype"}.lwr(),
            "name": tags{"name"},
            "version": tags{"version"},
            "has_python": has_python,
        };

        # insert tags
        oload.insertTags("service", tags{"name"}, serviceid, user_tags);

        # save tag info for release
        tags{"tags"} = user_tags;

        # process and load resources
        if (tags{"resource"} || tags{"template"} || tags{"text-resource"} || tags{"bin-resource"}) {
            string dir = dirname(file_name);
            processResources(tags, serviceid, dir, tags{"resource"}, "A");
            processResources(tags, serviceid, dir, tags{"template"}, "T");
            processResources(tags, serviceid, dir, tags{"text-resource"}, "N");
            processResources(tags, serviceid, dir, tags{"bin-resource"}, "B");
        }

        # save authentication labels
        map processAuthLabel($1.key, $1.value, serviceid), tags{"authlabels"}.pairIterator();

        # now delete all service methods in case there are some methods that no longer belong in this service
        sqlif.omqp.exec("delete from service_method_tags where service_methodid in (select service_methodid from "
            "service_methods where serviceid = %v)", serviceid);

        sqlif.omqp.exec("delete from service_methods where serviceid = %v", serviceid);

        DbInserter::insertLibraries("service", serviceid, tags);
        return serviceid;
    }

    softstring insertImpl(reference<hash<string, hash<auto>>> ohash, hash<auto> tags, hash<auto> user_tags,
            string file_name, code post_insert, *ReleaseFile release_file) {
        int id = insertService(tags, user_tags, file_name);

        hash<auto> release_method_info;

        foreach auto method in (tags{"methods"}) {
            insertMethodInternal(tags, method{"user_tags"} ?? user_tags, id, method, file_name);
            if (tags{"class-based"} == False) {
                release_method_info{method{"name"}} = method - ("user_tags",) + {"source": get_hash(method{"code"})};

                tags{"code"} += method{"code"};
            }
        }

        tags{"methods"} = release_method_info;
        tags{"tags"} = user_tags;
        finalize(tags, user_tags, id, release_file);
        return id;
    }

    private:internal finalize(hash<auto> tags, hash<auto> user_tags, int serviceid, *ReleaseFile release_file) {
        if (tags{"class-based"}) {
            # insert the class source in the services table
            *string lang_info_str;
            if (tags{"lang"} == "java") {
                # we need to compile any Java source to byte code for insertion in the DB
                lang_info_str = JavaClassHelper::compileJavaSource("service", tags{"class-name"} ?? tags{"name"},
                    tags{"version"}, tags{"code"}, tags{"tags"},
                    tags + {
                        "serviceid": serviceid,
                        "user_tags": user_tags,
                    }, cast<*list<string>>(tags{"classes"}));
            }

            AbstractTable services = oload::getTable("services");

            {
                on_error services.rollback();
                on_success services.commit();

                hash<auto> uh = {
                    "class_source": tags{"code"},
                    "language_info": lang_info_str,
                };

                services.update(uh, {"serviceid": serviceid});
            }

            # issue #3571: if a service has a Python dependency, then validate it in another process
            # in order to ensure that all modules can be loaded; some Python modules can only be loaded
            # once per process
            if (!has_python) {
                ServiceLoader service(tags{"servicetype"}, tags{"name"}, tags{"version"}, serviceid,
                                    user_tags, tags{"author"});
            } else {
                if (!oload.o.quiet && oload.o.verbose) {
                    printf("%s service %s v%s: validating externally due to dynamic Python dependency\n",
                        tags.servicetype, tags.name, tags.version);
                }

                ExternalProcessHelper proc("SERVICE-VALIDATION-ERROR", "--validate-service=-");
                hash<auto> opt = {
                    "id": serviceid,
                    "type": tags.servicetype,
                    "name": tags.name,
                    "version": tags.version,
                };
                proc.write(opt);
                # wait for process to complete
                proc.wait();
            }
            if (!oload.o.quiet) {
                if (oload.o.verbose) {
                    printf("validated %s service %s/%s (%d) on load\n", tags{"servicetype"}, tags{"name"},
                        tags{"version"}, serviceid);
                } else {
                    oload.mystat("V");
                }
            }

            # remove service from validation hash
            remove oload.valsh{serviceid};
            # make sure that service does not get validated again
            oload.already_validated_map.service{serviceid} = True;
        }

        # add release component
        if (release_file) {
            hash<auto> sh = tags + (
                "source": get_hash(tags{"code"}),
            );
            string source = sprintf("%y", sh);
            release_file.add(tags{"name"}, source, tags{"version"}, serviceid);
        }
    }

    static insertMethod(string type, string name, string version, int serviceid, hash<auto> tags,
                        *hash<auto> user_tags) {
        # otherwise insert new method
        softint service_methodid = get_next_sequence_value("seq_service_methods");
        if (oload.o.verbose) {
            printf("%s service %s:%s (%d) method %s (%d) inserting (lock: %s)\n", type, name, version, serviceid,
                   tags{"name"}, service_methodid, tags{"lock"});
        } else {
            oload.mystat("I");
        }
        sqlif.omqp.exec("insert into service_methods (service_methodid, serviceid, name, description, author, "
                        "locktype, internal, writeflag, body) values (%v, %v, %v, %v, %v, %v, %v, %v, %v)",
                        service_methodid, serviceid, tags{"name"}, tags{"desc"}, tags{"author"}, tags{"lock"},
                        int(tags{"internal"}), int(tags{"write"}), tags{"code"});

        # insert tags
        oload.insertTags("service_method", tags{"name"}, service_methodid, user_tags);
    }

    private processResources(hash tags, int serviceid, string dir, softlist l, string type) {
        foreach string rn in (l) {
            if (rn =~ /[\*\?]/) {
                doResourceGlob(tags, serviceid, dir, rn, type);
            } else {
                insertResourceFile(tags, serviceid, dir, rn, type);
            }
        }
    }

    private deleteAuthLabels(*hash<auto> current_labels, int serviceid) {
        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        # if there are no current labels, delete all existing auth labels for the service
        if (!current_labels) {
            int rows = sqlif.omqp.exec("delete from service_auth_labels where serviceid = %v", serviceid);
            if (rows) {
                oload::omqmap_reload.services{serviceid} = True;
            }
            return;
        }

        # otherwise delete all except current labels
        AbstractTable service_auth_labels = oload::getTable("service_auth_labels");
        int rows = service_auth_labels.del({
            "serviceid": serviceid,
            "authlabelid": op_not(op_in(keys current_labels)),
        });
        if (rows) {
            oload::omqmap_reload.services{serviceid} = True;
        }
    }

    private processAuthLabel(string label, string value, int serviceid) {
        if (!AUTH_LABEL_VALUES{value}) {
            throw "AUTH-ERROR", sprintf("Authentication label value \"%s\" not found",value);
        }

        *string val = sqlif.omqp.selectRow("select value from service_auth_labels where authlabelid = %v and serviceid = %v",
            label, serviceid).value;

        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        if (!val) {
            string inSql = "insert into service_auth_labels (authlabelid, serviceid, value, created, modified) values (%v, %v, %v, %v, %v)";
            sqlif.omqp.exec(inSql, label, serviceid, value, now_us(), now_us());
        } else {
            if (oload.o.override) {
                if (val != value) {
                    oload::warning("overriding authentication label %y = %y for service %y", label, value, serviceid);
                    sqlif.omqp.exec("update service_auth_labels set value = %v where serviceid = %v and authlabelid = %v",
                        value, serviceid, label);
                }
            } else {
                if (val != value) {
                   oload::warning("authentication label %y already exists for service %y; not updated", label, serviceid);
                }
            }
        }
    }

    private doResourceGlob(hash tags, int serviceid, string dir, string rn, string type) {
        foreach string fstr in (glob(dir + DirSep + rn)) {
            # skip files ending in "~" as backup files
            if (fstr =~ /~$/) {
                continue;
            }
            string ftype = hstat(fstr).type;
            if (ftype == "DIRECTORY") {
                doResourceGlob(tags, serviceid, dir, fstr.substr(dir.size() + 1) + DirSep + basename(rn), type);
                continue;
            }
            # skip special files, sockets, device files, etc
            if (ftype != "REGULAR") {
                continue;
            }
            insertResourceFile(tags, serviceid, dir, fstr.substr(dir.length() + 1), type);
        }
    }

    private insertResourceFile(hash<auto> tags, int serviceid, string dir, string rn, string type) {
        string rpath = dir + DirSep + rn;
        try {
            # try to auto-detect type
            if (type == "A") {
                # get extension - only the part after the very last dot
                *string ext = (rn =~ x/\.([^\.]+)$/)[0];
                if (ext) {
                    ext = ext.lwr();
                    if (oload::TemplateFileTypes{ext}) {
                        type = "T";
                    } else if (oload::TextFileTypes{ext}) {
                        type = "N";
                    } else {
                        # try to guess from MIME type
                        string mt = get_mime_type_from_ext(rn);
                        type = (mt =~ /^text\//) ? "N" : "B";
                    }
                } else {
                    type = "B";
                }
            }
            ReadOnlyFile f(rpath);
            # NOTE: ReadOnlyFile::read*(-1) returns NOTHING if the file is empty
            data body = f.readBinary(-1) ?? binary();
            sqlif.omqp.exec("insert into service_file_resources (serviceid, name, resource_type, body) values (%v, %v, %v, %v)",
                            serviceid, rn, type, body);

            # add resource info to info hash
            tags{"resources"}{rn} = (
                "type": type,
                "hash": get_hash(body),
            );

            if (oload.o.verbose) {
                printf("%s service %s:%s (%d) file resource (%s) %s: inserting %d bytes\n", tags{"servicetype"},
                       tags{"name"}, tags{"version"}, serviceid, type, rn, body.size());
            } else {
                oload.mystat("I");
            }
        } catch (hash<ExceptionInfo> ex) {
            oload.error("resource (%s) %y cannot be found as %y: %s: %s", type, rn, rpath, ex.err, ex.desc);
        }
    }
}

class JobDbInserter inherits DbInserter {
    softstring insertImpl(reference<hash<string, hash<auto>>> ohash, hash<auto> tags, hash<auto> user_tags,
            string file_name, code post_insert, *ReleaseFile release_file) {
        oload.registerObject("job", tags{"name"}, tags{"version"}, file_name);

        if (!exists tags{"remote"}) {
            tags{"remote"} = oload.coptions.remote;
        }

        # language info, if any
        *string lang_info;
        if (tags{"lang"} == "java") {
            # we need to compile any Java source to byte code for insertion in the DB
            lang_info = JavaClassHelper::compileJavaSource("job", tags{"name"}, tags{"version"}, tags{"code"},
                user_tags, tags, cast<*list<string>>(tags{"classes"}));
        }

        # prepare job_modules value
        string job_modules_string;
        if (tags{"modules"}) {
            job_modules_string = tags{"modules"}.typeCode() == NT_STRING ? tags{"modules"}
                                                                         : tags{"modules"}.join(",");
        }

        if (exists tags{"schedule"}) {
            try {
                CronTrigger cron_trigger(tags{"schedule"}{"minutes"}, tags{"schedule"}{"hours"},
                                         tags{"schedule"}{"days"}, tags{"schedule"}{"months"},
                                         tags{"schedule"}{"dow"});
            } catch (hash<ExceptionInfo> ex) {
                oload::error("cannot parse cron timer input: %s: %s", ex.err, ex.desc);
            }
        } else if (!tags.duration.intp()) {
            oload::error("%s: job %y is missing both the \"schedule\" and \"duration\" tags; jobs must have one "
                "of these tags to be loaded", file_name, tags.name);
        }

        # issue #3751: check for python dependencies
        if (tags.lang == "python") {
            has_python = True;
        } else if (tags."class-name" && tags."code") {
            # only check class-based services
            hash<auto> python_dep_info;
            has_python = JavaClassHelper::hasPythonDependencies(tags."class-name", tags."code",
                cast<*list<string>>(tags.classes), \python_dep_info);
        }

        # issue #3485: get any FSM triggers from the fsm_triggers column
        *string fsm_triggers = getFsmTriggers(\tags);

        softint jobid;

        # run in single transaction in case the object is rejected due to tags
        on_success sqlif.omqp.commit();
        on_error sqlif.omqp.rollback();

        # update job if it exists already
        *hash<auto> q = sqlif.omqp.selectRow("select jobid, active, manual_active, manually_updated, remote, "
            "manual_remote, month, day, wday, hour, minute, recurring, expiry_date from jobs where name = %v",
            tags{"name"});
        if (q.jobid) {
            # should we ignore the scheduling attributes (if already updated manually)
            bool change = (tags{"active"} != q.active
                || tags{"schedule"}{"months"} != q.month
                || tags{"schedule"}{"days"} != q.day || tags{"schedule"}{"hours"} != q.hour
                || tags{"schedule"}{"minutes"} != q.minute || tags{"schedule"}{"dow"} != q.wday
                || tags{"duration"} != q.recurring || tags{"expiry-date"} != q.expiry_date);
            bool ign = q.manually_updated && change;

            string action;
            if (ign) {
                if (oload.coptions."override-job-params" || oload.o.usesched || oload.o.override) {
                    ign = False;
                    action = "overriding job schedule params";
                } else {
                    action = "ignoring job schedule params";
                }
            } else {
                action = "updating job schedule params";
            }

            # should we ignore the remote value (if it's been updated manually already)
            bool ignore_remote = q.manual_remote && tags.remote != q.remote;

            # should we ignore the active value (if it's been updated manually already)
            bool ignore_active = q.manual_active && tags.active != q.active;

            if (ignore_remote && oload.o.override) {
                stderr.printf("WARNING: %s: overriding remote value %y for job %s\n", file_name, q.remote,
                              tags{"name"});
                ignore_remote = False;
            }
            if (ignore_active && oload.o.override) {
                stderr.printf("WARNING: %s: overriding active value %y for job %s\n", file_name, q.remote,
                              tags{"name"});
                ignore_active = False;
            }

            if (oload.o.verbose) {
                printf("job %s:%s (%d) updating: remote %y%s active: %y%s (%s)\n", tags{"name"}, tags{"version"},
                    q.jobid, tags.remote, ignore_remote ? " (ignoring: manually updated)" : "",
                    tags.active, ignore_active ? " (ignoring: manually updated)" : "", action);
            } else {
                oload.mystat(".");
            }

            if (!ign) {
                sqlif.omqp.exec("update jobs set version = %v, description = %v, author = %v, "
                    "active = %v, job_modules = %v, run_skipped = %v, code = %v, month = %v, day = %v, hour = %v, "
                    "minute = %v, wday = %v, recurring = %v, expiry_date = %v, class_based = %v, class_name = %v, "
                    "language = %v, language_info = %v, remote = %v, yaml_config_items = %v, yaml_fsm_triggers = %v "
                    "where jobid = %v",
                    tags{"version"}, tags{"desc"}, tags{"author"},
                    ignore_active ? int(q.active) : tags.active.toInt(), job_modules_string, int(tags{"run-skipped"}),
                    tags{"code"}, tags{"schedule"}{"months"}, tags{"schedule"}{"days"}, tags{"schedule"}{"hours"},
                    tags{"schedule"}{"minutes"}, tags{"schedule"}{"dow"}, tags{"duration"}, tags{"expiry-date"},
                    tags{"class-based"}.toInt(), tags{"class-name"}, tags{"lang"}, lang_info,
                    ignore_remote ? int(q.remote) : tags.remote.toInt(), tags{"config-items"}, fsm_triggers,
                    int(q.jobid));
            } else {
                sqlif.omqp.exec("update jobs set version = %v, description = %v, author = %v, "
                    "active = %v, job_modules = %v, run_skipped = %v, code = %v, class_based = %v, class_name = %v, "
                    "language = %v, language_info = %v, remote = %v, yaml_config_items = %v, yaml_fsm_triggers = %v "
                    "where jobid = %v",
                    tags{"version"}, tags{"desc"}, tags{"author"},
                    ignore_active ? int(q.active) : tags.active.toInt(), job_modules_string, int(tags{"run-skipped"}),
                    tags{"code"}, tags{"class-based"}.toInt(), tags{"class-name"}, tags{"lang"}, lang_info,
                    ignore_remote ? int(q.remote) : tags.remote.toInt(), tags{"config-items"}, fsm_triggers,
                    int(q.jobid));
            }

            jobid = q.jobid;

            # clear job tags
            sqlif.omqp.exec("delete from job_tags where jobid = %v", jobid);

            # delete job mappers when updating
            sqlif.omqp.exec("delete from job_mappers where jobid = %v", jobid);

            # delete job value maps when updating
            sqlif.omqp.exec("delete from job_vmaps where jobid = %v", jobid);
        } else {
            # otherwise insert new job
            jobid = get_next_sequence_value("seq_jobs");
            if (oload.o.verbose) {
                printf("job %s:%s (%d) inserting\n", tags{"name"}, tags{"version"}, jobid);
            } else {
                oload.mystat("I");
            }
            sqlif.omqp.exec("insert into jobs (jobid, name, version, description, author, active, "
                "job_modules, run_skipped, code, month, day, hour, minute, wday, recurring, expiry_date, class_based, "
                "class_name, language, language_info, remote, yaml_config_items, yaml_fsm_triggers) "
                "values (%v, %v, %v, %v, %v, %v, %v, %d, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, %v, "
                "%v)",
                jobid, tags{"name"}, tags{"version"}, tags{"desc"}, tags{"author"},
                int(tags{"active"}), job_modules_string, int(tags{"run-skipped"}), tags{"code"},
                tags{"schedule"}{"months"}, tags{"schedule"}{"days"}, tags{"schedule"}{"hours"},
                tags{"schedule"}{"minutes"}, tags{"schedule"}{"dow"}, tags{"duration"}, tags{"expiry-date"},
                tags{"class-based"}.toInt(), tags{"class-name"}, tags{"lang"}, lang_info, tags{"remote"}.toInt(),
                tags{"config-items"}, fsm_triggers);
        }

        # create / update job options
        oload.createInterfaceOptions("job", tags, NOTHING, jobid);

        # insert any job mappers
        *list mappers_list = oload.getMapperList(tags{"mappers"});
        foreach hash mapper in (mappers_list) {
            sqlif.omqp.exec("insert into job_mappers (jobid, mapperid) values (%v, %v)", jobid, mapper.mapperid);
            if (oload.o.verbose) {
                printf("job %s:%s (%d) adding mapper %s v%s (%d)\n", tags{"name"}, tags{"version"}, jobid, mapper.name,
                    mapper.version, mapper.mapperid);
            } else {
                oload.mystat("I");
            }
        }

        # insert any job value maps
        *list value_maps_list = oload.getVMapList(tags{"vmaps"});
        foreach hash<auto> value_map in (value_maps_list) {
            sqlif.omqp.exec("insert into job_vmaps (jobid, id) values (%v, %v)", jobid, value_map.id);
            if (oload.o.verbose) {
                printf("job %s:%s (%d) adding value map %s (%d)\n", tags{"name"}, tags{"version"}, jobid,
                       value_map.name, value_map.id);
            } else {
                oload.mystat("I");
            }
        }

        # create audit msg
        oload.audit.sourceLoadedEvent(NOTHING, NOTHING, jobid, NOTHING, file_name, "job", tags{"name"},
                                      tags{"version"}, jobid);

        # insert tags
        oload.insertTags("job", tags{"name"}, jobid, user_tags);

        # add release component
        if (release_file) {
           hash<auto> jh = tags + {
               "source": get_hash(tags{"code"}),
           };
           string src = sprintf("%y", jh);
           release_file.add(tags{"name"}, src, tags{"version"}, jobid);
        }

        # save job in list of created/updated jobs
        oload::omqmap_reload.jobs{jobid} = True;

        oload.valjh{jobid} = {
            "id": jobid,
            "name": tags{"name"},
            "version": tags{"version"},
            "has_python": has_python,
        };

        DbInserter::insertLibraries("job", jobid, tags);

        # issue #3705: NOTE: all jobs, including Java jobs, must be validated, as config item values will be processed
        # during the validation phase

        return jobid;
    }
}

class ClientMappers inherits OMQ::Mappers {
    private {
        bool init = False;
    }

    private nothing setupIntern(*list<auto> mml) {
        # add external connection handlers
        foreach string mod in (mml) {
            try {
                addMapperTypeProvider(mod);
            } catch (hash<ExceptionInfo> ex) {
                oload::error("failed to load user mapper provider module %y: %s: %s", mod, ex.err, ex.desc);
            }
        }
    }

    private nothing reloadIntern(*softlist ids) {
        mh = new hash<string, MapperContainerBase>();
    }

    ClientMapperContainer loadMapper(MapperValidator val, softint mapperid) {
        if (!init) {
            init = True;
            Mappers::init(NOTHING, oload.options."mapper-modules");
        }

        *hash lmh = sqlif.omqp.selectRow("select * from mappers where mapperid = %v", mapperid);
        if (!lmh)
            oload::error("cannot load mapperid %d; mapper does not exist", mapperid);

        ClientMapperContainer m;
        try {
            *AbstractMapperType mt = sh.(lmh.type);
            if (!mt)
                throw "MAPPER-ERROR", sprintf("unknown mapper type %y; known mappers: %y", lmh.type, sh.keys());
            m = new ClientMapperContainer(val, lmh.mapperid, lmh.name, lmh.version, lmh.patch, lmh.description, mt,
                                          lmh.parse_options, lmh.fields, lmh.options, lmh.created, lmh.modified,
                                          lmh.is_yaml.toBool());
        } catch (hash<ExceptionInfo> ex) {
            if (oload.o.verbose) {
                printf("%s\n", get_exception_string(ex));
            }
            oload::error("failed to register mapper %y v%s (%d) %s: %s: %s", lmh.name, lmh.version, lmh.mapperid, get_ex_pos(ex), ex.err, ex.desc);
        }

        return m;
    }
}

# this helper class is used to provide an access to MapperValidator
# in the MapperContainerBase constructor
class MapperValidationContainer {
    public {
        MapperValidator val;
    }

    constructor(MapperValidator vl) {
        val = vl;
    }
}

class ClientMapperProgram inherits MapperProgram {
    constructor(int po, *hash<auto> opts, *hash<auto> defs, list<string> ml = CommonModuleList) : MapperProgram(po, opts, defs, ml) {
    }

    importMapperApi() {
        disableParseOptions(PO_NO_MODULES);
        qorus_load_fake_mapper_api_module(self);
        setParseOptions(PO_NO_MODULES);
    }
}

class ClientMapperContainer inherits MapperValidationContainer, MapperContainerBase {
    public {
        const InputOptions = (
            "input",
            "mapper-input",
            "input-type",
            "input-factory",
            "input-connection",
            "input-datasource",
        );

        const OutputOptions = (
            "output",
            "mapper-output",
            "output-type",
            "output-factory",
            "output-connection",
            "output-datasource",
        );
    }

    private {
        MapperProgram pgm;
    }

    constructor(MapperValidator vl, softint mid, string n, string v, *string pt, *string d, AbstractMapperType m,
                *string parse_options, string fields, *string options, date c, date mod, bool is_yaml)
        : MapperValidationContainer(vl),
          MapperContainerBase(mid, n, v, pt, d, m, parse_options, fields, options, oload.options.defines, c, mod,
                              is_yaml) {
        if (options) {
            try {
                *hash topt = exec("options");
                if (mtype.requiresInput() && !topt{InputOptions}) {
                    error("failed to initialize mapper; missing one of the required options: %y options in mapper "
                        "describing the input record", InputOptions);
                }
                if (mtype.requiresOutput() && !topt{OutputOptions}) {
                    error("failed to initialize mapper; missing one of the required options: %y options in mapper "
                        "describing the output record", OutputOptions);
                }
            } catch (hash<ExceptionInfo> ex) {
                error(ex.desc);
            }
        }
    }

    nothing error(string fmt) {
        throw "MAPPER-ERROR", vsprintf(fmt, argv);
    }

    MapperProgram setupProgramImpl(int po, *hash props) {
        return pgm = new ClientMapperProgram(po, qorus_get_system_options(), props);
    }

    Program getProgram() {
        return pgm;
    }

    private loadLibraryImpl(MapperProgram p) {
        val.loadValidatorLibrary(sqlif.omqp.select("select * from mapper_lib where mapperid = %v order by load_order",
            mapperid), p);
    }

    private parseCommitImpl() {
        val.addWarnings(p.parseCommit(oload.warningMask));
    }
}

# dummy APIs for service code
sub slog(int ll, string fmt) { delete argv; }
sub slog_args(softlist<auto> args) {}
sub sqllog(string ds, string msg) { delete argv; }
sub osqllog(string ds, string msg) { delete argv; }
# dummy Service class for AbstractServiceHttpHandler
class OMQ::Service { memberGate(string m) {} methodGate(string m) {} }

# dummy classes for the fake workflow API
namespace OMQ {
    namespace UserApi {
        namespace Workflow {
            class WorkflowDataHelper {
                public {}
                any get(any field) {}
                nothing replace(*hash new_data) {}
                nothing update(hash new_data) {}
                nothing deleteKey(softlist keysv) {}
            }
            final class DynamicDataHelper inherits WorkflowDataHelper {}
            final class TempDataHelper inherits WorkflowDataHelper {}
            final class StepDataHelper inherits WorkflowDataHelper {}
            final class SensitiveDataHelper{
                public {}
                auto get(string skey, string svalue, auto field) {}
                hash getFromAlias(string alias, auto field) {return {};}
                hash getMetadata(string skey, string svalue) {return {};}
                hash getMetadataFromAlias(string alias) {return {};}
                hash getAliases() {return {};}
                hash getKeyValues() {return {};}
                nothing replace(string skey, string svalue, *hash new_data, *softlist aliases, *hash meta) {}
                nothing replaceFromAlias(string alias, *hash new_data, *hash meta) {}
                nothing replaceMulti(hash<string, hash<string, hash<SensitiveDataInfo>>> sinfo) {}
                nothing update(string skey, string svalue, *hash new_data, *softlist aliases, *hash meta) {}
                nothing updateFromAlias(string alias, *hash new_data, *hash meta) {}
                bool deleteKey(string skey, string svalue, softlist keysv) {return False;}
                bool del(string skey, string svalue) {return False;}
            }
        }
    }
}
class QdspClient inherits DatasourcePool;
class QdspStatement inherits SQLStatement;

public class UserConnectionFileHelper inherits AbstractConnectionFileHelper, OMQ::Connections {
    #! returns \c "user"
    string getType() {
        return "user";
    }

    #! creates the object using the supplied @ref Qore::SQL::AbstractDatasource object for querying the system schema
    constructor(AbstractDatasource ds) :
        AbstractConnectionFileHelper(qorus_get_system_option("defines")),
        Connections(new Table(ds, "connections").getTable(), new Table(ds, "connection_tags").getTable(),
            sub (string fmt) {
                if (oload.o.verbose && !oload.o.quiet) {
                    vprintf(fmt + "\n", argv);
                }
            },
            qorus_get_system_option("connection-modules")) {
    }

    logInfo(string msg) {
        if (oload.o.verbose) {
            stdout.vprintf(msg + "\n", argv);
        }
    }

    logDebug(string msg) {
        if (oload.o.verbose > 2) {
            stdout.vprintf(msg + "\n", argv);
        }
    }

    logSystem(string name, string fmt) {
        if (oload.o.verbose > 2) {
            softlist<auto> args += (name, fmt);
            if (argv) {
                args += argv;
            }
            call_function_args(\Connections::logSystem(), args);
        }
    }

    string getConnectionDbType() {
        return "OLOAD"; # just to silence abstract method error
    }
    string getErrorCode() {
        return "OLOAD-CONNECTION-ERROR";
    }

    #! returns an iterator to the @ref OMQ::AbstractConnection objects managed by this class
    HashIterator getDataIterator() {
        return data.iterator();
    }

    #! creates a new connection in the database
    insertConnection(AbstractConnection c) {
        Connections::insertDbConnectionIntern(c);
        if (!oload.o.verbose && !oload.o.quiet) {
            oload.mystat("I");
        }
    }

    #! updates an existing connection in the database
    updateConnection(AbstractConnection c, bool set_updated) {
        Connections::updateConnection(c, set_updated);
        if (!oload.o.verbose && !oload.o.quiet) {
            oload.mystat("U");
        }
    }

    #! returns the given connection object
    /*  @par Example:
        @code
my object $o = $ucf.get($name);
        @endcode

        @param name the name of the connection to return
        @param connect if @ref True "True" then the connection should be returned connected
        @param rtopts if the object acquisition requires any runtime options, then these are passed here

        @throw CONNECTION-ERROR the given connection does not exist
    */
    object get(string name, bool connect = True, *hash rtopts) {
        *AbstractConnection c = data{name};
        if (!c)
            throw "CONNECTION-ERROR", sprintf("cannot retrieve unknown user connection %y; known user connections: %y", name, data.keys());

        return c.get(connect, rtopts);
    }

    private nothing setupIntern(*list cml) {
        # add external connection handlers
        foreach string mod in (cml) {
            try {
                addConnectionProvider(mod);
            } catch (hash<ExceptionInfo> ex) {
                stderr.printf("WARNING: failed to load user connection provider module %y: %s: %s", mod, ex.err, ex.desc);
            }
        }
    }

    private nothing checkKeyValues(string fname, string n, reference h) {
    }

    private nothing parseImpl(string fname, *bool no_path) {
        hash<auto> dh = parseIntern(fname, no_path);

        foreach string conn_name in (keys dh) {
            hash<auto> h = dh{conn_name};

            string desc = h.desc ? remove h.desc : DefaultDescription;
            string url = remove h.url;

            hash<auto> attributes = {};
            if (exists h.tags) {
                attributes.tags = parse_to_qore_value(remove h.tags);
            }

            # create Connection object
            try {
                data{conn_name} = newConnection(conn_name, desc, url, attributes, h);
                data{conn_name}.parseTextOptions();
            } catch (hash<ExceptionInfo> ex) {
                printf("cannot register connection %y (%s) url: %y: %s: %s\n", conn_name, desc, url, ex.err, ex.desc);
            }
        }
    }
}

class UserSchemaHelper {
    private {
        AbstractSchema o;
        string dsdesc;
        string m_dsn;
    }

    constructor(string mn) {
        # load module
        Program pgm();
        # ensure old style for embedded Programs
        pgm.disableParseOptions(PO_NEW_STYLE);
        pgm.loadModule(mn);

        # get datasource name
        m_dsn = pgm.callFunction("get_datasource_name");

        # get datasource
        Datasource ds = omqclient.getDatasource(m_dsn);

        # assign description
        dsdesc = AbstractSqlUtilBase::makeDatasourceDesc(ds);

        # get schema object
        o = pgm.callFunction("get_user_schema", ds, oload.o.dts{m_dsn}, oload.o.its{m_dsn});
    }

    any resetSqlCache() {
        return qrest.put("system/sqlcache?action=clearCache&datasource=omquser");
    }

    # returns a string of the schema source; can be used to make a unique hash of the schema
    string getSource() {
        hash<auto> sh = {
            "name": self.getName(),
            "version": self.getVersion(),
            "column_options": self.getColumnOptions(),
            "creation_options": self.getCreationOptions(),
            "index_options": self.getIndexOptions(),
            "create_only_refdata": self.getCreateOnlyReferenceData(),
            "insert_only_refdata": self.getInsertOnlyReferenceData(),
            "refdata": self.getReferenceDataHash(),
            "strict_refdata": self.getStrictReferenceDataHash(),
            "functions": self.getFunctions(),
            "materialized_views": self.getMaterializedViews(),
            "packages": self.getPackages(),
            "procs": self.getProcedures(),
            "seqs": self.getSequences(),
            "tables": self.getTables(),
            "types": self.getTypes(),
        };
        return sprintf("%y\n", sh);
    }

    string getDesc() {
        return dsdesc;
    }

    any methodGate(string meth) {
        return call_object_method_args(o, meth, argv);
    }
}

bool sub my_is_executable(string fn) {
%ifdef HAVE_IS_EXECUTABLE
    return is_executable(fn);
%else
    return is_readable(fn);
%endif
}

string sub get_hash(data d) {
%ifdef HAVE_SHA512
    return d.toSHA512();
%else
    return d.toSHA1();
%endif
}

class HashBase inherits Serializable {
    private {
        string name;
        string hash_type;
        string hash_value;
    }

    constructor(string n, string src) {
        name = n;
%ifdef HAVE_SHA512
        hash_type = "SHA512";
        hash_value = src.toSHA512();
%else
        hash_type = "SHA1";
        hash_value = src.toSHA1();
%endif
    }

    string getHash() {
        return hash_value;
    }
}

class FileComponent inherits HashBase, Serializable {
    public {
    }

    private {
        *string ver;
        *int id;
    }

    constructor(string n, string src, *string v, *int i)  : HashBase(n, src) {
        ver = v;
        id = i;
    }

    commit(string rname, string fname, string ftype, AbstractTable release_file_contents) {
        #if (oload.o.verbose)
        #    printf(" - %s%s%s (%s)\n", name, ver ? sprintf(" v%s", ver) : "", id ? sprintf(" (id %d)", id) : "", ftype);

        release_file_contents.insert(("release_name": rname,
                                      "file_name": fname,
                                      "component": name,
                                      "component_version": ver,
                                      "component_id": id,
                                      "hash_type": hash_type,
                                      "hash": hash_value));
    }
}

class ReleaseFile inherits HashBase, Serializable {
    public {
    }

    private {
        string type;
        bool in_db;

        list<FileComponent> contents();
    }

    constructor(string n, string t, bool i, string src) : HashBase(n, src) {
        type = t;
        in_db = i;
    }

    FileComponent add(string name, string src, *string version, *int id) {
        FileComponent c(name, src, version, id);
        contents += c;
        return c;
    }

    commit(string rname, AbstractTable release_files, AbstractTable release_file_contents, reference cc) {
        release_files.insert(("release_name": rname,
                              "file_name": name,
                              "file_type": type,
                              "in_db": in_db.toInt(),
                              "hash_type": hash_type,
                              "hash": hash_value));

        map $1.commit(rname, name, type, release_file_contents), contents;

        cc += contents.size();
    }
}

class Release inherits Serializable {
    public {
    }

    private {
        # release name
        string name;

        # list of ReleaseComponent objects
        list<ReleaseFile> contents();
    }

    constructor(string n) {
        name = n;
    }

    ReleaseFile add(string name, string type, bool in_db, string src) {
        ReleaseFile f(name, type, in_db, src);
        contents += f;
        return f;
    }

    commit() {
        AbstractTable releases = oload::getTable("releases");
        AbstractTable release_files = oload::getTable("release_files");
        AbstractTable release_file_contents = oload::getTable("release_file_contents");

        on_success releases.commit();
        on_error releases.rollback();

        int rc = releases.upsert(("release_name": name, "os_user": getusername(), "os_host": gethostname(), "modified": now_us()));
        bool created = False;
        if (rc == AbstractTable::UR_Updated || rc == AbstractTable::UR_Verified) {
            # clear the release detail tables if a release is being reinstalled
            release_file_contents.del(("release_name": name));
            release_files.del(("release_name": name));
        } else {
            created = True;
        }

        int cc = 0;
        map $1.commit(name, release_files, release_file_contents, \cc), contents;

        if (oload.o.verbose)
            printf("%s release %y with %s file%s, %d component%s\n", created ? "created" : "updated", name,
            contents.size(), contents.size() == 1 ? "" : "s", cc, cc == 1 ? "" : "s");
    }
}

#! checks if the given module is loaded and returns the feature name
string sub qorus_check_module(string mod, reference<bool> loaded) {
    # get feature name
    string feature_name = (mod =~ /\.qm$/)
        ? basename_ext(mod, ".qm")
        : mod;
    loaded = exists get_module_hash(){feature_name};
    return feature_name;
}

sub qorus_load_fake_universal_api_module(QorusProgram p) {
    qorus_load_fake_workflow_api_module();
    qorus_load_fake_service_api_module();
    qorus_load_fake_job_api_module();
    p.loadModule("QorusFakeApiCommon");
    p.loadModule("QorusFakeApiWorkflow");
    p.loadModule("QorusFakeApiService");
    p.loadModule("QorusFakeApiJob");
%ifndef NO_JNI
    p.issueModuleCmd("jni", "mark-module-injected QorusFakeApiCommon");
    p.issueModuleCmd("jni", "mark-module-injected QorusFakeApiJob");
    p.issueModuleCmd("jni", "mark-module-injected QorusFakeApiService");
    p.issueModuleCmd("jni", "mark-module-injected QorusFakeApiWorkflow");
%endif
}

#! loads the fake service API module
sub qorus_load_fake_service_api_module(Program p) {
    qorus_load_fake_service_api_module();
    p.loadModule("QorusFakeApiCommon");
    p.loadModule("QorusFakeApiService");
%ifndef NO_JNI
    p.issueModuleCmd("jni", "mark-module-injected QorusFakeApiCommon");
    p.issueModuleCmd("jni", "mark-module-injected QorusFakeApiService");
%endif
}
sub qorus_load_fake_service_api_module() {
    qorus_load_fake_api_module("QorusFakeApiService");
}

sub qorus_load_fake_workflow_api_module(Program p) {
    qorus_load_fake_workflow_api_module();
    p.loadModule("QorusFakeApiCommon");
    p.loadModule("QorusFakeApiWorkflow");
%ifndef NO_JNI
    p.issueModuleCmd("jni", "mark-module-injected QorusFakeApiCommon");
    p.issueModuleCmd("jni", "mark-module-injected QorusFakeApiWorkflow");
%endif
}
sub qorus_load_fake_workflow_api_module() {
    qorus_load_fake_api_module("QorusFakeApiWorkflow");
}

sub qorus_load_fake_job_api_module(Program p) {
    qorus_load_fake_job_api_module();
    p.loadModule("QorusFakeApiCommon");
    p.loadModule("QorusFakeApiJob");
%ifndef NO_JNI
    p.issueModuleCmd("jni", "mark-module-injected QorusFakeApiCommon");
    p.issueModuleCmd("jni", "mark-module-injected QorusFakeApiJob");
%endif
}
sub qorus_load_fake_job_api_module() {
    qorus_load_fake_api_module("QorusFakeApiJob");
}

sub qorus_load_fake_mapper_api_module(Program p) {
    qorus_load_fake_mapper_api_module();
    #p.loadModule("QorusFakeApiCommon");
    p.loadModule("QorusFakeApiMapper");
%ifndef NO_JNI
    p.issueModuleCmd("jni", "mark-module-injected QorusFakeApiMapper");
%endif
}
sub qorus_load_fake_mapper_api_module() {
    qorus_load_fake_api_module("QorusFakeApiMapper");
}

#! loads a fake interface API module into the current program
sub qorus_load_fake_api_module(Program p, string name) {
    qorus_load_fake_api_module(name);
    p.loadModule(name);
%ifndef NO_JNI
    p.issueModuleCmd("jni", "mark-module-injected " + name);
%endif
}

#! loads fake interface API modules
sub qorus_load_fake_api_module(string name) {
    qorus_load_fake_api_module_intern("QorusFakeApiCommon");
    qorus_load_fake_api_module_intern(name);
}

#! loads a fake API module
sub qorus_load_fake_api_module_intern(string name) {
    if (get_module_hash(){name}) {
        return;
    }
    Program pgm(PO_NO_USER_API | PO_NO_THREAD_CONTROL | PO_NO_PROCESS_CONTROL);
    pgm.define("QORUS_FAKE_API_MODULE", True);
    pgm.loadApplyToUserModule(name);
}

#! loads the given module and returns the module name to cover the case when the argument is an absolute path
/** requires injection; cannot be in the client module
*/
string sub qorus_load_interface_module(string mod) {
    bool loaded;
    string feature_name = qorus_check_module(mod, \loaded);
    if (!loaded) {
        InterfaceModuleProgram pgm(PO_NO_USER_API);

        # load fake APIs for all interfaces
        qorus_load_fake_api_module(pgm, "QorusFakeApiCommon");
        qorus_load_fake_api_module(pgm, "QorusFakeApiWorkflow");
        qorus_load_fake_api_module(pgm, "QorusFakeApiService");
        qorus_load_fake_api_module(pgm, "QorusFakeApiJob");

        # import omqservice object to program
        pgm.importGlobalVariable("omqservice");

        # load the module into the Program container
        pgm.loadApplyToUserModule(mod);
    }

    return feature_name;
}

#! loads the given module and returns the module name to cover the case when the argument is an absolute path
/** requires injection; cannot be in the client module
*/
string sub qorus_load_workflow_module(string mod) {
    return qorus_load_interface_module(mod);
}

#! loads the given module and returns the module name to cover the case when the argument is an absolute path
/** requires injection; cannot be in the client module
*/
string sub qorus_load_service_module(string mod) {
    return qorus_load_interface_module(mod);
}

#! loads the given module and returns the module name to cover the case when the argument is an absolute path
/** requires injection; cannot be in the client module
*/
string sub qorus_load_job_module(string mod) {
    return qorus_load_interface_module(mod);
}

hashdecl StepDependencyListEntry {
    # a hash of the step iteself
    hash<auto> step;
    # if present, a step that must be COMPLETE before "step" can start
    *hash<auto> dep;
}

class ExternalProcessHelper {
    public {
    }

    private {
        StringOutputStream output();
        object proc;

        string err;
    }

    constructor(string err) {
        self.err = err;

        softlist<string> args = argv;
        if (oload.o.verbose) {
            args += "-" + strmul("v", oload.o.verbose);
        } else if (oload.o.quiet) {
            args += "-q";
        }
        if (oload.o.token) {
            args += "-t=" + oload.o.token;
        }
        if (oload.o.url) {
            args += "-u=" + oload.o.url;
        }
        if (oload.o.force) {
            args += "-f";
        }
        if (oload.o.override) {
            args += "-O";
        }
        if (oload.o.showfullerror) {
            args += "-w";
        }
        if (oload.o.redef) {
            args += "-A";
        }
        if (oload.o.refresh) {
            args += "-r";
        }
        if (oload.o.validate) {
            args += "-l";
        }
        if (oload.o.reload) {
            args += "-R";
        }

        if (oload.o.verbose > 2) {
            printf("running oload: %s %y\n", QORE_ARGV[1], args);
        }

        load_module("process");
        proc = create_object("Process", QORE_ARGV[1], args, {
            "stdout": output,
            "stderr": output,
        });
    }

    write(auto val) {
        proc.write(Serializable::serialize(val));
    }

    wait() {
        # wait for process to complete
        try {
            proc.wait();
        } catch (hash<ExceptionInfo> ex) {
            printf("error waiting for process to complete: %s: %s\n", ex.err, ex.desc);
        }
        int rc = proc.exitCode();
        output.reassignThread();
        if (rc && rc != -1) {
            throw err, sprintf("exit code %d: %s", rc, trim(output.getData()));
        }
        printf("%s", output.getData());
    }

}

*list<auto> sub get_stack() {
    if (!HAVE_RUNTIME_THREAD_STACK_TRACE)
        return;
    *list<auto> stack = get_thread_call_stack();
    if (!stack)
        return;
    splice stack, 0, 2;
    return map $1.type != "new-thread" ? sprintf("%s %s()", get_ex_pos($1), $1.function) : "new-thread", stack;
}

# processes workflow lists and creates a data structure with their dependencies
class WorkflowStepDependencyParser {
    public {
        # maps stepid -> type
        hash<string, string> stepTypeMap;

        # maps stepid -> array type
        hash<string, string> arrayTypeMap;

        # maps dstep dependencies: stepid -> list of predecessor step IDs
        hash<string, list<int>> stepDependencyMap;

        # workflow hash for error logging
        hash<auto> workflow;

        bool yaml_workflow;

        # list of steps
        list<hash<StepDependencyListEntry>> stepDependencyList();
    }

    # creates the object
    /** @param workflow workflow hash for error logging
        @param steps the steps to process; elements may be:
        - a list of any of these elements including other lists
        - a string identifying a step
        - a hash identifying a step
    */
    constructor(hash<auto> workflow, auto steps, bool yaml_workflow = False) {
        self.workflow = workflow;
        self.yaml_workflow = yaml_workflow;
        processWorkflowSteps(steps);
    }

    #! process a single workflow step and return a list of steps and step dependency information for the step
    /** @param predecessors zero or more steps that come before this step in the series; these are steps that must
    */
    processStep(auto step, *list<auto> predecessors) {
        #printf("processStep() %n, %n, %n\n", predecessors, fl, step);
        hash<auto> h = oload.getLoaderStepInfo(step, "PROCESS STEP ERROR", yaml_workflow);

        # check for duplicate step
        if (exists stepDependencyMap.(h.stepid)) {
            throw "DUPLICATE-STEP", sprintf("stepid %d (%s:%s) is listed more than once as a step in workflow %s:%s",
                h.stepid, h.name, h.version, workflow.name, workflow.version);
        }

        stepTypeMap.(h.stepid) = h.steptype;
        arrayTypeMap.(h.stepid) = h.arraytype;

        if (!oload.o.verbose) {
            oload.mystat(".");
        }

        if (predecessors) {
            foreach auto predecessor_step_entry in (predecessors) {
                hash<auto> predecessor_step_info = oload.getLoaderStepInfo(predecessor_step_entry, "STEP ERROR",
                    yaml_workflow);

                stepDependencyList += <StepDependencyListEntry>{
                    "step": h,
                    "dep": predecessor_step_info,
                };

                # add predecessor step ID as a dependency to this step
                stepDependencyMap.(h.stepid) += (predecessor_step_info.stepid,);
            }
        } else {
            stepDependencyList += <StepDependencyListEntry>{
                "step": h,
            };

            stepDependencyMap.(h.stepid) = ();
        }
    }

    #! process workflow steps and return a list of steps and step dependency information
    /** @param predecessors zero or more steps that come before this step in the series; these are steps that must
        be executed; elements are either strings or hashes identifying the predecessor step(s)
        @param workflow the workflow for error reporting
        @param steps the list of steps or single step to process; can be:
        - a list: a list of any of these types (even another list)
        - a hash: a hash that describes the step
        - a string: a string that identifies the step
        @param deps step dependency hash
    */
    processWorkflowSteps(auto steps, *softlist<auto> predecessors) {
        #printf("processWorkflowSteps() workflow=%s:%s last=%n steps=%n\n", workflow.name, workflow.version, predecessors, steps);
        foreach auto step_list_entry in (steps) {
            if (step_list_entry.typeCode() == NT_LIST) {
                # "step_list_entry" is a list, therefore each element in the list is run in parallel
                # here we add each parallel element in series after "predecessors"
                map processWorkflowSteps($1, predecessors), step_list_entry;
                # the dependencies are the last steps in each of the parallel series elements
                # clear and set predecessors with the last entries in each of the current parallel step lists
                predecessors = ();
                map getLastStep(\predecessors, $1), step_list_entry;
            } else {
                # add the step in series after "predecessors"
                processStep(step_list_entry, predecessors);
                # now "predecessors" is this step
                predecessors = step_list_entry;
            }
        }
    }

    # adds the given step to the predecessor list and returns itself
    /** @param predecessors a reference to a list of step info where each element identifies a single precdecessor
        step with a type of:
        - hash: identifying the predecessor step
        - string: identifying the predecessor step

        @return
    */
    static getLastStep(reference<softlist<auto>> predecessors, auto step) {
        predecessors += step;
    }

    # gets the last step in a series
    /** @param predecessors a reference to a list of step info where each element identifies a single precdecessor
        step with a type of:
        - hash: identifying the predecessor step
        - string: identifying the predecessor step
    */
    static getLastStep(reference<softlist<auto>> predecessors, list<auto> step) {
        map WorkflowStepDependencyParser::getLastStep(\predecessors, $1), step.last();
    }
}
